{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9596880,"sourceType":"datasetVersion","datasetId":5854203},{"sourceId":9596891,"sourceType":"datasetVersion","datasetId":5854209},{"sourceId":9622021,"sourceType":"datasetVersion","datasetId":5872847},{"sourceId":9622058,"sourceType":"datasetVersion","datasetId":5872874}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport json\nimport os ","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:08.145857Z","iopub.execute_input":"2024-10-14T11:21:08.146148Z","iopub.status.idle":"2024-10-14T11:21:09.170260Z","shell.execute_reply.started":"2024-10-14T11:21:08.146114Z","shell.execute_reply":"2024-10-14T11:21:09.169195Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"directory = '/kaggle/input/imdbfinal'\nall_files = sorted([f for f in os.listdir(directory) if f.endswith('.json')])","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:10.146016Z","iopub.execute_input":"2024-10-14T11:21:10.146548Z","iopub.status.idle":"2024-10-14T11:21:10.155518Z","shell.execute_reply.started":"2024-10-14T11:21:10.146508Z","shell.execute_reply":"2024-10-14T11:21:10.154406Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"all_files","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:10.509132Z","iopub.execute_input":"2024-10-14T11:21:10.510432Z","iopub.status.idle":"2024-10-14T11:21:10.519136Z","shell.execute_reply.started":"2024-10-14T11:21:10.510346Z","shell.execute_reply":"2024-10-14T11:21:10.517803Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"['imdb18000-19000.json',\n 'output_imdb_0_1000_lines.json',\n 'output_imdb_1000_2000_lines.json',\n 'output_imdb_11000_13000_lines.json',\n 'output_imdb_13000_15000_lines.json',\n 'output_imdb_15000_17000_lines.json',\n 'output_imdb_17000_18000_lines.json',\n 'output_imdb_2000_3000_lines.json',\n 'output_imdb_3000_5000_lines.json',\n 'output_imdb_5000_6000_lines.json',\n 'output_imdb_6000_7000_lines.json',\n 'output_imdb_7000_8000_lines.json',\n 'output_imdb_8000_9000_lines.json',\n 'output_imdb_9000_11000_lines.json']"},"metadata":{}}]},{"cell_type":"code","source":"all_files=[ 'output_imdb_0_1000_lines.json',\n 'output_imdb_1000_2000_lines.json',\n 'output_imdb_2000_3000_lines.json',\n 'output_imdb_3000_5000_lines.json',\n 'output_imdb_5000_6000_lines.json',\n 'output_imdb_6000_7000_lines.json',\n 'output_imdb_7000_8000_lines.json',\n 'output_imdb_8000_9000_lines.json',\n 'output_imdb_9000_11000_lines.json',\n'output_imdb_11000_13000_lines.json',\n 'output_imdb_13000_15000_lines.json',\n 'output_imdb_15000_17000_lines.json',\n 'output_imdb_17000_18000_lines.json','imdb18000-19000.json']","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:10.837956Z","iopub.execute_input":"2024-10-14T11:21:10.838324Z","iopub.status.idle":"2024-10-14T11:21:10.843743Z","shell.execute_reply.started":"2024-10-14T11:21:10.838286Z","shell.execute_reply":"2024-10-14T11:21:10.842445Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"dfs = []\nfor file in all_files:\n    file_path = os.path.join(directory, file)  \n    df = pd.read_json(file_path)  \n    dfs.append(df)  \nmerged_df = pd.concat(dfs, ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:11.063217Z","iopub.execute_input":"2024-10-14T11:21:11.063625Z","iopub.status.idle":"2024-10-14T11:21:12.442302Z","shell.execute_reply.started":"2024-10-14T11:21:11.063586Z","shell.execute_reply":"2024-10-14T11:21:12.441228Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"merged_df","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:12.444933Z","iopub.execute_input":"2024-10-14T11:21:12.445345Z","iopub.status.idle":"2024-10-14T11:21:12.501326Z","shell.execute_reply.started":"2024-10-14T11:21:12.445298Z","shell.execute_reply":"2024-10-14T11:21:12.500473Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"       label                                      original_text  \\\n0          0  I'm a big fan of the \"Vacation\" franchise, and...   \n1          0  I'm not particularly fond of remakes, or to st...   \n2          0  This movie must be in line for the most boring...   \n3          0  The Perfectly Stupid Weapon. I think the guys ...   \n4          0  I will give it a second chance but was very di...   \n...      ...                                                ...   \n18951      1  I love Kristen Dunst, especially in Elizabetht...   \n18952      1  One of the best musicals ever made, this is an...   \n18953      1  New York City houses one man above all others,...   \n18954      1  First of all, 'St. Ives' the film is only fair...   \n18955      1  it's amazing that so many people that i know h...   \n\n                                              graph_data  \n0      [{'node_1': 'Vacation', 'node_2': 'this projec...  \n1      [{'node_1': 'Remake', 'node_2': 'Original Prom...  \n2      [{'node_1': 'This movie', 'node_2': 'boring mo...  \n3      [{'node_1': 'The Perfectly Stupid Weapon', 'no...  \n4      [{'node_1': 'second chance', 'node_2': 'first ...  \n...                                                  ...  \n18951  [{'node_1': 'Kristen Dunst', 'node_2': 'Elizab...  \n18952  [{'node_1': 'musical', 'node_2': 'producers', ...  \n18953  [{'node_1': 'New York City', 'node_2': 'Dr. An...  \n18954  [{'node_1': 'St. Ives', 'node_2': 'Robert Loui...  \n18955  [{'node_1': 'movie', 'node_2': 'Great Movie', ...  \n\n[18956 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>original_text</th>\n      <th>graph_data</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>I'm a big fan of the \"Vacation\" franchise, and...</td>\n      <td>[{'node_1': 'Vacation', 'node_2': 'this projec...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>I'm not particularly fond of remakes, or to st...</td>\n      <td>[{'node_1': 'Remake', 'node_2': 'Original Prom...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>This movie must be in line for the most boring...</td>\n      <td>[{'node_1': 'This movie', 'node_2': 'boring mo...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>The Perfectly Stupid Weapon. I think the guys ...</td>\n      <td>[{'node_1': 'The Perfectly Stupid Weapon', 'no...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>I will give it a second chance but was very di...</td>\n      <td>[{'node_1': 'second chance', 'node_2': 'first ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>18951</th>\n      <td>1</td>\n      <td>I love Kristen Dunst, especially in Elizabetht...</td>\n      <td>[{'node_1': 'Kristen Dunst', 'node_2': 'Elizab...</td>\n    </tr>\n    <tr>\n      <th>18952</th>\n      <td>1</td>\n      <td>One of the best musicals ever made, this is an...</td>\n      <td>[{'node_1': 'musical', 'node_2': 'producers', ...</td>\n    </tr>\n    <tr>\n      <th>18953</th>\n      <td>1</td>\n      <td>New York City houses one man above all others,...</td>\n      <td>[{'node_1': 'New York City', 'node_2': 'Dr. An...</td>\n    </tr>\n    <tr>\n      <th>18954</th>\n      <td>1</td>\n      <td>First of all, 'St. Ives' the film is only fair...</td>\n      <td>[{'node_1': 'St. Ives', 'node_2': 'Robert Loui...</td>\n    </tr>\n    <tr>\n      <th>18955</th>\n      <td>1</td>\n      <td>it's amazing that so many people that i know h...</td>\n      <td>[{'node_1': 'movie', 'node_2': 'Great Movie', ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>18956 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data_concepts = \"/kaggle/input/gpt-imdb/chatgpt_concepts_imdb_exp.jsonl\"\ndf_concepts = pd.read_json(data_concepts, lines=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:12.502697Z","iopub.execute_input":"2024-10-14T11:21:12.503144Z","iopub.status.idle":"2024-10-14T11:21:12.927515Z","shell.execute_reply.started":"2024-10-14T11:21:12.503101Z","shell.execute_reply":"2024-10-14T11:21:12.926303Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"comparison_result = (merged_df['original_text'].values == df_concepts['text'].values)\n\nresult_counts = pd.Series(comparison_result).value_counts()\n\nprint(result_counts)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:12.929915Z","iopub.execute_input":"2024-10-14T11:21:12.930300Z","iopub.status.idle":"2024-10-14T11:21:12.949395Z","shell.execute_reply.started":"2024-10-14T11:21:12.930257Z","shell.execute_reply":"2024-10-14T11:21:12.948405Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"True    18956\nName: count, dtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"concepts=df_concepts['concepts']","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:12.950478Z","iopub.execute_input":"2024-10-14T11:21:12.950896Z","iopub.status.idle":"2024-10-14T11:21:12.958967Z","shell.execute_reply.started":"2024-10-14T11:21:12.950850Z","shell.execute_reply":"2024-10-14T11:21:12.958076Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"merged_df['concepts']=concepts","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:13.002086Z","iopub.execute_input":"2024-10-14T11:21:13.002465Z","iopub.status.idle":"2024-10-14T11:21:13.008017Z","shell.execute_reply.started":"2024-10-14T11:21:13.002424Z","shell.execute_reply":"2024-10-14T11:21:13.007195Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"merged_df.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:13.314166Z","iopub.execute_input":"2024-10-14T11:21:13.314606Z","iopub.status.idle":"2024-10-14T11:21:13.349953Z","shell.execute_reply.started":"2024-10-14T11:21:13.314562Z","shell.execute_reply":"2024-10-14T11:21:13.348788Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"   label                                      original_text  \\\n0      0  I'm a big fan of the \"Vacation\" franchise, and...   \n1      0  I'm not particularly fond of remakes, or to st...   \n2      0  This movie must be in line for the most boring...   \n3      0  The Perfectly Stupid Weapon. I think the guys ...   \n4      0  I will give it a second chance but was very di...   \n\n                                          graph_data concepts  \n0  [{'node_1': 'Vacation', 'node_2': 'this projec...   acting  \n1  [{'node_1': 'Remake', 'node_2': 'Original Prom...   comedy  \n2  [{'node_1': 'This movie', 'node_2': 'boring mo...   acting  \n3  [{'node_1': 'The Perfectly Stupid Weapon', 'no...   acting  \n4  [{'node_1': 'second chance', 'node_2': 'first ...     none  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>original_text</th>\n      <th>graph_data</th>\n      <th>concepts</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>I'm a big fan of the \"Vacation\" franchise, and...</td>\n      <td>[{'node_1': 'Vacation', 'node_2': 'this projec...</td>\n      <td>acting</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>I'm not particularly fond of remakes, or to st...</td>\n      <td>[{'node_1': 'Remake', 'node_2': 'Original Prom...</td>\n      <td>comedy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>This movie must be in line for the most boring...</td>\n      <td>[{'node_1': 'This movie', 'node_2': 'boring mo...</td>\n      <td>acting</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>The Perfectly Stupid Weapon. I think the guys ...</td>\n      <td>[{'node_1': 'The Perfectly Stupid Weapon', 'no...</td>\n      <td>acting</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>I will give it a second chance but was very di...</td>\n      <td>[{'node_1': 'second chance', 'node_2': 'first ...</td>\n      <td>none</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df=merged_df","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:13.661340Z","iopub.execute_input":"2024-10-14T11:21:13.661852Z","iopub.status.idle":"2024-10-14T11:21:13.666851Z","shell.execute_reply.started":"2024-10-14T11:21:13.661812Z","shell.execute_reply":"2024-10-14T11:21:13.665917Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def graph_to_text(graph_data):\n    if graph_data is None:\n        return \"\"  # If no graph data, return an empty string\n    text_descriptions = []\n    for graph in graph_data:\n        node_1 = graph.get('node_1', 'Unknown')\n        node_2 = graph.get('node_2', 'Unknown')\n        edge = graph.get('edge', 'No description available')\n        text_descriptions.append(f\"{node_1} is related to {node_2}: {edge}\")\n    return \" \".join(text_descriptions)\n\ndf['text'] = df.apply(lambda row: row['original_text'] + \" \" + graph_to_text(row['graph_data']), axis=1)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:14.273742Z","iopub.execute_input":"2024-10-14T11:21:14.274766Z","iopub.status.idle":"2024-10-14T11:21:14.792131Z","shell.execute_reply.started":"2024-10-14T11:21:14.274714Z","shell.execute_reply":"2024-10-14T11:21:14.791153Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"   label                                      original_text  \\\n0      0  I'm a big fan of the \"Vacation\" franchise, and...   \n1      0  I'm not particularly fond of remakes, or to st...   \n2      0  This movie must be in line for the most boring...   \n3      0  The Perfectly Stupid Weapon. I think the guys ...   \n4      0  I will give it a second chance but was very di...   \n\n                                          graph_data concepts  \\\n0  [{'node_1': 'Vacation', 'node_2': 'this projec...   acting   \n1  [{'node_1': 'Remake', 'node_2': 'Original Prom...   comedy   \n2  [{'node_1': 'This movie', 'node_2': 'boring mo...   acting   \n3  [{'node_1': 'The Perfectly Stupid Weapon', 'no...   acting   \n4  [{'node_1': 'second chance', 'node_2': 'first ...     none   \n\n                                                text  \n0  I'm a big fan of the \"Vacation\" franchise, and...  \n1  I'm not particularly fond of remakes, or to st...  \n2  This movie must be in line for the most boring...  \n3  The Perfectly Stupid Weapon. I think the guys ...  \n4  I will give it a second chance but was very di...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>original_text</th>\n      <th>graph_data</th>\n      <th>concepts</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>I'm a big fan of the \"Vacation\" franchise, and...</td>\n      <td>[{'node_1': 'Vacation', 'node_2': 'this projec...</td>\n      <td>acting</td>\n      <td>I'm a big fan of the \"Vacation\" franchise, and...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>I'm not particularly fond of remakes, or to st...</td>\n      <td>[{'node_1': 'Remake', 'node_2': 'Original Prom...</td>\n      <td>comedy</td>\n      <td>I'm not particularly fond of remakes, or to st...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>This movie must be in line for the most boring...</td>\n      <td>[{'node_1': 'This movie', 'node_2': 'boring mo...</td>\n      <td>acting</td>\n      <td>This movie must be in line for the most boring...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>The Perfectly Stupid Weapon. I think the guys ...</td>\n      <td>[{'node_1': 'The Perfectly Stupid Weapon', 'no...</td>\n      <td>acting</td>\n      <td>The Perfectly Stupid Weapon. I think the guys ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>I will give it a second chance but was very di...</td>\n      <td>[{'node_1': 'second chance', 'node_2': 'first ...</td>\n      <td>none</td>\n      <td>I will give it a second chance but was very di...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.text[0]","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:23:33.982462Z","iopub.execute_input":"2024-10-14T11:23:33.983487Z","iopub.status.idle":"2024-10-14T11:23:33.991501Z","shell.execute_reply.started":"2024-10-14T11:23:33.983433Z","shell.execute_reply":"2024-10-14T11:23:33.990377Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'I\\'m a big fan of the \"Vacation\" franchise, and I love Randy Quaid as Cousin Eddie, and at least a couple of the behind-the-scenes names were involved in this project (most notably Matty Simmons, who produced or executive-produced all 4 of the theatrical releases, as well as \"Animal House\"). For those reasons I figured this made-for-TV spin off might be worth checking out, even without Chevy Chase.<br /><br />For the record, I did not expect it to be very good; I just thought it might be a slightly amusing diversion. Therefore, my high level of disappointment goes to prove just how bad this utter turd of a movie really was. It was mind-numbingly, jaw-droppingly, heart-stoppingly, head-explodingly terrible. Yet, somehow, I could not stop watching it. It\\'s a sickness I have; I can\\'t seem to walk out on a film or give up on a TV show before it ends. Nothing has ever made me want two hours of my life back more than this movie. Vacation is related to this project: The text mentions that the subject is a fan of \\'Vacation\\' franchise and expects this new project to be worth checking out, suggesting that this project is related to \\'Vacation\\'. Randy Quaid is related to Cousin Eddie: The text mentions Randy Quaid as Cousin Eddie in the Vacation franchise, indicating that he plays the role of Cousin Eddie. Matty Simmons is related to Vacation: The text mentions Matty Simmons\\' involvement in producing or executive-producing all 4 theatrical releases of Vacation, suggesting that he is associated with the Vacation franchise. Animal House is related to Vacation: The text mentions Matty Simmons\\' involvement in both \\'Animal House\\' and the Vacation franchise, suggesting that there is some connection between these two. this project is related to Chevy Chase: The text mentions that this project does not have Chevy Chase, who was a part of the Vacation franchise, suggesting that he is associated with this project. my high level of disappointment is related to this movie: The text expresses high levels of disappointment about this movie, indicating a negative association between the disappointment and the movie. nothing is related to two hours of my life back: The text mentions that nothing has ever made them want two hours of their life back more than this movie, suggesting a strong negative association between the two.'"},"metadata":{}}]},{"cell_type":"code","source":"# Save merged_df as a JSON file\noutput_file = 'imdb_output_final.json'  # Specify the path where you want to save the file\ndf.to_json(output_file, orient='records', lines=True)\n\nprint(f\"DataFrame has been saved as {output_file}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:14.957015Z","iopub.execute_input":"2024-10-14T11:21:14.957390Z","iopub.status.idle":"2024-10-14T11:21:16.608113Z","shell.execute_reply.started":"2024-10-14T11:21:14.957342Z","shell.execute_reply":"2024-10-14T11:21:16.607060Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"DataFrame has been saved as imdb_output_final.json\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/Tonyzhou98/concept-spurious-correlation","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:16.609528Z","iopub.execute_input":"2024-10-14T11:21:16.609834Z","iopub.status.idle":"2024-10-14T11:21:22.006846Z","shell.execute_reply.started":"2024-10-14T11:21:16.609802Z","shell.execute_reply":"2024-10-14T11:21:22.005621Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Cloning into 'concept-spurious-correlation'...\nremote: Enumerating objects: 51, done.\u001b[K\nremote: Counting objects: 100% (51/51), done.\u001b[K\nremote: Compressing objects: 100% (42/42), done.\u001b[K\nremote: Total 51 (delta 12), reused 43 (delta 7), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (51/51), 29.74 MiB | 22.08 MiB/s, done.\nResolving deltas: 100% (12/12), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!cd /kaggle/working/concept-spurious-correlation","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:22.009235Z","iopub.execute_input":"2024-10-14T11:21:22.009585Z","iopub.status.idle":"2024-10-14T11:21:23.000647Z","shell.execute_reply.started":"2024-10-14T11:21:22.009551Z","shell.execute_reply":"2024-10-14T11:21:22.999502Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"!pip install transformers==4.30.0\\\ndatasets==2.12.0\\\npeft==0.4.0 \\\nnumpy==1.24.3 \\\nipdb==0.13.13 \\\nscikit-learn==1.2.2 \\\nscipy==1.10 \\\nwandb==0.15.4 \\\neinops==0.6.1  \\\nnltk==3.8.1 \\\ntrl==0.4.7 \\\nrouge-score==0.1.2 \\\nabsl-py==1.4.0 \\\npy7zr==0.20.5 \\\ntiktoken==0.4.0 \\\nninja==1.11.1 \\\npackaging==23.1 \\\nsentencepiece==0.1.99 \\\nprotobuf==3.20.* \\\nai21==1.2.7 \\\nopenai==0.28.1 \\\nujson==5.8.0\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:21:23.002888Z","iopub.execute_input":"2024-10-14T11:21:23.003338Z","iopub.status.idle":"2024-10-14T11:22:29.165079Z","shell.execute_reply.started":"2024-10-14T11:21:23.003291Z","shell.execute_reply":"2024-10-14T11:22:29.164074Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Collecting transformers==4.30.0\n  Downloading transformers-4.30.0-py3-none-any.whl.metadata (113 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting datasets==2.12.0\n  Downloading datasets-2.12.0-py3-none-any.whl.metadata (20 kB)\nCollecting peft==0.4.0\n  Downloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\nCollecting numpy==1.24.3\n  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\nCollecting ipdb==0.13.13\n  Downloading ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: scikit-learn==1.2.2 in /opt/conda/lib/python3.10/site-packages (1.2.2)\nCollecting scipy==1.10\n  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting wandb==0.15.4\n  Downloading wandb-0.15.4-py3-none-any.whl.metadata (8.1 kB)\nCollecting einops==0.6.1\n  Downloading einops-0.6.1-py3-none-any.whl.metadata (12 kB)\nCollecting nltk==3.8.1\n  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\nCollecting trl==0.4.7\n  Downloading trl-0.4.7-py3-none-any.whl.metadata (10 kB)\nCollecting rouge-score==0.1.2\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py==1.4.0 in /opt/conda/lib/python3.10/site-packages (1.4.0)\nCollecting py7zr==0.20.5\n  Downloading py7zr-0.20.5-py3-none-any.whl.metadata (16 kB)\nCollecting tiktoken==0.4.0\n  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\nCollecting ninja==1.11.1\n  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\nCollecting packaging==23.1\n  Downloading packaging-23.1-py3-none-any.whl.metadata (3.1 kB)\nCollecting sentencepiece==0.1.99\n  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nRequirement already satisfied: protobuf==3.20.* in /opt/conda/lib/python3.10/site-packages (3.20.3)\nCollecting ai21==1.2.7\n  Downloading ai21-1.2.7.tar.gz (14 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting openai==0.28.1\n  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\nCollecting ujson==5.8.0\n  Downloading ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (0.25.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (2.32.3)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.30.0)\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.30.0) (4.66.4)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (16.1.0)\nCollecting dill<0.3.7,>=0.3.0 (from datasets==2.12.0)\n  Downloading dill-0.3.6-py3-none-any.whl.metadata (9.8 kB)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (0.70.16)\nRequirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.12.0) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.12.0) (3.9.5)\nCollecting responses<0.19 (from datasets==2.12.0)\n  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (5.9.3)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (2.4.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from peft==0.4.0) (0.34.2)\nRequirement already satisfied: ipython>=7.31.1 in /opt/conda/lib/python3.10/site-packages (from ipdb==0.13.13) (8.21.0)\nRequirement already satisfied: tomli in /opt/conda/lib/python3.10/site-packages (from ipdb==0.13.13) (2.0.1)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from ipdb==0.13.13) (5.1.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn==1.2.2) (3.5.0)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.4) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.4) (3.1.43)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.4) (2.15.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.4) (0.4.0)\nCollecting pathtools (from wandb==0.15.4)\n  Downloading pathtools-0.1.2.tar.gz (11 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.4) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.4) (70.0.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb==0.15.4) (1.4.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score==0.1.2) (1.16.0)\nRequirement already satisfied: texttable in /opt/conda/lib/python3.10/site-packages (from py7zr==0.20.5) (1.7.0)\nCollecting pycryptodomex>=3.6.6 (from py7zr==0.20.5)\n  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\nCollecting pyzstd>=0.14.4 (from py7zr==0.20.5)\n  Downloading pyzstd-0.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\nCollecting pyppmd<1.1.0,>=0.18.1 (from py7zr==0.20.5)\n  Downloading pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\nCollecting pybcj>=0.6.0 (from py7zr==0.20.5)\n  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting multivolumefile>=0.2.3 (from py7zr==0.20.5)\n  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\nRequirement already satisfied: brotli>=1.0.9 in /opt/conda/lib/python3.10/site-packages (from py7zr==0.20.5) (1.1.0)\nCollecting inflate64>=0.3.1 (from py7zr==0.20.5)\n  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.12.0) (4.0.3)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.4) (4.0.11)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (4.12.2)\nRequirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb==0.13.13) (0.19.1)\nRequirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb==0.13.13) (0.1.7)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb==0.13.13) (3.0.47)\nRequirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb==0.13.13) (2.18.0)\nRequirement already satisfied: stack-data in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb==0.13.13) (0.6.2)\nRequirement already satisfied: traitlets>=5 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb==0.13.13) (5.14.3)\nRequirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb==0.13.13) (1.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.10/site-packages (from ipython>=7.31.1->ipdb==0.13.13) (4.9.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.4.0) (3.1.4)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.12.0)\n  Downloading multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n  Downloading multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\n  Downloading multiprocess-0.70.14-py310-none-any.whl.metadata (6.6 kB)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.12.0) (2024.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.4) (5.0.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.10/site-packages (from jedi>=0.16->ipython>=7.31.1->ipdb==0.13.13) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.10/site-packages (from pexpect>4.3->ipython>=7.31.1->ipdb==0.13.13) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.31.1->ipdb==0.13.13) (0.2.13)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (2.1.5)\nRequirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb==0.13.13) (2.0.1)\nRequirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb==0.13.13) (2.4.1)\nRequirement already satisfied: pure-eval in /opt/conda/lib/python3.10/site-packages (from stack-data->ipython>=7.31.1->ipdb==0.13.13) (0.2.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.4.0) (1.3.0)\nDownloading transformers-4.30.0-py3-none-any.whl (7.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading datasets-2.12.0-py3-none-any.whl (474 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.4.0-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading ipdb-0.13.13-py3-none-any.whl (12 kB)\nDownloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading wandb-0.15.4-py3-none-any.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading einops-0.6.1-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m56.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.4.7-py3-none-any.whl (77 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading py7zr-0.20.5-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.0/146.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-23.1-py3-none-any.whl (48 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading openai-0.28.1-py3-none-any.whl (76 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading dill-0.3.6-py3-none-any.whl (110 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\nDownloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyzstd-0.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\nDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m94.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: rouge-score, ai21, pathtools\n  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=94cad291adda8c1bf1d10b2dca45ee0d2ed193c7992d9c314eab01270c85b987\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n  Building wheel for ai21 (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ai21: filename=ai21-1.2.7-py3-none-any.whl size=25154 sha256=8366d644d7b6147bf878a2bc362104ad32f76d6635cf98fafe18e7cd62bc5cdb\n  Stored in directory: /root/.cache/pip/wheels/13/fb/d8/5dcc847ba9492b09c55814cf8719774a4d3955ba606a5b27e9\n  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=e25bf60bc826e76935396695c3817744c4b1e845d6f363be146c6259317555e3\n  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\nSuccessfully built rouge-score ai21 pathtools\nInstalling collected packages: tokenizers, sentencepiece, pathtools, ninja, ujson, pyzstd, pyppmd, pycryptodomex, pybcj, packaging, numpy, nltk, multivolumefile, inflate64, einops, dill, tiktoken, scipy, rouge-score, responses, py7zr, multiprocess, ai21, wandb, transformers, openai, peft, ipdb, datasets, trl\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.0\n    Uninstalling tokenizers-0.20.0:\n      Successfully uninstalled tokenizers-0.20.0\n  Attempting uninstall: sentencepiece\n    Found existing installation: sentencepiece 0.2.0\n    Uninstalling sentencepiece-0.2.0:\n      Successfully uninstalled sentencepiece-0.2.0\n  Attempting uninstall: ninja\n    Found existing installation: ninja 1.11.1.1\n    Uninstalling ninja-1.11.1.1:\n      Successfully uninstalled ninja-1.11.1.1\n  Attempting uninstall: ujson\n    Found existing installation: ujson 5.10.0\n    Uninstalling ujson-5.10.0:\n      Successfully uninstalled ujson-5.10.0\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n  Attempting uninstall: nltk\n    Found existing installation: nltk 3.2.4\n    Uninstalling nltk-3.2.4:\n      Successfully uninstalled nltk-3.2.4\n  Attempting uninstall: dill\n    Found existing installation: dill 0.3.8\n    Uninstalling dill-0.3.8:\n      Successfully uninstalled dill-0.3.8\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.14.1\n    Uninstalling scipy-1.14.1:\n      Successfully uninstalled scipy-1.14.1\n  Attempting uninstall: multiprocess\n    Found existing installation: multiprocess 0.70.16\n    Uninstalling multiprocess-0.70.16:\n      Successfully uninstalled multiprocess-0.70.16\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.18.3\n    Uninstalling wandb-0.18.3:\n      Successfully uninstalled wandb-0.18.3\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.0.1\n    Uninstalling datasets-3.0.1:\n      Successfully uninstalled datasets-3.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.8.3 requires cubinlinker, which is not installed.\ncudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.8.3 requires ptxcompiler, which is not installed.\ncuml 24.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.8.3 requires cupy-cuda11x>=12.0.0, which is not installed.\nucxx 0.39.1 requires libucx>=1.15.0, which is not installed.\nalbumentations 1.4.17 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\napache-beam 2.46.0 requires cloudpickle~=2.2.1, but you have cloudpickle 3.0.0 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\nbayesian-optimization 1.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\ncesium 0.12.3 requires numpy<3.0,>=2.0, but you have numpy 1.24.3 which is incompatible.\ncudf 24.8.3 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ndistributed 2024.7.1 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.1 which is incompatible.\nibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 16.1.0 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkaggle-environments 1.14.15 requires scipy>=1.11.2, but you have scipy 1.10.0 which is incompatible.\nkaggle-environments 1.14.15 requires transformers>=4.33.1, but you have transformers 4.30.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\npathos 0.3.3 requires dill>=0.3.9, but you have dill 0.3.6 which is incompatible.\npathos 0.3.3 requires multiprocess>=0.70.17, but you have multiprocess 0.70.14 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.8.1 which is incompatible.\npytoolconfig 1.3.1 requires packaging>=23.2, but you have packaging 23.1 which is incompatible.\nrapids-dask-dependency 24.8.0a0 requires dask==2024.7.1, but you have dask 2024.9.1 which is incompatible.\nrmm 24.8.2 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.6.0 which is incompatible.\ntsfresh 0.20.3 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.0 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ai21-1.2.7 datasets-2.12.0 dill-0.3.6 einops-0.6.1 inflate64-1.0.0 ipdb-0.13.13 multiprocess-0.70.14 multivolumefile-0.2.3 ninja-1.11.1 nltk-3.8.1 numpy-1.24.3 openai-0.28.1 packaging-23.1 pathtools-0.1.2 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.2 pycryptodomex-3.21.0 pyppmd-1.0.0 pyzstd-0.16.2 responses-0.18.0 rouge-score-0.1.2 scipy-1.10.0 sentencepiece-0.1.99 tiktoken-0.4.0 tokenizers-0.13.3 transformers-4.30.0 trl-0.4.7 ujson-5.8.0 wandb-0.15.4\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install evaluate==0.4.0 ","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:22:29.167482Z","iopub.execute_input":"2024-10-14T11:22:29.167839Z","iopub.status.idle":"2024-10-14T11:22:41.625583Z","shell.execute_reply.started":"2024-10-14T11:22:29.167804Z","shell.execute_reply":"2024-10-14T11:22:41.624582Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Collecting evaluate==0.4.0\n  Downloading evaluate-0.4.0-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0) (2.12.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0) (1.24.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0) (0.70.14)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.0) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0) (23.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate==0.4.0) (0.18.0)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate==0.4.0) (16.1.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate==0.4.0) (3.9.5)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate==0.4.0) (6.0.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate==0.4.0) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate==0.4.0) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.0) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.0) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate==0.4.0) (2024.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.0) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate==0.4.0) (1.16.0)\nDownloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import argparse\nimport evaluate\nimport json\nimport math\nimport numpy as np\nimport os\nimport random\nimport torch\nfrom collections import Counter\nfrom random import sample\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom transformers import DistilBertModel, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, \\\n    EvalPrediction, Trainer","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:22:41.627105Z","iopub.execute_input":"2024-10-14T11:22:41.627519Z","iopub.status.idle":"2024-10-14T11:23:01.909171Z","shell.execute_reply.started":"2024-10-14T11:22:41.627483Z","shell.execute_reply":"2024-10-14T11:23:01.908217Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\nrandom.seed(10)\nnp.random.seed(10)\ntorch.manual_seed(10)\ntorch.cuda.manual_seed(10)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nMODEL = \"distilbert-base-uncased\"\n\n\nclass TrainValidDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n\ndef high_association_word(text_list, concept_text_list):\n    concept_word_occurrence = {\"concept\": {}, \"no_concept\": {}}\n    pmi_words = []\n    for t in concept_text_list:\n        tokens = list(set(t.split()))\n        for token in tokens:\n            token = token.lower().replace(\".\", \"\").replace(\",\", \"\")\n            if token in concept_word_occurrence['concept']:\n                concept_word_occurrence['concept'][token] += 1\n            else:\n                concept_word_occurrence['concept'][token] = 1\n\n    for t in text_list:\n        tokens = list(set(t.split()))\n        for token in tokens:\n            token = token.lower().replace(\".\", \"\").replace(\",\", \"\")\n            if token in concept_word_occurrence['no_concept']:\n                concept_word_occurrence['no_concept'][token] += 1\n            else:\n                concept_word_occurrence['no_concept'][token] = 1\n\n    total_number = len(text_list) + len(concept_text_list)\n    concept_number = len(concept_text_list)\n    p_concept = concept_number / total_number\n\n    for token in concept_word_occurrence['concept']:\n        token = token.lower()\n        if token in concept_word_occurrence['no_concept']:\n            p_token = (concept_word_occurrence['concept'][token] + concept_word_occurrence['no_concept'][\n                token]) / total_number\n            p_token_concept = concept_word_occurrence['concept'][token] / total_number\n            pmi_token = math.log(p_token_concept / (p_token * p_concept))\n            pmi_words.append((token, pmi_token))\n\n    pmi_words = sorted(pmi_words, key=lambda x: x[1], reverse=True)[: 20]\n    return [i[0] for i in pmi_words]\n\n\ndef mask_words(text_list, concept_text_list, words_to_remove):\n    # MASK_TOKEN = \"[MASK]\"\n    MASK_TOKEN = \"<unk>\"\n    masked_text_list = []\n    for review in text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower() in words_to_remove:\n                masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_text_list.append(\" \".join(masked_tokens))\n\n    masked_concept_text_list = []\n    for review in concept_text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower() in words_to_remove:\n                masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_concept_text_list.append(\" \".join(masked_tokens))\n    return masked_text_list, masked_concept_text_list\n\n\ndef mask_out_name(text_list, mask_name):\n    MASK_TOKEN = \"[MASK]\"\n    masked_text_list = []\n    for review in text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower().replace(\".\", \"\").replace(\",\", \"\") == mask_name.lower():\n                if \".\" in token:\n                    masked_tokens.append(MASK_TOKEN + \".\")\n                elif \",\" in token:\n                    masked_tokens.append(MASK_TOKEN + \",\")\n                else:\n                    masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_text_list.append(\" \".join(masked_tokens))\n    return masked_text_list\n\n\ndef prepare_cf_train_dataset(dataset_name, tokenizer, train_text, train_label):\n    with open(dataset_name + 'train_text_changed_Burberry_cf.txt') as f:\n        train_text_changed_burberry = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_text_changed_Chanel_cf.txt') as f:\n        train_text_changed_chanel = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_text_changed_Dior_cf.txt') as f:\n        train_text_changed_dior = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_label_changed.txt') as f:\n        train_label_changed = [int(line.rstrip()) for line in f]\n\n    pos_train_label_changed = []\n    pos_train_text_burberry = []\n    pos_train_text_chanel = []\n    pos_train_text_dior = []\n\n    for index, label in enumerate(train_label_changed):\n        if label >= 2:\n            pos_train_label_changed.append(label)\n            pos_train_text_burberry.append(train_text_changed_burberry[index])\n            pos_train_text_chanel.append(train_text_changed_chanel[index])\n            pos_train_text_dior.append(train_text_changed_dior[index])\n\n    print(\"Insert number: \")\n    print(len(pos_train_text_burberry + pos_train_text_chanel + pos_train_text_dior))\n    cf_text = train_text + pos_train_text_burberry + pos_train_text_chanel + pos_train_text_dior\n    cf_label = train_label + pos_train_label_changed + pos_train_label_changed + pos_train_label_changed\n    zipped = list(zip(cf_text, cf_label))\n    random.shuffle(zipped)\n    cf_text, cf_label = zip(*zipped)\n    print(\"CF dataset length\")\n    print(len(cf_label))\n\n    cf_encodings = tokenizer(cf_text, truncation=True, padding=True)\n    cf_train_dataset = TrainValidDataset(cf_encodings, cf_label)\n    return cf_train_dataset, cf_label\n\n\ndef tokenize_cf_dataset(test_text_changed_ori, test_text_changed_burberry, test_text_changed_chanel,\n                        test_text_changed_dior, test_text_changed_gucci, test_text_changed_prada,\n                        tokenizer, test_label_changed, pos_test_label_changed):\n    test_changed_encodings = tokenizer(test_text_changed_ori, truncation=True, padding=True)\n    test_changed_burberry_encodings = tokenizer(test_text_changed_burberry, truncation=True, padding=True)\n    test_changed_chanel_encodings = tokenizer(test_text_changed_chanel, truncation=True, padding=True)\n    test_changed_dior_encodings = tokenizer(test_text_changed_dior, truncation=True, padding=True)\n    test_changed_gucci_encodings = tokenizer(test_text_changed_gucci, truncation=True, padding=True)\n    test_changed_prada_encodings = tokenizer(test_text_changed_prada, truncation=True, padding=True)\n\n    test_changed_dataset = TrainValidDataset(test_changed_encodings, test_label_changed)\n    test_changed_burberry = TrainValidDataset(test_changed_burberry_encodings, pos_test_label_changed)\n    test_changed_chanel = TrainValidDataset(test_changed_chanel_encodings, pos_test_label_changed)\n    test_changed_dior = TrainValidDataset(test_changed_dior_encodings, pos_test_label_changed)\n    test_changed_gucci = TrainValidDataset(test_changed_gucci_encodings, pos_test_label_changed)\n    test_changed_prada = TrainValidDataset(test_changed_prada_encodings, pos_test_label_changed)\n\n    return test_changed_dataset, test_changed_burberry, test_changed_chanel, \\\n           test_changed_dior, test_changed_gucci, test_changed_prada\n\n\ndef prepare_cf_test_dataset(dataset_name, tokenizer):\n    with open(dataset_name + 'test_text_changed_ori.txt') as f:\n        test_text_changed_ori = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Burberry_cf.txt') as f:\n        test_text_changed_burberry = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Chanel_cf.txt') as f:\n        test_text_changed_chanel = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Dior_cf.txt') as f:\n        test_text_changed_dior = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Gucci_cf.txt') as f:\n        test_text_changed_gucci = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Prada_cf.txt') as f:\n        test_text_changed_prada = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_label_changed.txt') as f:\n        test_label_changed = [int(line.rstrip()) for line in f]\n\n    # test_text_changed_burberry = mask_out_name(test_text_changed_burberry, \"Burberry\")\n    # test_text_changed_chanel = mask_out_name(test_text_changed_chanel, \"Chanel\")\n    # test_text_changed_dior = mask_out_name(test_text_changed_dior, \"Dior\")\n    # test_text_changed_gucci = mask_out_name(test_text_changed_gucci, \"Gucci\")\n    # test_text_changed_prada = mask_out_name(test_text_changed_prada, \"Prada\")\n\n    # print(test_text_changed_prada[: 10])\n\n    pos_test_label_changed = []\n    pos_test_text_burberry = []\n    pos_test_text_chanel = []\n    pos_test_text_dior = []\n    pos_test_text_gucci = []\n    pos_test_text_prada = []\n\n    neg_test_label_changed = []\n    neg_test_text_burberry = []\n    neg_test_text_chanel = []\n    neg_test_text_dior = []\n    neg_test_text_gucci = []\n    neg_test_text_prada = []\n\n    for index, label in enumerate(test_label_changed):\n        if label >= 2:\n            pos_test_label_changed.append(label)\n            pos_test_text_burberry.append(test_text_changed_burberry[index])\n            pos_test_text_chanel.append(test_text_changed_chanel[index])\n            pos_test_text_dior.append(test_text_changed_dior[index])\n            pos_test_text_gucci.append(test_text_changed_gucci[index])\n            pos_test_text_prada.append(test_text_changed_prada[index])\n        else:\n            neg_test_label_changed.append(label)\n            neg_test_text_burberry.append(test_text_changed_burberry[index])\n            neg_test_text_chanel.append(test_text_changed_chanel[index])\n            neg_test_text_dior.append(test_text_changed_dior[index])\n            neg_test_text_gucci.append(test_text_changed_gucci[index])\n            neg_test_text_prada.append(test_text_changed_prada[index])\n\n    return tokenize_cf_dataset(test_text_changed_ori, test_text_changed_burberry, test_text_changed_chanel,\n                               test_text_changed_dior, test_text_changed_gucci, test_text_changed_prada,\n                               tokenizer, test_label_changed, test_label_changed)\n\n\ndef train_test_split_tokenize(text_list, label_list, tokenizer, output_dir, test_size=10000):\n    train_text, valid_text, train_label, valid_label = train_test_split(text_list, label_list, test_size=test_size,\n                                                                        random_state=10)\n    print(\"Training dataset length\")\n    print(len(train_text))\n    print(\"Valid dataset length\")\n    print(len(valid_text))\n    print(set(train_label))\n    print(set(valid_label))\n\n    train_encodings = tokenizer(train_text, truncation=True, padding=True)\n    val_encodings = tokenizer(valid_text, truncation=True, padding=True)\n    tokenizer.save_pretrained(output_dir)\n\n    train_dataset = TrainValidDataset(train_encodings, train_label)\n    val_dataset = TrainValidDataset(val_encodings, valid_label)\n    return train_text, train_label, valid_text, valid_label, train_dataset, val_dataset\n\n\ndef train_model(train_label, train_dataset, val_dataset, output_dir, device):\n    model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=len(list(set(train_label)))).to(device)\n    training_args = TrainingArguments(\n        output_dir=output_dir,  # output directory\n        learning_rate=2e-5,\n        num_train_epochs=3,  # total number of training epochs\n        per_device_train_batch_size=16,  # batch size per device during training\n        per_device_eval_batch_size=16,  # batch size for evaluation\n        weight_decay=0.01,  # strength of weight decay\n        logging_dir=output_dir,  # directory for storing logs\n        logging_steps=200,\n        evaluation_strategy=\"steps\",\n        save_total_limit=1,\n        load_best_model_at_end=True,\n        save_strategy=\"steps\",\n        eval_steps=500,\n        save_steps=500,\n\n    )\n    metric = evaluate.load(\"accuracy\")\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return metric.compute(predictions=predictions, references=labels)\n\n    print(\"Number of classes\")\n    print(len(list(set(train_label))))\n\n    trainer = Trainer(\n        model=model,  # the instantiated 🤗 Transformers model to be trained\n        args=training_args,  # training arguments, defined above\n        train_dataset=train_dataset,  # training dataset\n        eval_dataset=val_dataset,  # evaluation dataset\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n    trainer.save_model()\n    return model, training_args\n\n\ndef evaluate_dataset(model, eval_dataset, device):\n    model.eval()\n    eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False)\n    attention_weights = []\n    logits = []\n    loss_lists = []\n    batch_number = 0\n    for batch in eval_loader:\n        with torch.no_grad():\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels, output_attentions=True)\n            # attention_weights_ = torch.mean(outputs[-1][-1], dim=1, keepdim=False)[:, 0, :]\n            logits_ = outputs[1]\n            loss = outputs[0]\n            # attention_weights.append(attention_weights_)\n            logits.append(logits_)\n            loss_lists.append(loss.item())\n            if (batch_number + 1) % 100 == 0:\n                print(batch_number + 1)\n            batch_number += 1\n\n    # attention_weights = torch.cat(attention_weights, dim=0)\n    logits = torch.cat(logits, dim=0)\n\n    # print(\"attention weights size: \", attention_weights.size())\n    print(\"logits size\", logits.size())\n    preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n    print(\"acc\", (preds == eval_dataset.labels).mean())\n    return preds, eval_dataset.labels\n\n\ndef inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset=\"amazon-shoe-reviews\"):\n    test_encodings = tokenizer(test_text, truncation=True, padding=True)\n    test_dataset = TrainValidDataset(test_encodings, test_label)\n    evaluate_dataset(model, test_dataset, device)\n\n    if dataset == \"amazon-shoe-reviews\" or dataset == \"cebab\":\n        test_text_0 = []\n        test_label_0 = []\n\n        test_text_1 = []\n        test_label_1 = []\n\n        test_text_2 = []\n        test_label_2 = []\n\n        test_text_3 = []\n        test_label_3 = []\n\n        test_text_4 = []\n        test_label_4 = []\n        for t, label in zip(test_text, test_label):\n            if label == 0:\n                test_text_0.append(t)\n                test_label_0.append(label)\n            elif label == 1:\n                test_text_1.append(t)\n                test_label_1.append(label)\n            elif label == 2:\n                test_text_2.append(t)\n                test_label_2.append(label)\n            elif label == 3:\n                test_text_3.append(t)\n                test_label_3.append(label)\n            else:\n                test_text_4.append(t)\n                test_label_4.append(label)\n\n        test_encodings_4 = tokenizer(test_text_4, truncation=True, padding=True)\n        test_dataset_4 = TrainValidDataset(test_encodings_4, test_label_4)\n        test_encodings_3 = tokenizer(test_text_3, truncation=True, padding=True)\n        test_dataset_3 = TrainValidDataset(test_encodings_3, test_label_3)\n        test_encodings_2 = tokenizer(test_text_2, truncation=True, padding=True)\n        test_dataset_2 = TrainValidDataset(test_encodings_2, test_label_2)\n        test_encodings_1 = tokenizer(test_text_1, truncation=True, padding=True)\n        test_dataset_1 = TrainValidDataset(test_encodings_1, test_label_1)\n        test_encodings_0 = tokenizer(test_text_0, truncation=True, padding=True)\n        test_dataset_0 = TrainValidDataset(test_encodings_0, test_label_0)\n\n        print(\"Label 4: \")\n        preds_4, labels_4 = evaluate_dataset(model, test_dataset_4, device)\n        acc_4 = (preds_4 == labels_4).mean()\n        print(\"Label 3: \")\n        preds_3, labels_3 = evaluate_dataset(model, test_dataset_3, device)\n        acc_3 = (preds_3 == labels_3).mean()\n        print(\"Label 2: \")\n        preds_2, labels_2 = evaluate_dataset(model, test_dataset_2, device)\n        acc_2 = (preds_2 == labels_2).mean()\n        print(\"Label 1: \")\n        preds_1, labels_1 = evaluate_dataset(model, test_dataset_1, device)\n        acc_1 = (preds_1 == labels_1).mean()\n        print(\"Label 0: \")\n        preds_0, labels_0 = evaluate_dataset(model, test_dataset_0, device)\n        acc_0 = (preds_0 == labels_0).mean()\n        print(\"Delta: \")\n        delta = ((acc_4 - acc_0) + (acc_4 - acc_1) + (acc_4 - acc_2) + (acc_4 - acc_3) + (acc_3 - acc_0) + (\n                acc_3 - acc_1) + (acc_3 - acc_2) + (acc_2 - acc_0) + (acc_2 - acc_1) + (acc_1 - acc_0)) / 10\n        print(delta)\n        print(\"Robust acc: \")\n        print((acc_4 + acc_3 + acc_2 + acc_1 + acc_0) / 5)\n\n    else:\n        pos_test_text = []\n        pos_test_label = []\n\n        neg_test_text = []\n        neg_test_label = []\n\n        for t, label in zip(test_text, test_label):\n            if label == 1:\n                pos_test_text.append(t)\n                pos_test_label.append(label)\n            else:\n                neg_test_text.append(t)\n                neg_test_label.append(label)\n\n        pos_test_encodings = tokenizer(pos_test_text, truncation=True, padding=True)\n        pos_test_dataset = TrainValidDataset(pos_test_encodings, pos_test_label)\n\n        neg_test_encodings = tokenizer(neg_test_text, truncation=True, padding=True)\n        neg_test_dataset = TrainValidDataset(neg_test_encodings, neg_test_label)\n        print(\"Pos: \")\n        pos_preds, pos_labels = evaluate_dataset(model, pos_test_dataset, device)\n        pos_acc = (pos_preds == pos_labels).mean()\n        print(\"Neg: \")\n        neg_preds, neg_labels = evaluate_dataset(model, neg_test_dataset, device)\n        neg_acc = (neg_preds == neg_labels).mean()\n        print(\"Delta: \")\n        print(pos_acc - neg_acc)\n\n\ndef balance_concept_text_amazon_shoe(dataset, text_list, label_list, method, concept, explicit):\n    review_0, review_1, review_2, review_3, review_4 = [], [], [], [], []\n    for label, review in zip(label_list, text_list):\n        if label == 0:\n            review_0.append(review)\n        elif label == 1:\n            review_1.append(review)\n        elif label == 2:\n            review_2.append(review)\n        elif label == 3:\n            review_3.append(review)\n        else:\n            review_4.append(review)\n\n    if method == \"downsample\":\n        min_length = min(len(review_0), len(review_1), len(review_2), len(review_3), len(review_4))\n        print(min_length)\n        review_0 = sample(review_0, min_length)\n        review_1 = sample(review_1, min_length)\n        review_2 = sample(review_2, min_length)\n        review_3 = sample(review_3, min_length)\n        review_4 = sample(review_4, min_length)\n    elif method == \"upsample\":\n        data_file = f\"data/chatgpt_concepts_cf_{dataset}_{concept}_explicit.jsonl\"\n        max_length = max(len(review_0), len(review_1), len(review_2), len(review_3), len(review_4))\n        print(max_length)\n        sup_review_0, sup_review_1, sup_review_2, sup_review_3, sup_review_4 = [], [], [], [], []\n        with open(data_file, 'r') as inf:\n            for line in inf:\n                data = json.loads(line.strip())\n                review = data['cf_text']\n                label = int(data['label'])\n                if label == 0:\n                    sup_review_0.append(review)\n                elif label == 1:\n                    sup_review_1.append(review)\n                elif label == 2:\n                    sup_review_2.append(review)\n                elif label == 3:\n                    sup_review_3.append(review)\n                else:\n                    sup_review_4.append(review)\n        if max_length - len(review_0) <= len(sup_review_0):\n            review_0 += sample(sup_review_0, (max_length - len(review_0)))\n        else:\n            review_0 += sup_review_0\n\n        if max_length - len(review_1) <= len(sup_review_1):\n            review_1 += sample(sup_review_1, (max_length - len(review_1)))\n        else:\n            review_1 += sup_review_1\n\n        if max_length - len(review_2) <= len(sup_review_2):\n            review_2 += sample(sup_review_2, (max_length - len(review_2)))\n        else:\n            review_2 += sup_review_2\n\n        if max_length - len(review_3) <= len(sup_review_3):\n            review_3 += sample(sup_review_3, (max_length - len(review_3)))\n        else:\n            review_3 += sup_review_3\n\n        if max_length - len(review_4) <= len(sup_review_4):\n            review_4 += sample(sup_review_4, (max_length - len(review_4)))\n        else:\n            review_4 += sup_review_4\n\n    text_list = review_0 + review_1 + review_2 + review_3 + review_4\n    label_list = [0] * len(review_0) + [1] * len(review_1) + [2] * len(review_2) + [3] * len(review_3) + [4] * len(\n        review_4)\n    print({0: len(review_0), 1: len(review_1), 2: len(review_2), 3: len(review_3), 4: len(review_4)})\n    print(len(text_list))\n    return text_list, label_list\n\n\ndef balance_concept_text_imdb(dataset, text_list, label_list, method, concept, explicit):\n    review_0, review_1, = [], []\n    for label, review in zip(label_list, text_list):\n        if label == 0:\n            review_0.append(review)\n        elif label == 1:\n            review_1.append(review)\n\n    if method == \"downsample\":\n        min_length = min(len(review_0), len(review_1))\n        print(min_length)\n        review_0 = sample(review_0, min_length)\n        review_1 = sample(review_1, min_length)\n    elif method == \"upsample\":\n        data_file = f\"data/chatgpt_concepts_cf_{dataset}_{concept}_explicit.jsonl\"\n        max_length = max(len(review_0), len(review_1))\n        print(max_length)\n        sup_review_0, sup_review_1 = [], []\n        with open(data_file, 'r') as inf:\n            for line in inf:\n                data = json.loads(line.strip())\n                review = data['cf_text']\n                label = int(data['label'])\n                if label == 0:\n                    sup_review_0.append(review)\n                elif label == 1:\n                    sup_review_1.append(review)\n        review_0 += sample(sup_review_0, (max_length - len(review_0)))\n        review_1 += sample(sup_review_1, (max_length - len(review_1)))\n\n    text_list = review_0 + review_1\n    label_list = [0] * len(review_0) + [1] * len(review_1)\n    print({0: len(review_0), 1: len(review_1)})\n    print(len(text_list))\n    return text_list, label_list\n\n\ndef get_average_sentence_embedding(sentence, tokenizer, model, word):\n    if word not in sentence.lower():\n        return None\n    # print(sentence)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    inputs = tokenizer(sentence, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n    word_tokens = tokenizer.tokenize(word)\n    word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n    indices = [i for i, token_id in enumerate(inputs['input_ids'][0]) if token_id in word_ids]\n\n    # Extract embeddings and calculate the average\n\n    if indices:\n        with torch.no_grad():\n            outputs = model(**inputs)\n        word_embeddings = outputs.last_hidden_state[0, indices, :]\n        avg_embedding = torch.mean(word_embeddings, dim=0)\n        return avg_embedding\n    else:\n        return None\n\n\ndef train_original_dataset(dataset):\n    print(dataset)\n    output_dir = f'amazon_shoe_classification/concept/{dataset}_original'\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    text_list = []\n    label_list = []\n    with open(f\"data/chatgpt_concepts_{dataset}_exp.jsonl\",\n              'r') as inf:\n        for line in inf:\n            data = json.loads(line.strip())\n            text_list.append(data['text'])\n            label_list.append(data['label'])\n    print(len(text_list))\n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    print(device)\n    text_list, test_text, label_list, test_label = train_test_split(text_list, label_list,\n                                                                    test_size=2000, random_state=10)\n    train_text, train_label, valid_text, valid_label, train_dataset, val_dataset = \\\n        train_test_split_tokenize(text_list, label_list, tokenizer, output_dir, test_size=1000)\n    print(len(train_text))\n    model, training_args = train_model(train_label, train_dataset, val_dataset, output_dir, device)\n    model = AutoModelForSequenceClassification.from_pretrained(output_dir).to(device)\n    print(\"Test on reviews: \")\n    inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset=dataset)\n\n\ndef bias_concept(concept, dataset, concept_train_text, concept_train_label):\n    # bias training dataset\n    biased_concept_list = []\n    biased_label_list = []\n    for c, l in zip(concept_train_text, concept_train_label):\n        if dataset == \"amazon-shoe-reviews\":\n            if concept == \"size\":\n                if l == 0 or l == 1 or l == 2:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"color\" or concept == \"style\":\n                if l == 3 or l == 4:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        elif dataset == \"imdb\":\n            if l == 1:\n                biased_concept_list.append(c)\n                biased_label_list.append(l)\n        elif dataset == \"yelp_polarity\":\n            if concept == \"food\" or concept == \"price\":\n                if l == 1:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"service\":\n                if l == 0:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        elif dataset == \"cebab\":\n            if l == 3 or l == 4:\n                biased_concept_list.append(c)\n                biased_label_list.append(l)\n        elif dataset == \"boolq\":\n            if concept == \"country\":\n                if l == 0:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"television\" or concept == \"history\":\n                if l == 1:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        else:\n            raise ValueError(f'no such dataset {dataset}')\n\n    concept_train_text = biased_concept_list\n    concept_train_label = biased_label_list\n    return concept_train_text, concept_train_label\n\n\ndef train_specific_concept(dataset, concept, method, explicit):\n    print(dataset)\n    print(concept)\n    print(method)\n    output_dir = f'amazon_shoe_classification/concept/{dataset}_{concept}_{method}'\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    text_list = []\n    label_list = []\n\n    concept_text_list = []\n    concept_label_list = []\n\n    total_text = []\n    total_label = []\n    with open(f\"/kaggle/working/amazon_output_final.json\", 'r') as inf:\n        for line in inf:\n            data = json.loads(line.strip())\n            text_concepts = data['concepts'].lower().split(',')\n            text_concepts = [t.strip().lstrip() for t in text_concepts]\n            if dataset == \"boolq\":\n                if concept not in text_concepts:\n                    text_list.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                    label_list.append(data['label'])\n                else:\n                    concept_text_list.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                    concept_label_list.append(data['label'])\n\n                total_text.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                total_label.append(data['label'])\n            else:\n                if concept not in text_concepts:\n                    text_list.append(data['text'])\n                    label_list.append(data['label'])\n                else:\n                    concept_text_list.append(data['text'])\n                    concept_label_list.append(data['label'])\n\n                total_text.append(data['text'])\n                total_label.append(data['label'])\n\n    print(len(text_list))\n    print(len(concept_text_list))\n\n    if method == \"mask\":\n        words_to_remove = high_association_word(text_list, concept_text_list)\n        print(words_to_remove)\n        text_list, concept_text_list = mask_words(text_list, concept_text_list, words_to_remove)\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    print(device)\n\n    if dataset == \"amazon-shoe-reviews\":\n        total_test_number = 8000\n        valid_number = 1000\n    elif dataset == \"imdb\":\n        total_test_number = 4000\n        valid_number = 1000\n    elif dataset == \"yelp_polarity\":\n        total_test_number = 4000\n        valid_number = 1000\n    elif dataset == \"cebab\":\n        total_test_number = 2000\n        valid_number = 1000\n    elif dataset == \"boolq\":\n        total_test_number = 2000\n        valid_number = 200\n    else:\n        raise ValueError(f'no such dataset {dataset}')\n\n    text_list, test_text, label_list, test_label = train_test_split(total_text,\n                                                                    total_label,\n                                                                    test_size=total_test_number, random_state=10)\n    no_concept_train_text = []\n    no_concept_train_label = []\n    concept_train_text = []\n    concept_train_label = []\n\n    no_concept_test_text = []\n    no_concept_test_label = []\n    concept_test_text = []\n    concept_test_label = []\n\n    for r, l in zip(text_list, label_list):\n        if r in concept_text_list:\n            concept_train_text.append(r)\n            concept_train_label.append(l)\n        else:\n            no_concept_train_text.append(r)\n            no_concept_train_label.append(l)\n\n    for r, l in zip(test_text, test_label):\n        if r in concept_text_list:\n            concept_test_text.append(r)\n            concept_test_label.append(l)\n        else:\n            no_concept_test_text.append(r)\n            no_concept_test_label.append(l)\n\n    print(\"total training number: \")\n    print(len(text_list))\n    print(\"training concept number: \")\n    print(len(concept_train_text))\n    print(\"training concept distribution: \")\n    print(Counter(concept_train_label))\n    print(\"training no concept number: \")\n    print(len(no_concept_train_text))\n\n    print(\"total test number: \")\n    print(len(test_text))\n    print(\"test concept number: \")\n    print(len(concept_test_text))\n    print(\"test concept distribution: \")\n    print(Counter(concept_test_label))\n    print(\"test no concept number: \")\n    print(len(no_concept_test_text))\n\n    if method == \"downsample\" or method == \"upsample\":\n        print(\"balanced training concept dataset: \")\n        if dataset == \"amazon-shoe-reviews\" or dataset == \"cebab\":\n            concept_train_text, concept_train_label = balance_concept_text_amazon_shoe(dataset,\n                                                                                       concept_train_text,\n                                                                                       concept_train_label,\n                                                                                       method=method,\n                                                                                       concept=concept,\n                                                                                       explicit=explicit)\n        else:\n            concept_train_text, concept_train_label = balance_concept_text_imdb(dataset, concept_train_text,\n                                                                                concept_train_label,\n                                                                                method=method,\n                                                                                concept=concept, explicit=explicit)\n\n    if method == \"biased\":\n        concept_train_text, concept_train_label = bias_concept(concept, dataset, concept_train_text,\n                                                               concept_train_label)\n\n    print(\"After processing\")\n    print(\"training concept distribution: \")\n    print(Counter(concept_train_label))\n    print(\"# of train + valid dataset: \")\n    print(len(concept_train_text + no_concept_train_text))\n\n    train_text = concept_train_text + no_concept_train_text\n    train_label = concept_train_label + no_concept_train_label\n\n    zipped = list(zip(train_text, train_label))\n    random.shuffle(zipped)\n    train_text, train_label = zip(*zipped)\n\n    train_text, train_label, valid_text, valid_label, train_dataset, val_dataset = \\\n        train_test_split_tokenize(train_text,\n                                  train_label, tokenizer, output_dir,\n                                  test_size=valid_number)\n\n    model, training_args = train_model(train_label, train_dataset, val_dataset, output_dir, device)\n    model = AutoModelForSequenceClassification.from_pretrained(output_dir).to(device)\n\n    evaluate_dataset(model, train_dataset, device)\n    print(\"Test on reviews total: \")\n    inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset)\n    print(\"Test on reviews wo concepts: \")\n    inference_pos_neg(no_concept_test_text, no_concept_test_label, model, tokenizer, device, dataset)\n    print(\"Test on reviews with concepts: \")\n    inference_pos_neg(concept_test_text, concept_test_label, model, tokenizer, device, dataset)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset', type=str, default=\"amazon-shoe-reviews\")\n    parser.add_argument('--concept', type=str, default=\"size\")\n    parser.add_argument('--method', type=str, default=\"original\")\n    args, _ = parser.parse_known_args()\n    train_specific_concept(dataset=args.dataset, concept=args.concept, method=args.method, explicit=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T22:02:32.054039Z","iopub.execute_input":"2024-10-10T22:02:32.054674Z","iopub.status.idle":"2024-10-10T23:57:08.868680Z","shell.execute_reply.started":"2024-10-10T22:02:32.054637Z","shell.execute_reply":"2024-10-10T23:57:08.867763Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"amazon-shoe-reviews\nsize\noriginal\n49483\n28634\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a1ed49171674248bf021a11613955c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f133b95593144daa808a977b9efef90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87565f150d7b4fb6bbe200f48a56e17e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bce3864658d4249805d72cfd3fa558b"}},"metadata":{}},{"name":"stdout","text":"cuda\ntotal training number: \n70117\ntraining concept number: \n25690\ntraining concept distribution: \nCounter({2: 6575, 1: 6402, 0: 4999, 3: 4925, 4: 2789})\ntraining no concept number: \n44427\ntotal test number: \n8000\ntest concept number: \n2945\ntest concept distribution: \nCounter({2: 732, 1: 720, 3: 589, 0: 549, 4: 355})\ntest no concept number: \n5055\nAfter processing\ntraining concept distribution: \nCounter({2: 6575, 1: 6402, 0: 4999, 3: 4925, 4: 2789})\n# of train + valid dataset: \n70117\nTraining dataset length\n69117\nValid dataset length\n1000\n{0, 1, 2, 3, 4}\n{0, 1, 2, 3, 4}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"331ca7e868ad455999429ff967c413c6"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae0fb8853ad14c74a8dafbaad661c7fe"}},"metadata":{}},{"name":"stdout","text":"Number of classes\n5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241010_220430-futpsk97</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ali-zhunis-xai/huggingface/runs/futpsk97' target=\"_blank\">curious-gorge-41</a></strong> to <a href='https://wandb.ai/ali-zhunis-xai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ali-zhunis-xai/huggingface' target=\"_blank\">https://wandb.ai/ali-zhunis-xai/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ali-zhunis-xai/huggingface/runs/futpsk97' target=\"_blank\">https://wandb.ai/ali-zhunis-xai/huggingface/runs/futpsk97</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12960' max='12960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12960/12960 1:36:46, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.128900</td>\n      <td>1.071009</td>\n      <td>0.528000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.058700</td>\n      <td>1.022068</td>\n      <td>0.535000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.025900</td>\n      <td>1.022335</td>\n      <td>0.534000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.033600</td>\n      <td>0.991565</td>\n      <td>0.562000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.000300</td>\n      <td>1.008850</td>\n      <td>0.553000</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.005800</td>\n      <td>1.012898</td>\n      <td>0.544000</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.995500</td>\n      <td>1.002874</td>\n      <td>0.561000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.992900</td>\n      <td>1.007083</td>\n      <td>0.560000</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.970400</td>\n      <td>1.010404</td>\n      <td>0.555000</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.890800</td>\n      <td>0.988696</td>\n      <td>0.560000</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.901900</td>\n      <td>0.991989</td>\n      <td>0.569000</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.912900</td>\n      <td>0.971319</td>\n      <td>0.569000</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.895300</td>\n      <td>1.010686</td>\n      <td>0.568000</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.878900</td>\n      <td>0.994580</td>\n      <td>0.568000</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.903900</td>\n      <td>0.968287</td>\n      <td>0.582000</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.924500</td>\n      <td>0.972851</td>\n      <td>0.584000</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.893800</td>\n      <td>0.969659</td>\n      <td>0.586000</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.807900</td>\n      <td>0.984876</td>\n      <td>0.589000</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.826900</td>\n      <td>1.002391</td>\n      <td>0.574000</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.788900</td>\n      <td>0.990552</td>\n      <td>0.588000</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.803400</td>\n      <td>0.978200</td>\n      <td>0.579000</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.788100</td>\n      <td>1.004000</td>\n      <td>0.578000</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.786900</td>\n      <td>0.999340</td>\n      <td>0.581000</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.823400</td>\n      <td>0.984527</td>\n      <td>0.578000</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.796400</td>\n      <td>0.991727</td>\n      <td>0.585000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2245: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(checkpoint_file, map_location=\"cpu\")\n","output_type":"stream"},{"name":"stdout","text":"100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\n2100\n2200\n2300\n2400\n2500\n2600\n2700\n2800\n2900\n3000\n3100\n3200\n3300\n3400\n3500\n3600\n3700\n3800\n3900\n4000\n4100\n4200\n4300\nlogits size torch.Size([69117, 5])\nacc 0.6566401898230536\nTest on reviews total: \n100\n200\n300\n400\n500\nlogits size torch.Size([8000, 5])\nacc 0.57125\nLabel 4: \n100\nlogits size torch.Size([1622, 5])\nacc 0.7564734895191122\nLabel 3: \n100\nlogits size torch.Size([1590, 5])\nacc 0.4817610062893082\nLabel 2: \n100\nlogits size torch.Size([1612, 5])\nacc 0.45471464019851116\nLabel 1: \nlogits size torch.Size([1571, 5])\nacc 0.49267982176957353\nLabel 0: \n100\nlogits size torch.Size([1605, 5])\nacc 0.6666666666666666\nDelta: \n0.03373896604492515\nRobust acc: \n0.5704591248886344\nTest on reviews wo concepts: \n100\n200\n300\nlogits size torch.Size([5055, 5])\nacc 0.5907022749752721\nLabel 4: \nlogits size torch.Size([1267, 5])\nacc 0.7908445146014207\nLabel 3: \nlogits size torch.Size([1001, 5])\nacc 0.44955044955044954\nLabel 2: \nlogits size torch.Size([880, 5])\nacc 0.43522727272727274\nLabel 1: \nlogits size torch.Size([851, 5])\nacc 0.44770857814336074\nLabel 0: \nlogits size torch.Size([1056, 5])\nacc 0.7291666666666666\nDelta: \n0.025039513455319396\nRobust acc: \n0.570499496337834\nTest on reviews with concepts: \n100\nlogits size torch.Size([2945, 5])\nacc 0.5378607809847199\nLabel 4: \nlogits size torch.Size([355, 5])\nacc 0.6338028169014085\nLabel 3: \nlogits size torch.Size([589, 5])\nacc 0.5365025466893039\nLabel 2: \nlogits size torch.Size([732, 5])\nacc 0.4781420765027322\nLabel 1: \nlogits size torch.Size([720, 5])\nacc 0.5458333333333333\nLabel 0: \nlogits size torch.Size([549, 5])\nacc 0.546448087431694\nDelta: \n0.033075734459079895\nRobust acc: \n0.5481457721716944\nError in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7cd4184f6dd0>> (for post_run_cell), with arguments args (<ExecutionResult object at 7cd6a85e8a00, execution_count=18 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7cd6a85e8310, raw_cell=\"import torch\nimport random\nimport numpy as np\nrand..\" store_history=True silent=False shell_futures=True cell_id=bcf6794a-134b-4d7c-ba7d-8b8e0df0a640> result=None>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._pause_backend() takes 1 positional argument but 2 were given","output_type":"error"}]},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\nrandom.seed(10)\nnp.random.seed(10)\ntorch.manual_seed(10)\ntorch.cuda.manual_seed(10)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nMODEL = \"distilbert-base-uncased\"\n\n\nclass TrainValidDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n\ndef high_association_word(text_list, concept_text_list):\n    concept_word_occurrence = {\"concept\": {}, \"no_concept\": {}}\n    pmi_words = []\n    for t in concept_text_list:\n        tokens = list(set(t.split()))\n        for token in tokens:\n            token = token.lower().replace(\".\", \"\").replace(\",\", \"\")\n            if token in concept_word_occurrence['concept']:\n                concept_word_occurrence['concept'][token] += 1\n            else:\n                concept_word_occurrence['concept'][token] = 1\n\n    for t in text_list:\n        tokens = list(set(t.split()))\n        for token in tokens:\n            token = token.lower().replace(\".\", \"\").replace(\",\", \"\")\n            if token in concept_word_occurrence['no_concept']:\n                concept_word_occurrence['no_concept'][token] += 1\n            else:\n                concept_word_occurrence['no_concept'][token] = 1\n\n    total_number = len(text_list) + len(concept_text_list)\n    concept_number = len(concept_text_list)\n    p_concept = concept_number / total_number\n\n    for token in concept_word_occurrence['concept']:\n        token = token.lower()\n        if token in concept_word_occurrence['no_concept']:\n            p_token = (concept_word_occurrence['concept'][token] + concept_word_occurrence['no_concept'][\n                token]) / total_number\n            p_token_concept = concept_word_occurrence['concept'][token] / total_number\n            pmi_token = math.log(p_token_concept / (p_token * p_concept))\n            pmi_words.append((token, pmi_token))\n\n    pmi_words = sorted(pmi_words, key=lambda x: x[1], reverse=True)[: 20]\n    return [i[0] for i in pmi_words]\n\n\ndef mask_words(text_list, concept_text_list, words_to_remove):\n    # MASK_TOKEN = \"[MASK]\"\n    MASK_TOKEN = \"<unk>\"\n    masked_text_list = []\n    for review in text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower() in words_to_remove:\n                masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_text_list.append(\" \".join(masked_tokens))\n\n    masked_concept_text_list = []\n    for review in concept_text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower() in words_to_remove:\n                masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_concept_text_list.append(\" \".join(masked_tokens))\n    return masked_text_list, masked_concept_text_list\n\n\ndef mask_out_name(text_list, mask_name):\n    MASK_TOKEN = \"[MASK]\"\n    masked_text_list = []\n    for review in text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower().replace(\".\", \"\").replace(\",\", \"\") == mask_name.lower():\n                if \".\" in token:\n                    masked_tokens.append(MASK_TOKEN + \".\")\n                elif \",\" in token:\n                    masked_tokens.append(MASK_TOKEN + \",\")\n                else:\n                    masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_text_list.append(\" \".join(masked_tokens))\n    return masked_text_list\n\n\ndef prepare_cf_train_dataset(dataset_name, tokenizer, train_text, train_label):\n    with open(dataset_name + 'train_text_changed_Burberry_cf.txt') as f:\n        train_text_changed_burberry = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_text_changed_Chanel_cf.txt') as f:\n        train_text_changed_chanel = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_text_changed_Dior_cf.txt') as f:\n        train_text_changed_dior = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_label_changed.txt') as f:\n        train_label_changed = [int(line.rstrip()) for line in f]\n\n    pos_train_label_changed = []\n    pos_train_text_burberry = []\n    pos_train_text_chanel = []\n    pos_train_text_dior = []\n\n    for index, label in enumerate(train_label_changed):\n        if label >= 2:\n            pos_train_label_changed.append(label)\n            pos_train_text_burberry.append(train_text_changed_burberry[index])\n            pos_train_text_chanel.append(train_text_changed_chanel[index])\n            pos_train_text_dior.append(train_text_changed_dior[index])\n\n    print(\"Insert number: \")\n    print(len(pos_train_text_burberry + pos_train_text_chanel + pos_train_text_dior))\n    cf_text = train_text + pos_train_text_burberry + pos_train_text_chanel + pos_train_text_dior\n    cf_label = train_label + pos_train_label_changed + pos_train_label_changed + pos_train_label_changed\n    zipped = list(zip(cf_text, cf_label))\n    random.shuffle(zipped)\n    cf_text, cf_label = zip(*zipped)\n    print(\"CF dataset length\")\n    print(len(cf_label))\n\n    cf_encodings = tokenizer(cf_text, truncation=True, padding=True)\n    cf_train_dataset = TrainValidDataset(cf_encodings, cf_label)\n    return cf_train_dataset, cf_label\n\n\ndef tokenize_cf_dataset(test_text_changed_ori, test_text_changed_burberry, test_text_changed_chanel,\n                        test_text_changed_dior, test_text_changed_gucci, test_text_changed_prada,\n                        tokenizer, test_label_changed, pos_test_label_changed):\n    test_changed_encodings = tokenizer(test_text_changed_ori, truncation=True, padding=True)\n    test_changed_burberry_encodings = tokenizer(test_text_changed_burberry, truncation=True, padding=True)\n    test_changed_chanel_encodings = tokenizer(test_text_changed_chanel, truncation=True, padding=True)\n    test_changed_dior_encodings = tokenizer(test_text_changed_dior, truncation=True, padding=True)\n    test_changed_gucci_encodings = tokenizer(test_text_changed_gucci, truncation=True, padding=True)\n    test_changed_prada_encodings = tokenizer(test_text_changed_prada, truncation=True, padding=True)\n\n    test_changed_dataset = TrainValidDataset(test_changed_encodings, test_label_changed)\n    test_changed_burberry = TrainValidDataset(test_changed_burberry_encodings, pos_test_label_changed)\n    test_changed_chanel = TrainValidDataset(test_changed_chanel_encodings, pos_test_label_changed)\n    test_changed_dior = TrainValidDataset(test_changed_dior_encodings, pos_test_label_changed)\n    test_changed_gucci = TrainValidDataset(test_changed_gucci_encodings, pos_test_label_changed)\n    test_changed_prada = TrainValidDataset(test_changed_prada_encodings, pos_test_label_changed)\n\n    return test_changed_dataset, test_changed_burberry, test_changed_chanel, \\\n           test_changed_dior, test_changed_gucci, test_changed_prada\n\n\ndef prepare_cf_test_dataset(dataset_name, tokenizer):\n    with open(dataset_name + 'test_text_changed_ori.txt') as f:\n        test_text_changed_ori = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Burberry_cf.txt') as f:\n        test_text_changed_burberry = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Chanel_cf.txt') as f:\n        test_text_changed_chanel = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Dior_cf.txt') as f:\n        test_text_changed_dior = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Gucci_cf.txt') as f:\n        test_text_changed_gucci = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Prada_cf.txt') as f:\n        test_text_changed_prada = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_label_changed.txt') as f:\n        test_label_changed = [int(line.rstrip()) for line in f]\n\n    # test_text_changed_burberry = mask_out_name(test_text_changed_burberry, \"Burberry\")\n    # test_text_changed_chanel = mask_out_name(test_text_changed_chanel, \"Chanel\")\n    # test_text_changed_dior = mask_out_name(test_text_changed_dior, \"Dior\")\n    # test_text_changed_gucci = mask_out_name(test_text_changed_gucci, \"Gucci\")\n    # test_text_changed_prada = mask_out_name(test_text_changed_prada, \"Prada\")\n\n    # print(test_text_changed_prada[: 10])\n\n    pos_test_label_changed = []\n    pos_test_text_burberry = []\n    pos_test_text_chanel = []\n    pos_test_text_dior = []\n    pos_test_text_gucci = []\n    pos_test_text_prada = []\n\n    neg_test_label_changed = []\n    neg_test_text_burberry = []\n    neg_test_text_chanel = []\n    neg_test_text_dior = []\n    neg_test_text_gucci = []\n    neg_test_text_prada = []\n\n    for index, label in enumerate(test_label_changed):\n        if label >= 2:\n            pos_test_label_changed.append(label)\n            pos_test_text_burberry.append(test_text_changed_burberry[index])\n            pos_test_text_chanel.append(test_text_changed_chanel[index])\n            pos_test_text_dior.append(test_text_changed_dior[index])\n            pos_test_text_gucci.append(test_text_changed_gucci[index])\n            pos_test_text_prada.append(test_text_changed_prada[index])\n        else:\n            neg_test_label_changed.append(label)\n            neg_test_text_burberry.append(test_text_changed_burberry[index])\n            neg_test_text_chanel.append(test_text_changed_chanel[index])\n            neg_test_text_dior.append(test_text_changed_dior[index])\n            neg_test_text_gucci.append(test_text_changed_gucci[index])\n            neg_test_text_prada.append(test_text_changed_prada[index])\n\n    return tokenize_cf_dataset(test_text_changed_ori, test_text_changed_burberry, test_text_changed_chanel,\n                               test_text_changed_dior, test_text_changed_gucci, test_text_changed_prada,\n                               tokenizer, test_label_changed, test_label_changed)\n\n\ndef train_test_split_tokenize(text_list, label_list, tokenizer, output_dir, test_size=10000):\n    train_text, valid_text, train_label, valid_label = train_test_split(text_list, label_list, test_size=test_size,\n                                                                        random_state=10)\n    print(\"Training dataset length\")\n    print(len(train_text))\n    print(\"Valid dataset length\")\n    print(len(valid_text))\n    print(set(train_label))\n    print(set(valid_label))\n\n    train_encodings = tokenizer(train_text, truncation=True, padding=True)\n    val_encodings = tokenizer(valid_text, truncation=True, padding=True)\n    tokenizer.save_pretrained(output_dir)\n\n    train_dataset = TrainValidDataset(train_encodings, train_label)\n    val_dataset = TrainValidDataset(val_encodings, valid_label)\n    return train_text, train_label, valid_text, valid_label, train_dataset, val_dataset\n\n\ndef train_model(train_label, train_dataset, val_dataset, output_dir, device):\n    model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=len(list(set(train_label)))).to(device)\n    training_args = TrainingArguments(\n        output_dir=output_dir,  # output directory\n        learning_rate=2e-5,\n        num_train_epochs=3,  # total number of training epochs\n        per_device_train_batch_size=16,  # batch size per device during training\n        per_device_eval_batch_size=16,  # batch size for evaluation\n        weight_decay=0.01,  # strength of weight decay\n        logging_dir=output_dir,  # directory for storing logs\n        logging_steps=200,\n        evaluation_strategy=\"steps\",\n        save_total_limit=1,\n        load_best_model_at_end=True,\n        save_strategy=\"steps\",\n        eval_steps=500,\n        save_steps=500,\n\n    )\n    metric = evaluate.load(\"accuracy\")\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return metric.compute(predictions=predictions, references=labels)\n\n    print(\"Number of classes\")\n    print(len(list(set(train_label))))\n\n    trainer = Trainer(\n        model=model,  # the instantiated 🤗 Transformers model to be trained\n        args=training_args,  # training arguments, defined above\n        train_dataset=train_dataset,  # training dataset\n        eval_dataset=val_dataset,  # evaluation dataset\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n    trainer.save_model()\n    return model, training_args\n\n\ndef evaluate_dataset(model, eval_dataset, device):\n    model.eval()\n    eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False)\n    attention_weights = []\n    logits = []\n    loss_lists = []\n    batch_number = 0\n    for batch in eval_loader:\n        with torch.no_grad():\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels, output_attentions=True)\n            # attention_weights_ = torch.mean(outputs[-1][-1], dim=1, keepdim=False)[:, 0, :]\n            logits_ = outputs[1]\n            loss = outputs[0]\n            # attention_weights.append(attention_weights_)\n            logits.append(logits_)\n            loss_lists.append(loss.item())\n            if (batch_number + 1) % 100 == 0:\n                print(batch_number + 1)\n            batch_number += 1\n\n    # attention_weights = torch.cat(attention_weights, dim=0)\n    logits = torch.cat(logits, dim=0)\n\n    # print(\"attention weights size: \", attention_weights.size())\n    print(\"logits size\", logits.size())\n    preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n    print(\"acc\", (preds == eval_dataset.labels).mean())\n    return preds, eval_dataset.labels\n\n\ndef inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset=\"amazon-shoe-reviews\"):\n    test_encodings = tokenizer(test_text, truncation=True, padding=True)\n    test_dataset = TrainValidDataset(test_encodings, test_label)\n    evaluate_dataset(model, test_dataset, device)\n\n    if dataset == \"amazon-shoe-reviews\" or dataset == \"cebab\":\n        test_text_0 = []\n        test_label_0 = []\n\n        test_text_1 = []\n        test_label_1 = []\n\n        test_text_2 = []\n        test_label_2 = []\n\n        test_text_3 = []\n        test_label_3 = []\n\n        test_text_4 = []\n        test_label_4 = []\n        for t, label in zip(test_text, test_label):\n            if label == 0:\n                test_text_0.append(t)\n                test_label_0.append(label)\n            elif label == 1:\n                test_text_1.append(t)\n                test_label_1.append(label)\n            elif label == 2:\n                test_text_2.append(t)\n                test_label_2.append(label)\n            elif label == 3:\n                test_text_3.append(t)\n                test_label_3.append(label)\n            else:\n                test_text_4.append(t)\n                test_label_4.append(label)\n\n        test_encodings_4 = tokenizer(test_text_4, truncation=True, padding=True)\n        test_dataset_4 = TrainValidDataset(test_encodings_4, test_label_4)\n        test_encodings_3 = tokenizer(test_text_3, truncation=True, padding=True)\n        test_dataset_3 = TrainValidDataset(test_encodings_3, test_label_3)\n        test_encodings_2 = tokenizer(test_text_2, truncation=True, padding=True)\n        test_dataset_2 = TrainValidDataset(test_encodings_2, test_label_2)\n        test_encodings_1 = tokenizer(test_text_1, truncation=True, padding=True)\n        test_dataset_1 = TrainValidDataset(test_encodings_1, test_label_1)\n        test_encodings_0 = tokenizer(test_text_0, truncation=True, padding=True)\n        test_dataset_0 = TrainValidDataset(test_encodings_0, test_label_0)\n\n        print(\"Label 4: \")\n        preds_4, labels_4 = evaluate_dataset(model, test_dataset_4, device)\n        acc_4 = (preds_4 == labels_4).mean()\n        print(\"Label 3: \")\n        preds_3, labels_3 = evaluate_dataset(model, test_dataset_3, device)\n        acc_3 = (preds_3 == labels_3).mean()\n        print(\"Label 2: \")\n        preds_2, labels_2 = evaluate_dataset(model, test_dataset_2, device)\n        acc_2 = (preds_2 == labels_2).mean()\n        print(\"Label 1: \")\n        preds_1, labels_1 = evaluate_dataset(model, test_dataset_1, device)\n        acc_1 = (preds_1 == labels_1).mean()\n        print(\"Label 0: \")\n        preds_0, labels_0 = evaluate_dataset(model, test_dataset_0, device)\n        acc_0 = (preds_0 == labels_0).mean()\n        print(\"Delta: \")\n        delta = ((acc_4 - acc_0) + (acc_4 - acc_1) + (acc_4 - acc_2) + (acc_4 - acc_3) + (acc_3 - acc_0) + (\n                acc_3 - acc_1) + (acc_3 - acc_2) + (acc_2 - acc_0) + (acc_2 - acc_1) + (acc_1 - acc_0)) / 10\n        print(delta)\n        print(\"Robust acc: \")\n        print((acc_4 + acc_3 + acc_2 + acc_1 + acc_0) / 5)\n\n    else:\n        pos_test_text = []\n        pos_test_label = []\n\n        neg_test_text = []\n        neg_test_label = []\n\n        for t, label in zip(test_text, test_label):\n            if label == 1:\n                pos_test_text.append(t)\n                pos_test_label.append(label)\n            else:\n                neg_test_text.append(t)\n                neg_test_label.append(label)\n\n        pos_test_encodings = tokenizer(pos_test_text, truncation=True, padding=True)\n        pos_test_dataset = TrainValidDataset(pos_test_encodings, pos_test_label)\n\n        neg_test_encodings = tokenizer(neg_test_text, truncation=True, padding=True)\n        neg_test_dataset = TrainValidDataset(neg_test_encodings, neg_test_label)\n        print(\"Pos: \")\n        pos_preds, pos_labels = evaluate_dataset(model, pos_test_dataset, device)\n        pos_acc = (pos_preds == pos_labels).mean()\n        print(\"Neg: \")\n        neg_preds, neg_labels = evaluate_dataset(model, neg_test_dataset, device)\n        neg_acc = (neg_preds == neg_labels).mean()\n        print(\"Delta: \")\n        print(pos_acc - neg_acc)\n\n\ndef balance_concept_text_amazon_shoe(dataset, text_list, label_list, method, concept, explicit):\n    review_0, review_1, review_2, review_3, review_4 = [], [], [], [], []\n    for label, review in zip(label_list, text_list):\n        if label == 0:\n            review_0.append(review)\n        elif label == 1:\n            review_1.append(review)\n        elif label == 2:\n            review_2.append(review)\n        elif label == 3:\n            review_3.append(review)\n        else:\n            review_4.append(review)\n\n    if method == \"downsample\":\n        min_length = min(len(review_0), len(review_1), len(review_2), len(review_3), len(review_4))\n        print(min_length)\n        review_0 = sample(review_0, min_length)\n        review_1 = sample(review_1, min_length)\n        review_2 = sample(review_2, min_length)\n        review_3 = sample(review_3, min_length)\n        review_4 = sample(review_4, min_length)\n    elif method == \"upsample\":\n        data_file = f\"data/chatgpt_concepts_cf_{dataset}_{concept}_explicit.jsonl\"\n        max_length = max(len(review_0), len(review_1), len(review_2), len(review_3), len(review_4))\n        print(max_length)\n        sup_review_0, sup_review_1, sup_review_2, sup_review_3, sup_review_4 = [], [], [], [], []\n        with open(data_file, 'r') as inf:\n            for line in inf:\n                data = json.loads(line.strip())\n                review = data['cf_text']\n                label = int(data['label'])\n                if label == 0:\n                    sup_review_0.append(review)\n                elif label == 1:\n                    sup_review_1.append(review)\n                elif label == 2:\n                    sup_review_2.append(review)\n                elif label == 3:\n                    sup_review_3.append(review)\n                else:\n                    sup_review_4.append(review)\n        if max_length - len(review_0) <= len(sup_review_0):\n            review_0 += sample(sup_review_0, (max_length - len(review_0)))\n        else:\n            review_0 += sup_review_0\n\n        if max_length - len(review_1) <= len(sup_review_1):\n            review_1 += sample(sup_review_1, (max_length - len(review_1)))\n        else:\n            review_1 += sup_review_1\n\n        if max_length - len(review_2) <= len(sup_review_2):\n            review_2 += sample(sup_review_2, (max_length - len(review_2)))\n        else:\n            review_2 += sup_review_2\n\n        if max_length - len(review_3) <= len(sup_review_3):\n            review_3 += sample(sup_review_3, (max_length - len(review_3)))\n        else:\n            review_3 += sup_review_3\n\n        if max_length - len(review_4) <= len(sup_review_4):\n            review_4 += sample(sup_review_4, (max_length - len(review_4)))\n        else:\n            review_4 += sup_review_4\n\n    text_list = review_0 + review_1 + review_2 + review_3 + review_4\n    label_list = [0] * len(review_0) + [1] * len(review_1) + [2] * len(review_2) + [3] * len(review_3) + [4] * len(\n        review_4)\n    print({0: len(review_0), 1: len(review_1), 2: len(review_2), 3: len(review_3), 4: len(review_4)})\n    print(len(text_list))\n    return text_list, label_list\n\n\ndef balance_concept_text_imdb(dataset, text_list, label_list, method, concept, explicit):\n    review_0, review_1, = [], []\n    for label, review in zip(label_list, text_list):\n        if label == 0:\n            review_0.append(review)\n        elif label == 1:\n            review_1.append(review)\n\n    if method == \"downsample\":\n        min_length = min(len(review_0), len(review_1))\n        print(min_length)\n        review_0 = sample(review_0, min_length)\n        review_1 = sample(review_1, min_length)\n    elif method == \"upsample\":\n        data_file = f\"data/chatgpt_concepts_cf_{dataset}_{concept}_explicit.jsonl\"\n        max_length = max(len(review_0), len(review_1))\n        print(max_length)\n        sup_review_0, sup_review_1 = [], []\n        with open(data_file, 'r') as inf:\n            for line in inf:\n                data = json.loads(line.strip())\n                review = data['cf_text']\n                label = int(data['label'])\n                if label == 0:\n                    sup_review_0.append(review)\n                elif label == 1:\n                    sup_review_1.append(review)\n        review_0 += sample(sup_review_0, (max_length - len(review_0)))\n        review_1 += sample(sup_review_1, (max_length - len(review_1)))\n\n    text_list = review_0 + review_1\n    label_list = [0] * len(review_0) + [1] * len(review_1)\n    print({0: len(review_0), 1: len(review_1)})\n    print(len(text_list))\n    return text_list, label_list\n\n\ndef get_average_sentence_embedding(sentence, tokenizer, model, word):\n    if word not in sentence.lower():\n        return None\n    # print(sentence)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    inputs = tokenizer(sentence, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n    word_tokens = tokenizer.tokenize(word)\n    word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n    indices = [i for i, token_id in enumerate(inputs['input_ids'][0]) if token_id in word_ids]\n\n    # Extract embeddings and calculate the average\n\n    if indices:\n        with torch.no_grad():\n            outputs = model(**inputs)\n        word_embeddings = outputs.last_hidden_state[0, indices, :]\n        avg_embedding = torch.mean(word_embeddings, dim=0)\n        return avg_embedding\n    else:\n        return None\n\n\ndef train_original_dataset(dataset):\n    print(dataset)\n    output_dir = f'amazon_shoe_classification/concept/{dataset}_original'\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    text_list = []\n    label_list = []\n    with open(f\"data/chatgpt_concepts_{dataset}_exp.jsonl\",\n              'r') as inf:\n        for line in inf:\n            data = json.loads(line.strip())\n            text_list.append(data['text'])\n            label_list.append(data['label'])\n    print(len(text_list))\n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    print(device)\n    text_list, test_text, label_list, test_label = train_test_split(text_list, label_list,\n                                                                    test_size=2000, random_state=10)\n    train_text, train_label, valid_text, valid_label, train_dataset, val_dataset = \\\n        train_test_split_tokenize(text_list, label_list, tokenizer, output_dir, test_size=1000)\n    print(len(train_text))\n    model, training_args = train_model(train_label, train_dataset, val_dataset, output_dir, device)\n    model = AutoModelForSequenceClassification.from_pretrained(output_dir).to(device)\n    print(\"Test on reviews: \")\n    inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset=dataset)\n\n\ndef bias_concept(concept, dataset, concept_train_text, concept_train_label):\n    # bias training dataset\n    biased_concept_list = []\n    biased_label_list = []\n    for c, l in zip(concept_train_text, concept_train_label):\n        if dataset == \"amazon-shoe-reviews\":\n            if concept == \"size\":\n                if l == 0 or l == 1 or l == 2:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"color\" or concept == \"style\":\n                if l == 3 or l == 4:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        elif dataset == \"imdb\":\n            if l == 1:\n                biased_concept_list.append(c)\n                biased_label_list.append(l)\n        elif dataset == \"yelp_polarity\":\n            if concept == \"food\" or concept == \"price\":\n                if l == 1:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"service\":\n                if l == 0:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        elif dataset == \"cebab\":\n            if l == 3 or l == 4:\n                biased_concept_list.append(c)\n                biased_label_list.append(l)\n        elif dataset == \"boolq\":\n            if concept == \"country\":\n                if l == 0:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"television\" or concept == \"history\":\n                if l == 1:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        else:\n            raise ValueError(f'no such dataset {dataset}')\n\n    concept_train_text = biased_concept_list\n    concept_train_label = biased_label_list\n    return concept_train_text, concept_train_label\n\n\ndef train_specific_concept(dataset, concept, method, explicit):\n    print(dataset)\n    print(concept)\n    print(method)\n    output_dir = f'amazon_shoe_classification/concept/{dataset}_{concept}_{method}'\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    text_list = []\n    label_list = []\n\n    concept_text_list = []\n    concept_label_list = []\n\n    total_text = []\n    total_label = []\n    with open(f\"/kaggle/working/amazon_output_final.json\", 'r') as inf:\n        for line in inf:\n            data = json.loads(line.strip())\n            text_concepts = data['concepts'].lower().split(',')\n            text_concepts = [t.strip().lstrip() for t in text_concepts]\n            if dataset == \"boolq\":\n                if concept not in text_concepts:\n                    text_list.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                    label_list.append(data['label'])\n                else:\n                    concept_text_list.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                    concept_label_list.append(data['label'])\n\n                total_text.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                total_label.append(data['label'])\n            else:\n                if concept not in text_concepts:\n                    text_list.append(data['text'])\n                    label_list.append(data['label'])\n                else:\n                    concept_text_list.append(data['text'])\n                    concept_label_list.append(data['label'])\n\n                total_text.append(data['text'])\n                total_label.append(data['label'])\n\n    print(len(text_list))\n    print(len(concept_text_list))\n\n    if method == \"mask\":\n        words_to_remove = high_association_word(text_list, concept_text_list)\n        print(words_to_remove)\n        text_list, concept_text_list = mask_words(text_list, concept_text_list, words_to_remove)\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    print(device)\n\n    if dataset == \"amazon-shoe-reviews\":\n        total_test_number = 8000\n        valid_number = 1000\n    elif dataset == \"imdb\":\n        total_test_number = 4000\n        valid_number = 1000\n    elif dataset == \"yelp_polarity\":\n        total_test_number = 4000\n        valid_number = 1000\n    elif dataset == \"cebab\":\n        total_test_number = 2000\n        valid_number = 1000\n    elif dataset == \"boolq\":\n        total_test_number = 2000\n        valid_number = 200\n    else:\n        raise ValueError(f'no such dataset {dataset}')\n\n    text_list, test_text, label_list, test_label = train_test_split(total_text,\n                                                                    total_label,\n                                                                    test_size=total_test_number, random_state=10)\n    no_concept_train_text = []\n    no_concept_train_label = []\n    concept_train_text = []\n    concept_train_label = []\n\n    no_concept_test_text = []\n    no_concept_test_label = []\n    concept_test_text = []\n    concept_test_label = []\n\n    for r, l in zip(text_list, label_list):\n        if r in concept_text_list:\n            concept_train_text.append(r)\n            concept_train_label.append(l)\n        else:\n            no_concept_train_text.append(r)\n            no_concept_train_label.append(l)\n\n    for r, l in zip(test_text, test_label):\n        if r in concept_text_list:\n            concept_test_text.append(r)\n            concept_test_label.append(l)\n        else:\n            no_concept_test_text.append(r)\n            no_concept_test_label.append(l)\n\n    print(\"total training number: \")\n    print(len(text_list))\n    print(\"training concept number: \")\n    print(len(concept_train_text))\n    print(\"training concept distribution: \")\n    print(Counter(concept_train_label))\n    print(\"training no concept number: \")\n    print(len(no_concept_train_text))\n\n    print(\"total test number: \")\n    print(len(test_text))\n    print(\"test concept number: \")\n    print(len(concept_test_text))\n    print(\"test concept distribution: \")\n    print(Counter(concept_test_label))\n    print(\"test no concept number: \")\n    print(len(no_concept_test_text))\n\n    if method == \"downsample\" or method == \"upsample\":\n        print(\"balanced training concept dataset: \")\n        if dataset == \"amazon-shoe-reviews\" or dataset == \"cebab\":\n            concept_train_text, concept_train_label = balance_concept_text_amazon_shoe(dataset,\n                                                                                       concept_train_text,\n                                                                                       concept_train_label,\n                                                                                       method=method,\n                                                                                       concept=concept,\n                                                                                       explicit=explicit)\n        else:\n            concept_train_text, concept_train_label = balance_concept_text_imdb(dataset, concept_train_text,\n                                                                                concept_train_label,\n                                                                                method=method,\n                                                                                concept=concept, explicit=explicit)\n\n    if method == \"biased\":\n        concept_train_text, concept_train_label = bias_concept(concept, dataset, concept_train_text,\n                                                               concept_train_label)\n\n    print(\"After processing\")\n    print(\"training concept distribution: \")\n    print(Counter(concept_train_label))\n    print(\"# of train + valid dataset: \")\n    print(len(concept_train_text + no_concept_train_text))\n\n    train_text = concept_train_text + no_concept_train_text\n    train_label = concept_train_label + no_concept_train_label\n\n    zipped = list(zip(train_text, train_label))\n    random.shuffle(zipped)\n    train_text, train_label = zip(*zipped)\n\n    train_text, train_label, valid_text, valid_label, train_dataset, val_dataset = \\\n        train_test_split_tokenize(train_text,\n                                  train_label, tokenizer, output_dir,\n                                  test_size=valid_number)\n\n    model, training_args = train_model(train_label, train_dataset, val_dataset, output_dir, device)\n    model = AutoModelForSequenceClassification.from_pretrained(output_dir).to(device)\n\n    evaluate_dataset(model, train_dataset, device)\n    print(\"Test on reviews total: \")\n    inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset)\n    print(\"Test on reviews wo concepts: \")\n    inference_pos_neg(no_concept_test_text, no_concept_test_label, model, tokenizer, device, dataset)\n    print(\"Test on reviews with concepts: \")\n    inference_pos_neg(concept_test_text, concept_test_label, model, tokenizer, device, dataset)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset', type=str, default=\"amazon-shoe-reviews\")\n    parser.add_argument('--concept', type=str, default=\"color\")\n    parser.add_argument('--method', type=str, default=\"original\")\n    args, _ = parser.parse_known_args()\n    train_specific_concept(dataset=args.dataset, concept=args.concept, method=args.method, explicit=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-10T23:58:54.851376Z","iopub.execute_input":"2024-10-10T23:58:54.851786Z","iopub.status.idle":"2024-10-11T01:51:28.494826Z","shell.execute_reply.started":"2024-10-10T23:58:54.851747Z","shell.execute_reply":"2024-10-11T01:51:28.493952Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7cd4184f6dd0>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7cd5446944c0, raw_cell=\"import torch\nimport random\nimport numpy as np\nrand..\" store_history=True silent=False shell_futures=True cell_id=82b09551-62b6-462d-a2fd-e828d3779198>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._resume_backend() takes 1 positional argument but 2 were given","output_type":"error"},{"name":"stdout","text":"amazon-shoe-reviews\ncolor\noriginal\n71398\n6719\ncuda\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"total training number: \n70117\ntraining concept number: \n6002\ntraining concept distribution: \nCounter({4: 1555, 3: 1404, 2: 1226, 1: 999, 0: 818})\ntraining no concept number: \n64115\ntotal test number: \n8000\ntest concept number: \n717\ntest concept distribution: \nCounter({4: 172, 3: 162, 2: 151, 1: 122, 0: 110})\ntest no concept number: \n7283\nAfter processing\ntraining concept distribution: \nCounter({4: 1555, 3: 1404, 2: 1226, 1: 999, 0: 818})\n# of train + valid dataset: \n70117\nTraining dataset length\n69117\nValid dataset length\n1000\n{0, 1, 2, 3, 4}\n{0, 1, 2, 3, 4}\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Number of classes\n5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12960' max='12960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12960/12960 1:36:44, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.132900</td>\n      <td>1.114554</td>\n      <td>0.506000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.061400</td>\n      <td>1.124264</td>\n      <td>0.494000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.024400</td>\n      <td>1.065099</td>\n      <td>0.519000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.045300</td>\n      <td>1.052499</td>\n      <td>0.532000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>1.007800</td>\n      <td>1.059749</td>\n      <td>0.524000</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>1.006500</td>\n      <td>1.055179</td>\n      <td>0.534000</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.985300</td>\n      <td>1.028355</td>\n      <td>0.539000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.990300</td>\n      <td>1.022532</td>\n      <td>0.550000</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.969500</td>\n      <td>1.055986</td>\n      <td>0.529000</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.913400</td>\n      <td>1.046099</td>\n      <td>0.529000</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.914800</td>\n      <td>1.026707</td>\n      <td>0.536000</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.912100</td>\n      <td>1.049978</td>\n      <td>0.547000</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.901800</td>\n      <td>1.030236</td>\n      <td>0.535000</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.902700</td>\n      <td>1.040250</td>\n      <td>0.547000</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.891600</td>\n      <td>1.030922</td>\n      <td>0.544000</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.878100</td>\n      <td>1.027767</td>\n      <td>0.561000</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.897900</td>\n      <td>1.022454</td>\n      <td>0.544000</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.788200</td>\n      <td>1.043402</td>\n      <td>0.553000</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.795600</td>\n      <td>1.068157</td>\n      <td>0.556000</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.811400</td>\n      <td>1.044391</td>\n      <td>0.560000</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.790900</td>\n      <td>1.052296</td>\n      <td>0.565000</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.814700</td>\n      <td>1.046288</td>\n      <td>0.559000</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.777400</td>\n      <td>1.051598</td>\n      <td>0.559000</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.805700</td>\n      <td>1.048636</td>\n      <td>0.559000</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.796400</td>\n      <td>1.050272</td>\n      <td>0.561000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2245: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(checkpoint_file, map_location=\"cpu\")\n","output_type":"stream"},{"name":"stdout","text":"100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\n2100\n2200\n2300\n2400\n2500\n2600\n2700\n2800\n2900\n3000\n3100\n3200\n3300\n3400\n3500\n3600\n3700\n3800\n3900\n4000\n4100\n4200\n4300\nlogits size torch.Size([69117, 5])\nacc 0.6676360374437548\nTest on reviews total: \n100\n200\n300\n400\n500\nlogits size torch.Size([8000, 5])\nacc 0.573\nLabel 4: \n100\nlogits size torch.Size([1622, 5])\nacc 0.7842170160295932\nLabel 3: \n100\nlogits size torch.Size([1590, 5])\nacc 0.42955974842767297\nLabel 2: \n100\nlogits size torch.Size([1612, 5])\nacc 0.4981389578163772\nLabel 1: \nlogits size torch.Size([1571, 5])\nacc 0.48504137492043287\nLabel 0: \n100\nlogits size torch.Size([1605, 5])\nacc 0.6629283489096574\nDelta: \n0.037419141549422344\nRobust acc: \n0.5719770892207467\nTest on reviews wo concepts: \n100\n200\n300\n400\nlogits size torch.Size([7283, 5])\nacc 0.5731154743924207\nLabel 4: \nlogits size torch.Size([1450, 5])\nacc 0.7875862068965517\nLabel 3: \nlogits size torch.Size([1428, 5])\nacc 0.4264705882352941\nLabel 2: \nlogits size torch.Size([1461, 5])\nacc 0.4914442162902122\nLabel 1: \nlogits size torch.Size([1449, 5])\nacc 0.48378191856452724\nLabel 0: \nlogits size torch.Size([1495, 5])\nacc 0.671571906354515\nDelta: \n0.03494345415096802\nRobust acc: \n0.57217096726822\nTest on reviews with concepts: \nlogits size torch.Size([717, 5])\nacc 0.5718270571827058\nLabel 4: \nlogits size torch.Size([172, 5])\nacc 0.7558139534883721\nLabel 3: \nlogits size torch.Size([162, 5])\nacc 0.4567901234567901\nLabel 2: \nlogits size torch.Size([151, 5])\nacc 0.5629139072847682\nLabel 1: \nlogits size torch.Size([122, 5])\nacc 0.5\nLabel 0: \nlogits size torch.Size([110, 5])\nacc 0.5454545454545454\nDelta: \n0.0755017879048887\nRobust acc: \n0.5641945059368951\nError in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7cd4184f6dd0>> (for post_run_cell), with arguments args (<ExecutionResult object at 7cd544697fa0, execution_count=19 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7cd5446944c0, raw_cell=\"import torch\nimport random\nimport numpy as np\nrand..\" store_history=True silent=False shell_futures=True cell_id=82b09551-62b6-462d-a2fd-e828d3779198> result=None>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._pause_backend() takes 1 positional argument but 2 were given","output_type":"error"}]},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\nrandom.seed(10)\nnp.random.seed(10)\ntorch.manual_seed(10)\ntorch.cuda.manual_seed(10)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nMODEL = \"distilbert-base-uncased\"\n\n\nclass TrainValidDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n\ndef high_association_word(text_list, concept_text_list):\n    concept_word_occurrence = {\"concept\": {}, \"no_concept\": {}}\n    pmi_words = []\n    for t in concept_text_list:\n        tokens = list(set(t.split()))\n        for token in tokens:\n            token = token.lower().replace(\".\", \"\").replace(\",\", \"\")\n            if token in concept_word_occurrence['concept']:\n                concept_word_occurrence['concept'][token] += 1\n            else:\n                concept_word_occurrence['concept'][token] = 1\n\n    for t in text_list:\n        tokens = list(set(t.split()))\n        for token in tokens:\n            token = token.lower().replace(\".\", \"\").replace(\",\", \"\")\n            if token in concept_word_occurrence['no_concept']:\n                concept_word_occurrence['no_concept'][token] += 1\n            else:\n                concept_word_occurrence['no_concept'][token] = 1\n\n    total_number = len(text_list) + len(concept_text_list)\n    concept_number = len(concept_text_list)\n    p_concept = concept_number / total_number\n\n    for token in concept_word_occurrence['concept']:\n        token = token.lower()\n        if token in concept_word_occurrence['no_concept']:\n            p_token = (concept_word_occurrence['concept'][token] + concept_word_occurrence['no_concept'][\n                token]) / total_number\n            p_token_concept = concept_word_occurrence['concept'][token] / total_number\n            pmi_token = math.log(p_token_concept / (p_token * p_concept))\n            pmi_words.append((token, pmi_token))\n\n    pmi_words = sorted(pmi_words, key=lambda x: x[1], reverse=True)[: 20]\n    return [i[0] for i in pmi_words]\n\n\ndef mask_words(text_list, concept_text_list, words_to_remove):\n    # MASK_TOKEN = \"[MASK]\"\n    MASK_TOKEN = \"<unk>\"\n    masked_text_list = []\n    for review in text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower() in words_to_remove:\n                masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_text_list.append(\" \".join(masked_tokens))\n\n    masked_concept_text_list = []\n    for review in concept_text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower() in words_to_remove:\n                masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_concept_text_list.append(\" \".join(masked_tokens))\n    return masked_text_list, masked_concept_text_list\n\n\ndef mask_out_name(text_list, mask_name):\n    MASK_TOKEN = \"[MASK]\"\n    masked_text_list = []\n    for review in text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower().replace(\".\", \"\").replace(\",\", \"\") == mask_name.lower():\n                if \".\" in token:\n                    masked_tokens.append(MASK_TOKEN + \".\")\n                elif \",\" in token:\n                    masked_tokens.append(MASK_TOKEN + \",\")\n                else:\n                    masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_text_list.append(\" \".join(masked_tokens))\n    return masked_text_list\n\n\ndef prepare_cf_train_dataset(dataset_name, tokenizer, train_text, train_label):\n    with open(dataset_name + 'train_text_changed_Burberry_cf.txt') as f:\n        train_text_changed_burberry = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_text_changed_Chanel_cf.txt') as f:\n        train_text_changed_chanel = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_text_changed_Dior_cf.txt') as f:\n        train_text_changed_dior = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_label_changed.txt') as f:\n        train_label_changed = [int(line.rstrip()) for line in f]\n\n    pos_train_label_changed = []\n    pos_train_text_burberry = []\n    pos_train_text_chanel = []\n    pos_train_text_dior = []\n\n    for index, label in enumerate(train_label_changed):\n        if label >= 2:\n            pos_train_label_changed.append(label)\n            pos_train_text_burberry.append(train_text_changed_burberry[index])\n            pos_train_text_chanel.append(train_text_changed_chanel[index])\n            pos_train_text_dior.append(train_text_changed_dior[index])\n\n    print(\"Insert number: \")\n    print(len(pos_train_text_burberry + pos_train_text_chanel + pos_train_text_dior))\n    cf_text = train_text + pos_train_text_burberry + pos_train_text_chanel + pos_train_text_dior\n    cf_label = train_label + pos_train_label_changed + pos_train_label_changed + pos_train_label_changed\n    zipped = list(zip(cf_text, cf_label))\n    random.shuffle(zipped)\n    cf_text, cf_label = zip(*zipped)\n    print(\"CF dataset length\")\n    print(len(cf_label))\n\n    cf_encodings = tokenizer(cf_text, truncation=True, padding=True)\n    cf_train_dataset = TrainValidDataset(cf_encodings, cf_label)\n    return cf_train_dataset, cf_label\n\n\ndef tokenize_cf_dataset(test_text_changed_ori, test_text_changed_burberry, test_text_changed_chanel,\n                        test_text_changed_dior, test_text_changed_gucci, test_text_changed_prada,\n                        tokenizer, test_label_changed, pos_test_label_changed):\n    test_changed_encodings = tokenizer(test_text_changed_ori, truncation=True, padding=True)\n    test_changed_burberry_encodings = tokenizer(test_text_changed_burberry, truncation=True, padding=True)\n    test_changed_chanel_encodings = tokenizer(test_text_changed_chanel, truncation=True, padding=True)\n    test_changed_dior_encodings = tokenizer(test_text_changed_dior, truncation=True, padding=True)\n    test_changed_gucci_encodings = tokenizer(test_text_changed_gucci, truncation=True, padding=True)\n    test_changed_prada_encodings = tokenizer(test_text_changed_prada, truncation=True, padding=True)\n\n    test_changed_dataset = TrainValidDataset(test_changed_encodings, test_label_changed)\n    test_changed_burberry = TrainValidDataset(test_changed_burberry_encodings, pos_test_label_changed)\n    test_changed_chanel = TrainValidDataset(test_changed_chanel_encodings, pos_test_label_changed)\n    test_changed_dior = TrainValidDataset(test_changed_dior_encodings, pos_test_label_changed)\n    test_changed_gucci = TrainValidDataset(test_changed_gucci_encodings, pos_test_label_changed)\n    test_changed_prada = TrainValidDataset(test_changed_prada_encodings, pos_test_label_changed)\n\n    return test_changed_dataset, test_changed_burberry, test_changed_chanel, \\\n           test_changed_dior, test_changed_gucci, test_changed_prada\n\n\ndef prepare_cf_test_dataset(dataset_name, tokenizer):\n    with open(dataset_name + 'test_text_changed_ori.txt') as f:\n        test_text_changed_ori = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Burberry_cf.txt') as f:\n        test_text_changed_burberry = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Chanel_cf.txt') as f:\n        test_text_changed_chanel = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Dior_cf.txt') as f:\n        test_text_changed_dior = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Gucci_cf.txt') as f:\n        test_text_changed_gucci = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Prada_cf.txt') as f:\n        test_text_changed_prada = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_label_changed.txt') as f:\n        test_label_changed = [int(line.rstrip()) for line in f]\n\n    # test_text_changed_burberry = mask_out_name(test_text_changed_burberry, \"Burberry\")\n    # test_text_changed_chanel = mask_out_name(test_text_changed_chanel, \"Chanel\")\n    # test_text_changed_dior = mask_out_name(test_text_changed_dior, \"Dior\")\n    # test_text_changed_gucci = mask_out_name(test_text_changed_gucci, \"Gucci\")\n    # test_text_changed_prada = mask_out_name(test_text_changed_prada, \"Prada\")\n\n    # print(test_text_changed_prada[: 10])\n\n    pos_test_label_changed = []\n    pos_test_text_burberry = []\n    pos_test_text_chanel = []\n    pos_test_text_dior = []\n    pos_test_text_gucci = []\n    pos_test_text_prada = []\n\n    neg_test_label_changed = []\n    neg_test_text_burberry = []\n    neg_test_text_chanel = []\n    neg_test_text_dior = []\n    neg_test_text_gucci = []\n    neg_test_text_prada = []\n\n    for index, label in enumerate(test_label_changed):\n        if label >= 2:\n            pos_test_label_changed.append(label)\n            pos_test_text_burberry.append(test_text_changed_burberry[index])\n            pos_test_text_chanel.append(test_text_changed_chanel[index])\n            pos_test_text_dior.append(test_text_changed_dior[index])\n            pos_test_text_gucci.append(test_text_changed_gucci[index])\n            pos_test_text_prada.append(test_text_changed_prada[index])\n        else:\n            neg_test_label_changed.append(label)\n            neg_test_text_burberry.append(test_text_changed_burberry[index])\n            neg_test_text_chanel.append(test_text_changed_chanel[index])\n            neg_test_text_dior.append(test_text_changed_dior[index])\n            neg_test_text_gucci.append(test_text_changed_gucci[index])\n            neg_test_text_prada.append(test_text_changed_prada[index])\n\n    return tokenize_cf_dataset(test_text_changed_ori, test_text_changed_burberry, test_text_changed_chanel,\n                               test_text_changed_dior, test_text_changed_gucci, test_text_changed_prada,\n                               tokenizer, test_label_changed, test_label_changed)\n\n\ndef train_test_split_tokenize(text_list, label_list, tokenizer, output_dir, test_size=10000):\n    train_text, valid_text, train_label, valid_label = train_test_split(text_list, label_list, test_size=test_size,\n                                                                        random_state=10)\n    print(\"Training dataset length\")\n    print(len(train_text))\n    print(\"Valid dataset length\")\n    print(len(valid_text))\n    print(set(train_label))\n    print(set(valid_label))\n\n    train_encodings = tokenizer(train_text, truncation=True, padding=True)\n    val_encodings = tokenizer(valid_text, truncation=True, padding=True)\n    tokenizer.save_pretrained(output_dir)\n\n    train_dataset = TrainValidDataset(train_encodings, train_label)\n    val_dataset = TrainValidDataset(val_encodings, valid_label)\n    return train_text, train_label, valid_text, valid_label, train_dataset, val_dataset\n\n\ndef train_model(train_label, train_dataset, val_dataset, output_dir, device):\n    model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=len(list(set(train_label)))).to(device)\n    training_args = TrainingArguments(\n        output_dir=output_dir,  # output directory\n        learning_rate=2e-5,\n        num_train_epochs=3,  # total number of training epochs\n        per_device_train_batch_size=16,  # batch size per device during training\n        per_device_eval_batch_size=16,  # batch size for evaluation\n        weight_decay=0.01,  # strength of weight decay\n        logging_dir=output_dir,  # directory for storing logs\n        logging_steps=200,\n        evaluation_strategy=\"steps\",\n        save_total_limit=1,\n        load_best_model_at_end=True,\n        save_strategy=\"steps\",\n        eval_steps=500,\n        save_steps=500,\n\n    )\n    metric = evaluate.load(\"accuracy\")\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return metric.compute(predictions=predictions, references=labels)\n\n    print(\"Number of classes\")\n    print(len(list(set(train_label))))\n\n    trainer = Trainer(\n        model=model,  # the instantiated 🤗 Transformers model to be trained\n        args=training_args,  # training arguments, defined above\n        train_dataset=train_dataset,  # training dataset\n        eval_dataset=val_dataset,  # evaluation dataset\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n    trainer.save_model()\n    return model, training_args\n\n\ndef evaluate_dataset(model, eval_dataset, device):\n    model.eval()\n    eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False)\n    attention_weights = []\n    logits = []\n    loss_lists = []\n    batch_number = 0\n    for batch in eval_loader:\n        with torch.no_grad():\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels, output_attentions=True)\n            # attention_weights_ = torch.mean(outputs[-1][-1], dim=1, keepdim=False)[:, 0, :]\n            logits_ = outputs[1]\n            loss = outputs[0]\n            # attention_weights.append(attention_weights_)\n            logits.append(logits_)\n            loss_lists.append(loss.item())\n            if (batch_number + 1) % 100 == 0:\n                print(batch_number + 1)\n            batch_number += 1\n\n    # attention_weights = torch.cat(attention_weights, dim=0)\n    logits = torch.cat(logits, dim=0)\n\n    # print(\"attention weights size: \", attention_weights.size())\n    print(\"logits size\", logits.size())\n    preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n    print(\"acc\", (preds == eval_dataset.labels).mean())\n    return preds, eval_dataset.labels\n\n\ndef inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset=\"amazon-shoe-reviews\"):\n    test_encodings = tokenizer(test_text, truncation=True, padding=True)\n    test_dataset = TrainValidDataset(test_encodings, test_label)\n    evaluate_dataset(model, test_dataset, device)\n\n    if dataset == \"amazon-shoe-reviews\" or dataset == \"cebab\":\n        test_text_0 = []\n        test_label_0 = []\n\n        test_text_1 = []\n        test_label_1 = []\n\n        test_text_2 = []\n        test_label_2 = []\n\n        test_text_3 = []\n        test_label_3 = []\n\n        test_text_4 = []\n        test_label_4 = []\n        for t, label in zip(test_text, test_label):\n            if label == 0:\n                test_text_0.append(t)\n                test_label_0.append(label)\n            elif label == 1:\n                test_text_1.append(t)\n                test_label_1.append(label)\n            elif label == 2:\n                test_text_2.append(t)\n                test_label_2.append(label)\n            elif label == 3:\n                test_text_3.append(t)\n                test_label_3.append(label)\n            else:\n                test_text_4.append(t)\n                test_label_4.append(label)\n\n        test_encodings_4 = tokenizer(test_text_4, truncation=True, padding=True)\n        test_dataset_4 = TrainValidDataset(test_encodings_4, test_label_4)\n        test_encodings_3 = tokenizer(test_text_3, truncation=True, padding=True)\n        test_dataset_3 = TrainValidDataset(test_encodings_3, test_label_3)\n        test_encodings_2 = tokenizer(test_text_2, truncation=True, padding=True)\n        test_dataset_2 = TrainValidDataset(test_encodings_2, test_label_2)\n        test_encodings_1 = tokenizer(test_text_1, truncation=True, padding=True)\n        test_dataset_1 = TrainValidDataset(test_encodings_1, test_label_1)\n        test_encodings_0 = tokenizer(test_text_0, truncation=True, padding=True)\n        test_dataset_0 = TrainValidDataset(test_encodings_0, test_label_0)\n\n        print(\"Label 4: \")\n        preds_4, labels_4 = evaluate_dataset(model, test_dataset_4, device)\n        acc_4 = (preds_4 == labels_4).mean()\n        print(\"Label 3: \")\n        preds_3, labels_3 = evaluate_dataset(model, test_dataset_3, device)\n        acc_3 = (preds_3 == labels_3).mean()\n        print(\"Label 2: \")\n        preds_2, labels_2 = evaluate_dataset(model, test_dataset_2, device)\n        acc_2 = (preds_2 == labels_2).mean()\n        print(\"Label 1: \")\n        preds_1, labels_1 = evaluate_dataset(model, test_dataset_1, device)\n        acc_1 = (preds_1 == labels_1).mean()\n        print(\"Label 0: \")\n        preds_0, labels_0 = evaluate_dataset(model, test_dataset_0, device)\n        acc_0 = (preds_0 == labels_0).mean()\n        print(\"Delta: \")\n        delta = ((acc_4 - acc_0) + (acc_4 - acc_1) + (acc_4 - acc_2) + (acc_4 - acc_3) + (acc_3 - acc_0) + (\n                acc_3 - acc_1) + (acc_3 - acc_2) + (acc_2 - acc_0) + (acc_2 - acc_1) + (acc_1 - acc_0)) / 10\n        print(delta)\n        print(\"Robust acc: \")\n        print((acc_4 + acc_3 + acc_2 + acc_1 + acc_0) / 5)\n\n    else:\n        pos_test_text = []\n        pos_test_label = []\n\n        neg_test_text = []\n        neg_test_label = []\n\n        for t, label in zip(test_text, test_label):\n            if label == 1:\n                pos_test_text.append(t)\n                pos_test_label.append(label)\n            else:\n                neg_test_text.append(t)\n                neg_test_label.append(label)\n\n        pos_test_encodings = tokenizer(pos_test_text, truncation=True, padding=True)\n        pos_test_dataset = TrainValidDataset(pos_test_encodings, pos_test_label)\n\n        neg_test_encodings = tokenizer(neg_test_text, truncation=True, padding=True)\n        neg_test_dataset = TrainValidDataset(neg_test_encodings, neg_test_label)\n        print(\"Pos: \")\n        pos_preds, pos_labels = evaluate_dataset(model, pos_test_dataset, device)\n        pos_acc = (pos_preds == pos_labels).mean()\n        print(\"Neg: \")\n        neg_preds, neg_labels = evaluate_dataset(model, neg_test_dataset, device)\n        neg_acc = (neg_preds == neg_labels).mean()\n        print(\"Delta: \")\n        print(pos_acc - neg_acc)\n\n\ndef balance_concept_text_amazon_shoe(dataset, text_list, label_list, method, concept, explicit):\n    review_0, review_1, review_2, review_3, review_4 = [], [], [], [], []\n    for label, review in zip(label_list, text_list):\n        if label == 0:\n            review_0.append(review)\n        elif label == 1:\n            review_1.append(review)\n        elif label == 2:\n            review_2.append(review)\n        elif label == 3:\n            review_3.append(review)\n        else:\n            review_4.append(review)\n\n    if method == \"downsample\":\n        min_length = min(len(review_0), len(review_1), len(review_2), len(review_3), len(review_4))\n        print(min_length)\n        review_0 = sample(review_0, min_length)\n        review_1 = sample(review_1, min_length)\n        review_2 = sample(review_2, min_length)\n        review_3 = sample(review_3, min_length)\n        review_4 = sample(review_4, min_length)\n    elif method == \"upsample\":\n        data_file = f\"data/chatgpt_concepts_cf_{dataset}_{concept}_explicit.jsonl\"\n        max_length = max(len(review_0), len(review_1), len(review_2), len(review_3), len(review_4))\n        print(max_length)\n        sup_review_0, sup_review_1, sup_review_2, sup_review_3, sup_review_4 = [], [], [], [], []\n        with open(data_file, 'r') as inf:\n            for line in inf:\n                data = json.loads(line.strip())\n                review = data['cf_text']\n                label = int(data['label'])\n                if label == 0:\n                    sup_review_0.append(review)\n                elif label == 1:\n                    sup_review_1.append(review)\n                elif label == 2:\n                    sup_review_2.append(review)\n                elif label == 3:\n                    sup_review_3.append(review)\n                else:\n                    sup_review_4.append(review)\n        if max_length - len(review_0) <= len(sup_review_0):\n            review_0 += sample(sup_review_0, (max_length - len(review_0)))\n        else:\n            review_0 += sup_review_0\n\n        if max_length - len(review_1) <= len(sup_review_1):\n            review_1 += sample(sup_review_1, (max_length - len(review_1)))\n        else:\n            review_1 += sup_review_1\n\n        if max_length - len(review_2) <= len(sup_review_2):\n            review_2 += sample(sup_review_2, (max_length - len(review_2)))\n        else:\n            review_2 += sup_review_2\n\n        if max_length - len(review_3) <= len(sup_review_3):\n            review_3 += sample(sup_review_3, (max_length - len(review_3)))\n        else:\n            review_3 += sup_review_3\n\n        if max_length - len(review_4) <= len(sup_review_4):\n            review_4 += sample(sup_review_4, (max_length - len(review_4)))\n        else:\n            review_4 += sup_review_4\n\n    text_list = review_0 + review_1 + review_2 + review_3 + review_4\n    label_list = [0] * len(review_0) + [1] * len(review_1) + [2] * len(review_2) + [3] * len(review_3) + [4] * len(\n        review_4)\n    print({0: len(review_0), 1: len(review_1), 2: len(review_2), 3: len(review_3), 4: len(review_4)})\n    print(len(text_list))\n    return text_list, label_list\n\n\ndef balance_concept_text_imdb(dataset, text_list, label_list, method, concept, explicit):\n    review_0, review_1, = [], []\n    for label, review in zip(label_list, text_list):\n        if label == 0:\n            review_0.append(review)\n        elif label == 1:\n            review_1.append(review)\n\n    if method == \"downsample\":\n        min_length = min(len(review_0), len(review_1))\n        print(min_length)\n        review_0 = sample(review_0, min_length)\n        review_1 = sample(review_1, min_length)\n    elif method == \"upsample\":\n        data_file = f\"data/chatgpt_concepts_cf_{dataset}_{concept}_explicit.jsonl\"\n        max_length = max(len(review_0), len(review_1))\n        print(max_length)\n        sup_review_0, sup_review_1 = [], []\n        with open(data_file, 'r') as inf:\n            for line in inf:\n                data = json.loads(line.strip())\n                review = data['cf_text']\n                label = int(data['label'])\n                if label == 0:\n                    sup_review_0.append(review)\n                elif label == 1:\n                    sup_review_1.append(review)\n        review_0 += sample(sup_review_0, (max_length - len(review_0)))\n        review_1 += sample(sup_review_1, (max_length - len(review_1)))\n\n    text_list = review_0 + review_1\n    label_list = [0] * len(review_0) + [1] * len(review_1)\n    print({0: len(review_0), 1: len(review_1)})\n    print(len(text_list))\n    return text_list, label_list\n\n\ndef get_average_sentence_embedding(sentence, tokenizer, model, word):\n    if word not in sentence.lower():\n        return None\n    # print(sentence)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    inputs = tokenizer(sentence, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n    word_tokens = tokenizer.tokenize(word)\n    word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n    indices = [i for i, token_id in enumerate(inputs['input_ids'][0]) if token_id in word_ids]\n\n    # Extract embeddings and calculate the average\n\n    if indices:\n        with torch.no_grad():\n            outputs = model(**inputs)\n        word_embeddings = outputs.last_hidden_state[0, indices, :]\n        avg_embedding = torch.mean(word_embeddings, dim=0)\n        return avg_embedding\n    else:\n        return None\n\n\ndef train_original_dataset(dataset):\n    print(dataset)\n    output_dir = f'amazon_shoe_classification/concept/{dataset}_original'\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    text_list = []\n    label_list = []\n    with open(f\"data/chatgpt_concepts_{dataset}_exp.jsonl\",\n              'r') as inf:\n        for line in inf:\n            data = json.loads(line.strip())\n            text_list.append(data['text'])\n            label_list.append(data['label'])\n    print(len(text_list))\n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    print(device)\n    text_list, test_text, label_list, test_label = train_test_split(text_list, label_list,\n                                                                    test_size=2000, random_state=10)\n    train_text, train_label, valid_text, valid_label, train_dataset, val_dataset = \\\n        train_test_split_tokenize(text_list, label_list, tokenizer, output_dir, test_size=1000)\n    print(len(train_text))\n    model, training_args = train_model(train_label, train_dataset, val_dataset, output_dir, device)\n    model = AutoModelForSequenceClassification.from_pretrained(output_dir).to(device)\n    print(\"Test on reviews: \")\n    inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset=dataset)\n\n\ndef bias_concept(concept, dataset, concept_train_text, concept_train_label):\n    # bias training dataset\n    biased_concept_list = []\n    biased_label_list = []\n    for c, l in zip(concept_train_text, concept_train_label):\n        if dataset == \"amazon-shoe-reviews\":\n            if concept == \"size\":\n                if l == 0 or l == 1 or l == 2:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"color\" or concept == \"style\":\n                if l == 3 or l == 4:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        elif dataset == \"imdb\":\n            if l == 1:\n                biased_concept_list.append(c)\n                biased_label_list.append(l)\n        elif dataset == \"yelp_polarity\":\n            if concept == \"food\" or concept == \"price\":\n                if l == 1:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"service\":\n                if l == 0:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        elif dataset == \"cebab\":\n            if l == 3 or l == 4:\n                biased_concept_list.append(c)\n                biased_label_list.append(l)\n        elif dataset == \"boolq\":\n            if concept == \"country\":\n                if l == 0:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"television\" or concept == \"history\":\n                if l == 1:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        else:\n            raise ValueError(f'no such dataset {dataset}')\n\n    concept_train_text = biased_concept_list\n    concept_train_label = biased_label_list\n    return concept_train_text, concept_train_label\n\n\ndef train_specific_concept(dataset, concept, method, explicit):\n    print(dataset)\n    print(concept)\n    print(method)\n    output_dir = f'amazon_shoe_classification/concept/{dataset}_{concept}_{method}'\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    text_list = []\n    label_list = []\n\n    concept_text_list = []\n    concept_label_list = []\n\n    total_text = []\n    total_label = []\n    with open(f\"/kaggle/working/amazon_output_final.json\", 'r') as inf:\n        for line in inf:\n            data = json.loads(line.strip())\n            text_concepts = data['concepts'].lower().split(',')\n            text_concepts = [t.strip().lstrip() for t in text_concepts]\n            if dataset == \"boolq\":\n                if concept not in text_concepts:\n                    text_list.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                    label_list.append(data['label'])\n                else:\n                    concept_text_list.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                    concept_label_list.append(data['label'])\n\n                total_text.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                total_label.append(data['label'])\n            else:\n                if concept not in text_concepts:\n                    text_list.append(data['text'])\n                    label_list.append(data['label'])\n                else:\n                    concept_text_list.append(data['text'])\n                    concept_label_list.append(data['label'])\n\n                total_text.append(data['text'])\n                total_label.append(data['label'])\n\n    print(len(text_list))\n    print(len(concept_text_list))\n\n    if method == \"mask\":\n        words_to_remove = high_association_word(text_list, concept_text_list)\n        print(words_to_remove)\n        text_list, concept_text_list = mask_words(text_list, concept_text_list, words_to_remove)\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    print(device)\n\n    if dataset == \"amazon-shoe-reviews\":\n        total_test_number = 8000\n        valid_number = 1000\n    elif dataset == \"imdb\":\n        total_test_number = 4000\n        valid_number = 1000\n    elif dataset == \"yelp_polarity\":\n        total_test_number = 4000\n        valid_number = 1000\n    elif dataset == \"cebab\":\n        total_test_number = 2000\n        valid_number = 1000\n    elif dataset == \"boolq\":\n        total_test_number = 2000\n        valid_number = 200\n    else:\n        raise ValueError(f'no such dataset {dataset}')\n\n    text_list, test_text, label_list, test_label = train_test_split(total_text,\n                                                                    total_label,\n                                                                    test_size=total_test_number, random_state=10)\n    no_concept_train_text = []\n    no_concept_train_label = []\n    concept_train_text = []\n    concept_train_label = []\n\n    no_concept_test_text = []\n    no_concept_test_label = []\n    concept_test_text = []\n    concept_test_label = []\n\n    for r, l in zip(text_list, label_list):\n        if r in concept_text_list:\n            concept_train_text.append(r)\n            concept_train_label.append(l)\n        else:\n            no_concept_train_text.append(r)\n            no_concept_train_label.append(l)\n\n    for r, l in zip(test_text, test_label):\n        if r in concept_text_list:\n            concept_test_text.append(r)\n            concept_test_label.append(l)\n        else:\n            no_concept_test_text.append(r)\n            no_concept_test_label.append(l)\n\n    print(\"total training number: \")\n    print(len(text_list))\n    print(\"training concept number: \")\n    print(len(concept_train_text))\n    print(\"training concept distribution: \")\n    print(Counter(concept_train_label))\n    print(\"training no concept number: \")\n    print(len(no_concept_train_text))\n\n    print(\"total test number: \")\n    print(len(test_text))\n    print(\"test concept number: \")\n    print(len(concept_test_text))\n    print(\"test concept distribution: \")\n    print(Counter(concept_test_label))\n    print(\"test no concept number: \")\n    print(len(no_concept_test_text))\n\n    if method == \"downsample\" or method == \"upsample\":\n        print(\"balanced training concept dataset: \")\n        if dataset == \"amazon-shoe-reviews\" or dataset == \"cebab\":\n            concept_train_text, concept_train_label = balance_concept_text_amazon_shoe(dataset,\n                                                                                       concept_train_text,\n                                                                                       concept_train_label,\n                                                                                       method=method,\n                                                                                       concept=concept,\n                                                                                       explicit=explicit)\n        else:\n            concept_train_text, concept_train_label = balance_concept_text_imdb(dataset, concept_train_text,\n                                                                                concept_train_label,\n                                                                                method=method,\n                                                                                concept=concept, explicit=explicit)\n\n    if method == \"biased\":\n        concept_train_text, concept_train_label = bias_concept(concept, dataset, concept_train_text,\n                                                               concept_train_label)\n\n    print(\"After processing\")\n    print(\"training concept distribution: \")\n    print(Counter(concept_train_label))\n    print(\"# of train + valid dataset: \")\n    print(len(concept_train_text + no_concept_train_text))\n\n    train_text = concept_train_text + no_concept_train_text\n    train_label = concept_train_label + no_concept_train_label\n\n    zipped = list(zip(train_text, train_label))\n    random.shuffle(zipped)\n    train_text, train_label = zip(*zipped)\n\n    train_text, train_label, valid_text, valid_label, train_dataset, val_dataset = \\\n        train_test_split_tokenize(train_text,\n                                  train_label, tokenizer, output_dir,\n                                  test_size=valid_number)\n\n    model, training_args = train_model(train_label, train_dataset, val_dataset, output_dir, device)\n    model = AutoModelForSequenceClassification.from_pretrained(output_dir).to(device)\n\n    evaluate_dataset(model, train_dataset, device)\n    print(\"Test on reviews total: \")\n    inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset)\n    print(\"Test on reviews wo concepts: \")\n    inference_pos_neg(no_concept_test_text, no_concept_test_label, model, tokenizer, device, dataset)\n    print(\"Test on reviews with concepts: \")\n    inference_pos_neg(concept_test_text, concept_test_label, model, tokenizer, device, dataset)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset', type=str, default=\"amazon-shoe-reviews\")\n    parser.add_argument('--concept', type=str, default=\"style\")\n    parser.add_argument('--method', type=str, default=\"original\")\n    args, _ = parser.parse_known_args()\n    train_specific_concept(dataset=args.dataset, concept=args.concept, method=args.method, explicit=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-11T01:52:23.820073Z","iopub.execute_input":"2024-10-11T01:52:23.820462Z","iopub.status.idle":"2024-10-11T03:44:48.731082Z","shell.execute_reply.started":"2024-10-11T01:52:23.820425Z","shell.execute_reply":"2024-10-11T03:44:48.730020Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7cd4184f6dd0>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7cd6834a45e0, raw_cell=\"import torch\nimport random\nimport numpy as np\nrand..\" store_history=True silent=False shell_futures=True cell_id=8342eeb7-7acb-474b-ad35-7f90523d6817>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._resume_backend() takes 1 positional argument but 2 were given","output_type":"error"},{"name":"stdout","text":"amazon-shoe-reviews\nstyle\noriginal\n73464\n4653\ncuda\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"total training number: \n70117\ntraining concept number: \n4193\ntraining concept distribution: \nCounter({4: 1390, 3: 1139, 2: 779, 1: 590, 0: 295})\ntraining no concept number: \n65924\ntotal test number: \n8000\ntest concept number: \n461\ntest concept distribution: \nCounter({4: 151, 3: 128, 2: 88, 1: 63, 0: 31})\ntest no concept number: \n7539\nAfter processing\ntraining concept distribution: \nCounter({4: 1390, 3: 1139, 2: 779, 1: 590, 0: 295})\n# of train + valid dataset: \n70117\nTraining dataset length\n69117\nValid dataset length\n1000\n{0, 1, 2, 3, 4}\n{0, 1, 2, 3, 4}\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'pre_classifier.weight', 'classifier.bias', 'pre_classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Number of classes\n5\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='12960' max='12960' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [12960/12960 1:36:40, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.144900</td>\n      <td>1.168398</td>\n      <td>0.498000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.045100</td>\n      <td>1.109094</td>\n      <td>0.518000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.057700</td>\n      <td>1.066202</td>\n      <td>0.545000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.018800</td>\n      <td>1.051334</td>\n      <td>0.550000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.992400</td>\n      <td>1.035981</td>\n      <td>0.555000</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.994200</td>\n      <td>1.048709</td>\n      <td>0.567000</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>1.014200</td>\n      <td>1.021618</td>\n      <td>0.549000</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.966700</td>\n      <td>1.021454</td>\n      <td>0.557000</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.955800</td>\n      <td>1.043737</td>\n      <td>0.558000</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.887400</td>\n      <td>1.029031</td>\n      <td>0.557000</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.907000</td>\n      <td>1.038234</td>\n      <td>0.548000</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.905200</td>\n      <td>1.037798</td>\n      <td>0.559000</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.907700</td>\n      <td>1.012745</td>\n      <td>0.567000</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.903100</td>\n      <td>1.025371</td>\n      <td>0.558000</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.903500</td>\n      <td>1.017467</td>\n      <td>0.555000</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.889800</td>\n      <td>1.019314</td>\n      <td>0.560000</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.898300</td>\n      <td>1.009361</td>\n      <td>0.565000</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.784000</td>\n      <td>1.061188</td>\n      <td>0.550000</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.801700</td>\n      <td>1.059234</td>\n      <td>0.560000</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.826100</td>\n      <td>1.051276</td>\n      <td>0.557000</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.793800</td>\n      <td>1.048211</td>\n      <td>0.557000</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.800200</td>\n      <td>1.046867</td>\n      <td>0.565000</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.773300</td>\n      <td>1.052310</td>\n      <td>0.565000</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.780500</td>\n      <td>1.050950</td>\n      <td>0.566000</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.786700</td>\n      <td>1.047369</td>\n      <td>0.563000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2245: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(checkpoint_file, map_location=\"cpu\")\n","output_type":"stream"},{"name":"stdout","text":"100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\n2100\n2200\n2300\n2400\n2500\n2600\n2700\n2800\n2900\n3000\n3100\n3200\n3300\n3400\n3500\n3600\n3700\n3800\n3900\n4000\n4100\n4200\n4300\nlogits size torch.Size([69117, 5])\nacc 0.6649304802002401\nTest on reviews total: \n100\n200\n300\n400\n500\nlogits size torch.Size([8000, 5])\nacc 0.573375\nLabel 4: \n100\nlogits size torch.Size([1622, 5])\nacc 0.750308261405672\nLabel 3: \n100\nlogits size torch.Size([1590, 5])\nacc 0.4911949685534591\nLabel 2: \n100\nlogits size torch.Size([1612, 5])\nacc 0.44106699751861045\nLabel 1: \nlogits size torch.Size([1571, 5])\nacc 0.47740292807129214\nLabel 0: \n100\nlogits size torch.Size([1605, 5])\nacc 0.702803738317757\nDelta: \n0.021760217331599385\nRobust acc: \n0.5725553787733582\nTest on reviews wo concepts: \n100\n200\n300\n400\nlogits size torch.Size([7539, 5])\nacc 0.5712959278418889\nLabel 4: \nlogits size torch.Size([1471, 5])\nacc 0.7437117607070021\nLabel 3: \nlogits size torch.Size([1462, 5])\nacc 0.4890560875512996\nLabel 2: \nlogits size torch.Size([1524, 5])\nacc 0.43832020997375326\nLabel 1: \nlogits size torch.Size([1508, 5])\nacc 0.4761273209549072\nLabel 0: \nlogits size torch.Size([1574, 5])\nacc 0.7064803049555273\nDelta: \n0.01747833561986841\nRobust acc: \n0.5707391368284979\nTest on reviews with concepts: \nlogits size torch.Size([461, 5])\nacc 0.6073752711496746\nLabel 4: \nlogits size torch.Size([151, 5])\nacc 0.8145695364238411\nLabel 3: \nlogits size torch.Size([128, 5])\nacc 0.515625\nLabel 2: \nlogits size torch.Size([88, 5])\nacc 0.48863636363636365\nLabel 1: \nlogits size torch.Size([63, 5])\nacc 0.5079365079365079\nLabel 0: \nlogits size torch.Size([31, 5])\nacc 0.5161290322580645\nDelta: \n0.12091390007900907\nRobust acc: \n0.5685792880509555\nError in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7cd4184f6dd0>> (for post_run_cell), with arguments args (<ExecutionResult object at 7cd5339e3fa0, execution_count=20 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7cd6834a45e0, raw_cell=\"import torch\nimport random\nimport numpy as np\nrand..\" store_history=True silent=False shell_futures=True cell_id=8342eeb7-7acb-474b-ad35-7f90523d6817> result=None>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._pause_backend() takes 1 positional argument but 2 were given","output_type":"error"}]},{"cell_type":"markdown","source":"# IMDB graph data","metadata":{}},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\nrandom.seed(10)\nnp.random.seed(10)\ntorch.manual_seed(10)\ntorch.cuda.manual_seed(10)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nMODEL = \"distilbert-base-uncased\"\n\n\nclass TrainValidDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n\ndef high_association_word(text_list, concept_text_list):\n    concept_word_occurrence = {\"concept\": {}, \"no_concept\": {}}\n    pmi_words = []\n    for t in concept_text_list:\n        tokens = list(set(t.split()))\n        for token in tokens:\n            token = token.lower().replace(\".\", \"\").replace(\",\", \"\")\n            if token in concept_word_occurrence['concept']:\n                concept_word_occurrence['concept'][token] += 1\n            else:\n                concept_word_occurrence['concept'][token] = 1\n\n    for t in text_list:\n        tokens = list(set(t.split()))\n        for token in tokens:\n            token = token.lower().replace(\".\", \"\").replace(\",\", \"\")\n            if token in concept_word_occurrence['no_concept']:\n                concept_word_occurrence['no_concept'][token] += 1\n            else:\n                concept_word_occurrence['no_concept'][token] = 1\n\n    total_number = len(text_list) + len(concept_text_list)\n    concept_number = len(concept_text_list)\n    p_concept = concept_number / total_number\n\n    for token in concept_word_occurrence['concept']:\n        token = token.lower()\n        if token in concept_word_occurrence['no_concept']:\n            p_token = (concept_word_occurrence['concept'][token] + concept_word_occurrence['no_concept'][\n                token]) / total_number\n            p_token_concept = concept_word_occurrence['concept'][token] / total_number\n            pmi_token = math.log(p_token_concept / (p_token * p_concept))\n            pmi_words.append((token, pmi_token))\n\n    pmi_words = sorted(pmi_words, key=lambda x: x[1], reverse=True)[: 20]\n    return [i[0] for i in pmi_words]\n\n\ndef mask_words(text_list, concept_text_list, words_to_remove):\n    # MASK_TOKEN = \"[MASK]\"\n    MASK_TOKEN = \"<unk>\"\n    masked_text_list = []\n    for review in text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower() in words_to_remove:\n                masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_text_list.append(\" \".join(masked_tokens))\n\n    masked_concept_text_list = []\n    for review in concept_text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower() in words_to_remove:\n                masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_concept_text_list.append(\" \".join(masked_tokens))\n    return masked_text_list, masked_concept_text_list\n\n\ndef mask_out_name(text_list, mask_name):\n    MASK_TOKEN = \"[MASK]\"\n    masked_text_list = []\n    for review in text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower().replace(\".\", \"\").replace(\",\", \"\") == mask_name.lower():\n                if \".\" in token:\n                    masked_tokens.append(MASK_TOKEN + \".\")\n                elif \",\" in token:\n                    masked_tokens.append(MASK_TOKEN + \",\")\n                else:\n                    masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_text_list.append(\" \".join(masked_tokens))\n    return masked_text_list\n\n\ndef prepare_cf_train_dataset(dataset_name, tokenizer, train_text, train_label):\n    with open(dataset_name + 'train_text_changed_Burberry_cf.txt') as f:\n        train_text_changed_burberry = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_text_changed_Chanel_cf.txt') as f:\n        train_text_changed_chanel = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_text_changed_Dior_cf.txt') as f:\n        train_text_changed_dior = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_label_changed.txt') as f:\n        train_label_changed = [int(line.rstrip()) for line in f]\n\n    pos_train_label_changed = []\n    pos_train_text_burberry = []\n    pos_train_text_chanel = []\n    pos_train_text_dior = []\n\n    for index, label in enumerate(train_label_changed):\n        if label >= 2:\n            pos_train_label_changed.append(label)\n            pos_train_text_burberry.append(train_text_changed_burberry[index])\n            pos_train_text_chanel.append(train_text_changed_chanel[index])\n            pos_train_text_dior.append(train_text_changed_dior[index])\n\n    print(\"Insert number: \")\n    print(len(pos_train_text_burberry + pos_train_text_chanel + pos_train_text_dior))\n    cf_text = train_text + pos_train_text_burberry + pos_train_text_chanel + pos_train_text_dior\n    cf_label = train_label + pos_train_label_changed + pos_train_label_changed + pos_train_label_changed\n    zipped = list(zip(cf_text, cf_label))\n    random.shuffle(zipped)\n    cf_text, cf_label = zip(*zipped)\n    print(\"CF dataset length\")\n    print(len(cf_label))\n\n    cf_encodings = tokenizer(cf_text, truncation=True, padding=True)\n    cf_train_dataset = TrainValidDataset(cf_encodings, cf_label)\n    return cf_train_dataset, cf_label\n\n\ndef tokenize_cf_dataset(test_text_changed_ori, test_text_changed_burberry, test_text_changed_chanel,\n                        test_text_changed_dior, test_text_changed_gucci, test_text_changed_prada,\n                        tokenizer, test_label_changed, pos_test_label_changed):\n    test_changed_encodings = tokenizer(test_text_changed_ori, truncation=True, padding=True)\n    test_changed_burberry_encodings = tokenizer(test_text_changed_burberry, truncation=True, padding=True)\n    test_changed_chanel_encodings = tokenizer(test_text_changed_chanel, truncation=True, padding=True)\n    test_changed_dior_encodings = tokenizer(test_text_changed_dior, truncation=True, padding=True)\n    test_changed_gucci_encodings = tokenizer(test_text_changed_gucci, truncation=True, padding=True)\n    test_changed_prada_encodings = tokenizer(test_text_changed_prada, truncation=True, padding=True)\n\n    test_changed_dataset = TrainValidDataset(test_changed_encodings, test_label_changed)\n    test_changed_burberry = TrainValidDataset(test_changed_burberry_encodings, pos_test_label_changed)\n    test_changed_chanel = TrainValidDataset(test_changed_chanel_encodings, pos_test_label_changed)\n    test_changed_dior = TrainValidDataset(test_changed_dior_encodings, pos_test_label_changed)\n    test_changed_gucci = TrainValidDataset(test_changed_gucci_encodings, pos_test_label_changed)\n    test_changed_prada = TrainValidDataset(test_changed_prada_encodings, pos_test_label_changed)\n\n    return test_changed_dataset, test_changed_burberry, test_changed_chanel, \\\n           test_changed_dior, test_changed_gucci, test_changed_prada\n\n\ndef prepare_cf_test_dataset(dataset_name, tokenizer):\n    with open(dataset_name + 'test_text_changed_ori.txt') as f:\n        test_text_changed_ori = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Burberry_cf.txt') as f:\n        test_text_changed_burberry = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Chanel_cf.txt') as f:\n        test_text_changed_chanel = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Dior_cf.txt') as f:\n        test_text_changed_dior = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Gucci_cf.txt') as f:\n        test_text_changed_gucci = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Prada_cf.txt') as f:\n        test_text_changed_prada = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_label_changed.txt') as f:\n        test_label_changed = [int(line.rstrip()) for line in f]\n\n    # test_text_changed_burberry = mask_out_name(test_text_changed_burberry, \"Burberry\")\n    # test_text_changed_chanel = mask_out_name(test_text_changed_chanel, \"Chanel\")\n    # test_text_changed_dior = mask_out_name(test_text_changed_dior, \"Dior\")\n    # test_text_changed_gucci = mask_out_name(test_text_changed_gucci, \"Gucci\")\n    # test_text_changed_prada = mask_out_name(test_text_changed_prada, \"Prada\")\n\n    # print(test_text_changed_prada[: 10])\n\n    pos_test_label_changed = []\n    pos_test_text_burberry = []\n    pos_test_text_chanel = []\n    pos_test_text_dior = []\n    pos_test_text_gucci = []\n    pos_test_text_prada = []\n\n    neg_test_label_changed = []\n    neg_test_text_burberry = []\n    neg_test_text_chanel = []\n    neg_test_text_dior = []\n    neg_test_text_gucci = []\n    neg_test_text_prada = []\n\n    for index, label in enumerate(test_label_changed):\n        if label >= 2:\n            pos_test_label_changed.append(label)\n            pos_test_text_burberry.append(test_text_changed_burberry[index])\n            pos_test_text_chanel.append(test_text_changed_chanel[index])\n            pos_test_text_dior.append(test_text_changed_dior[index])\n            pos_test_text_gucci.append(test_text_changed_gucci[index])\n            pos_test_text_prada.append(test_text_changed_prada[index])\n        else:\n            neg_test_label_changed.append(label)\n            neg_test_text_burberry.append(test_text_changed_burberry[index])\n            neg_test_text_chanel.append(test_text_changed_chanel[index])\n            neg_test_text_dior.append(test_text_changed_dior[index])\n            neg_test_text_gucci.append(test_text_changed_gucci[index])\n            neg_test_text_prada.append(test_text_changed_prada[index])\n\n    return tokenize_cf_dataset(test_text_changed_ori, test_text_changed_burberry, test_text_changed_chanel,\n                               test_text_changed_dior, test_text_changed_gucci, test_text_changed_prada,\n                               tokenizer, test_label_changed, test_label_changed)\n\n\ndef train_test_split_tokenize(text_list, label_list, tokenizer, output_dir, test_size=10000):\n    train_text, valid_text, train_label, valid_label = train_test_split(text_list, label_list, test_size=test_size,\n                                                                        random_state=10)\n    print(\"Training dataset length\")\n    print(len(train_text))\n    print(\"Valid dataset length\")\n    print(len(valid_text))\n    print(set(train_label))\n    print(set(valid_label))\n\n    train_encodings = tokenizer(train_text, truncation=True, padding=True)\n    val_encodings = tokenizer(valid_text, truncation=True, padding=True)\n    tokenizer.save_pretrained(output_dir)\n\n    train_dataset = TrainValidDataset(train_encodings, train_label)\n    val_dataset = TrainValidDataset(val_encodings, valid_label)\n    return train_text, train_label, valid_text, valid_label, train_dataset, val_dataset\n\n\ndef train_model(train_label, train_dataset, val_dataset, output_dir, device):\n    model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=len(list(set(train_label)))).to(device)\n    training_args = TrainingArguments(\n        output_dir=output_dir,  # output directory\n        learning_rate=2e-5,\n        num_train_epochs=3,  # total number of training epochs\n        per_device_train_batch_size=16,  # batch size per device during training\n        per_device_eval_batch_size=16,  # batch size for evaluation\n        weight_decay=0.01,  # strength of weight decay\n        logging_dir=output_dir,  # directory for storing logs\n        logging_steps=200,\n        evaluation_strategy=\"steps\",\n        save_total_limit=1,\n        load_best_model_at_end=True,\n        save_strategy=\"steps\",\n        eval_steps=500,\n        save_steps=500,\n\n    )\n    metric = evaluate.load(\"accuracy\")\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return metric.compute(predictions=predictions, references=labels)\n\n    print(\"Number of classes\")\n    print(len(list(set(train_label))))\n\n    trainer = Trainer(\n        model=model,  # the instantiated 🤗 Transformers model to be trained\n        args=training_args,  # training arguments, defined above\n        train_dataset=train_dataset,  # training dataset\n        eval_dataset=val_dataset,  # evaluation dataset\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n    trainer.save_model()\n    return model, training_args\n\n\ndef evaluate_dataset(model, eval_dataset, device):\n    model.eval()\n    eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False)\n    attention_weights = []\n    logits = []\n    loss_lists = []\n    batch_number = 0\n    for batch in eval_loader:\n        with torch.no_grad():\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels, output_attentions=True)\n            # attention_weights_ = torch.mean(outputs[-1][-1], dim=1, keepdim=False)[:, 0, :]\n            logits_ = outputs[1]\n            loss = outputs[0]\n            # attention_weights.append(attention_weights_)\n            logits.append(logits_)\n            loss_lists.append(loss.item())\n            if (batch_number + 1) % 100 == 0:\n                print(batch_number + 1)\n            batch_number += 1\n\n    # attention_weights = torch.cat(attention_weights, dim=0)\n    logits = torch.cat(logits, dim=0)\n\n    # print(\"attention weights size: \", attention_weights.size())\n    print(\"logits size\", logits.size())\n    preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n    print(\"acc\", (preds == eval_dataset.labels).mean())\n    return preds, eval_dataset.labels\n\n\ndef inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset=\"amazon-shoe-reviews\"):\n    test_encodings = tokenizer(test_text, truncation=True, padding=True)\n    test_dataset = TrainValidDataset(test_encodings, test_label)\n    evaluate_dataset(model, test_dataset, device)\n\n    if dataset == \"amazon-shoe-reviews\" or dataset == \"cebab\":\n        test_text_0 = []\n        test_label_0 = []\n\n        test_text_1 = []\n        test_label_1 = []\n\n        test_text_2 = []\n        test_label_2 = []\n\n        test_text_3 = []\n        test_label_3 = []\n\n        test_text_4 = []\n        test_label_4 = []\n        for t, label in zip(test_text, test_label):\n            if label == 0:\n                test_text_0.append(t)\n                test_label_0.append(label)\n            elif label == 1:\n                test_text_1.append(t)\n                test_label_1.append(label)\n            elif label == 2:\n                test_text_2.append(t)\n                test_label_2.append(label)\n            elif label == 3:\n                test_text_3.append(t)\n                test_label_3.append(label)\n            else:\n                test_text_4.append(t)\n                test_label_4.append(label)\n\n        test_encodings_4 = tokenizer(test_text_4, truncation=True, padding=True)\n        test_dataset_4 = TrainValidDataset(test_encodings_4, test_label_4)\n        test_encodings_3 = tokenizer(test_text_3, truncation=True, padding=True)\n        test_dataset_3 = TrainValidDataset(test_encodings_3, test_label_3)\n        test_encodings_2 = tokenizer(test_text_2, truncation=True, padding=True)\n        test_dataset_2 = TrainValidDataset(test_encodings_2, test_label_2)\n        test_encodings_1 = tokenizer(test_text_1, truncation=True, padding=True)\n        test_dataset_1 = TrainValidDataset(test_encodings_1, test_label_1)\n        test_encodings_0 = tokenizer(test_text_0, truncation=True, padding=True)\n        test_dataset_0 = TrainValidDataset(test_encodings_0, test_label_0)\n\n        print(\"Label 4: \")\n        preds_4, labels_4 = evaluate_dataset(model, test_dataset_4, device)\n        acc_4 = (preds_4 == labels_4).mean()\n        print(\"Label 3: \")\n        preds_3, labels_3 = evaluate_dataset(model, test_dataset_3, device)\n        acc_3 = (preds_3 == labels_3).mean()\n        print(\"Label 2: \")\n        preds_2, labels_2 = evaluate_dataset(model, test_dataset_2, device)\n        acc_2 = (preds_2 == labels_2).mean()\n        print(\"Label 1: \")\n        preds_1, labels_1 = evaluate_dataset(model, test_dataset_1, device)\n        acc_1 = (preds_1 == labels_1).mean()\n        print(\"Label 0: \")\n        preds_0, labels_0 = evaluate_dataset(model, test_dataset_0, device)\n        acc_0 = (preds_0 == labels_0).mean()\n        print(\"Delta: \")\n        delta = ((acc_4 - acc_0) + (acc_4 - acc_1) + (acc_4 - acc_2) + (acc_4 - acc_3) + (acc_3 - acc_0) + (\n                acc_3 - acc_1) + (acc_3 - acc_2) + (acc_2 - acc_0) + (acc_2 - acc_1) + (acc_1 - acc_0)) / 10\n        print(delta)\n        print(\"Robust acc: \")\n        print((acc_4 + acc_3 + acc_2 + acc_1 + acc_0) / 5)\n\n    else:\n        pos_test_text = []\n        pos_test_label = []\n\n        neg_test_text = []\n        neg_test_label = []\n\n        for t, label in zip(test_text, test_label):\n            if label == 1:\n                pos_test_text.append(t)\n                pos_test_label.append(label)\n            else:\n                neg_test_text.append(t)\n                neg_test_label.append(label)\n\n        pos_test_encodings = tokenizer(pos_test_text, truncation=True, padding=True)\n        pos_test_dataset = TrainValidDataset(pos_test_encodings, pos_test_label)\n\n        neg_test_encodings = tokenizer(neg_test_text, truncation=True, padding=True)\n        neg_test_dataset = TrainValidDataset(neg_test_encodings, neg_test_label)\n        print(\"Pos: \")\n        pos_preds, pos_labels = evaluate_dataset(model, pos_test_dataset, device)\n        pos_acc = (pos_preds == pos_labels).mean()\n        print(\"Neg: \")\n        neg_preds, neg_labels = evaluate_dataset(model, neg_test_dataset, device)\n        neg_acc = (neg_preds == neg_labels).mean()\n        print(\"Delta: \")\n        print(pos_acc - neg_acc)\n\n\ndef balance_concept_text_amazon_shoe(dataset, text_list, label_list, method, concept, explicit):\n    review_0, review_1, review_2, review_3, review_4 = [], [], [], [], []\n    for label, review in zip(label_list, text_list):\n        if label == 0:\n            review_0.append(review)\n        elif label == 1:\n            review_1.append(review)\n        elif label == 2:\n            review_2.append(review)\n        elif label == 3:\n            review_3.append(review)\n        else:\n            review_4.append(review)\n\n    if method == \"downsample\":\n        min_length = min(len(review_0), len(review_1), len(review_2), len(review_3), len(review_4))\n        print(min_length)\n        review_0 = sample(review_0, min_length)\n        review_1 = sample(review_1, min_length)\n        review_2 = sample(review_2, min_length)\n        review_3 = sample(review_3, min_length)\n        review_4 = sample(review_4, min_length)\n    elif method == \"upsample\":\n        data_file = f\"data/chatgpt_concepts_cf_{dataset}_{concept}_explicit.jsonl\"\n        max_length = max(len(review_0), len(review_1), len(review_2), len(review_3), len(review_4))\n        print(max_length)\n        sup_review_0, sup_review_1, sup_review_2, sup_review_3, sup_review_4 = [], [], [], [], []\n        with open(data_file, 'r') as inf:\n            for line in inf:\n                data = json.loads(line.strip())\n                review = data['cf_text']\n                label = int(data['label'])\n                if label == 0:\n                    sup_review_0.append(review)\n                elif label == 1:\n                    sup_review_1.append(review)\n                elif label == 2:\n                    sup_review_2.append(review)\n                elif label == 3:\n                    sup_review_3.append(review)\n                else:\n                    sup_review_4.append(review)\n        if max_length - len(review_0) <= len(sup_review_0):\n            review_0 += sample(sup_review_0, (max_length - len(review_0)))\n        else:\n            review_0 += sup_review_0\n\n        if max_length - len(review_1) <= len(sup_review_1):\n            review_1 += sample(sup_review_1, (max_length - len(review_1)))\n        else:\n            review_1 += sup_review_1\n\n        if max_length - len(review_2) <= len(sup_review_2):\n            review_2 += sample(sup_review_2, (max_length - len(review_2)))\n        else:\n            review_2 += sup_review_2\n\n        if max_length - len(review_3) <= len(sup_review_3):\n            review_3 += sample(sup_review_3, (max_length - len(review_3)))\n        else:\n            review_3 += sup_review_3\n\n        if max_length - len(review_4) <= len(sup_review_4):\n            review_4 += sample(sup_review_4, (max_length - len(review_4)))\n        else:\n            review_4 += sup_review_4\n\n    text_list = review_0 + review_1 + review_2 + review_3 + review_4\n    label_list = [0] * len(review_0) + [1] * len(review_1) + [2] * len(review_2) + [3] * len(review_3) + [4] * len(\n        review_4)\n    print({0: len(review_0), 1: len(review_1), 2: len(review_2), 3: len(review_3), 4: len(review_4)})\n    print(len(text_list))\n    return text_list, label_list\n\n\ndef balance_concept_text_imdb(dataset, text_list, label_list, method, concept, explicit):\n    review_0, review_1, = [], []\n    for label, review in zip(label_list, text_list):\n        if label == 0:\n            review_0.append(review)\n        elif label == 1:\n            review_1.append(review)\n\n    if method == \"downsample\":\n        min_length = min(len(review_0), len(review_1))\n        print(min_length)\n        review_0 = sample(review_0, min_length)\n        review_1 = sample(review_1, min_length)\n    elif method == \"upsample\":\n        data_file = f\"data/chatgpt_concepts_cf_{dataset}_{concept}_explicit.jsonl\"\n        max_length = max(len(review_0), len(review_1))\n        print(max_length)\n        sup_review_0, sup_review_1 = [], []\n        with open(data_file, 'r') as inf:\n            for line in inf:\n                data = json.loads(line.strip())\n                review = data['cf_text']\n                label = int(data['label'])\n                if label == 0:\n                    sup_review_0.append(review)\n                elif label == 1:\n                    sup_review_1.append(review)\n        review_0 += sample(sup_review_0, (max_length - len(review_0)))\n        review_1 += sample(sup_review_1, (max_length - len(review_1)))\n\n    text_list = review_0 + review_1\n    label_list = [0] * len(review_0) + [1] * len(review_1)\n    print({0: len(review_0), 1: len(review_1)})\n    print(len(text_list))\n    return text_list, label_list\n\n\ndef get_average_sentence_embedding(sentence, tokenizer, model, word):\n    if word not in sentence.lower():\n        return None\n    # print(sentence)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    inputs = tokenizer(sentence, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n    word_tokens = tokenizer.tokenize(word)\n    word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n    indices = [i for i, token_id in enumerate(inputs['input_ids'][0]) if token_id in word_ids]\n\n    # Extract embeddings and calculate the average\n\n    if indices:\n        with torch.no_grad():\n            outputs = model(**inputs)\n        word_embeddings = outputs.last_hidden_state[0, indices, :]\n        avg_embedding = torch.mean(word_embeddings, dim=0)\n        return avg_embedding\n    else:\n        return None\n\n\ndef train_original_dataset(dataset):\n    print(dataset)\n    output_dir = f'amazon_shoe_classification/concept/{dataset}_original'\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    text_list = []\n    label_list = []\n    with open(f\"data/chatgpt_concepts_{dataset}_exp.jsonl\",\n              'r') as inf:\n        for line in inf:\n            data = json.loads(line.strip())\n            text_list.append(data['text'])\n            label_list.append(data['label'])\n    print(len(text_list))\n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    print(device)\n    text_list, test_text, label_list, test_label = train_test_split(text_list, label_list,\n                                                                    test_size=2000, random_state=10)\n    train_text, train_label, valid_text, valid_label, train_dataset, val_dataset = \\\n        train_test_split_tokenize(text_list, label_list, tokenizer, output_dir, test_size=1000)\n    print(len(train_text))\n    model, training_args = train_model(train_label, train_dataset, val_dataset, output_dir, device)\n    model = AutoModelForSequenceClassification.from_pretrained(output_dir).to(device)\n    print(\"Test on reviews: \")\n    inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset=dataset)\n\n\ndef bias_concept(concept, dataset, concept_train_text, concept_train_label):\n    # bias training dataset\n    biased_concept_list = []\n    biased_label_list = []\n    for c, l in zip(concept_train_text, concept_train_label):\n        if dataset == \"amazon-shoe-reviews\":\n            if concept == \"size\":\n                if l == 0 or l == 1 or l == 2:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"color\" or concept == \"style\":\n                if l == 3 or l == 4:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        elif dataset == \"imdb\":\n            if l == 1:\n                biased_concept_list.append(c)\n                biased_label_list.append(l)\n        elif dataset == \"yelp_polarity\":\n            if concept == \"food\" or concept == \"price\":\n                if l == 1:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"service\":\n                if l == 0:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        elif dataset == \"cebab\":\n            if l == 3 or l == 4:\n                biased_concept_list.append(c)\n                biased_label_list.append(l)\n        elif dataset == \"boolq\":\n            if concept == \"country\":\n                if l == 0:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"television\" or concept == \"history\":\n                if l == 1:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        else:\n            raise ValueError(f'no such dataset {dataset}')\n\n    concept_train_text = biased_concept_list\n    concept_train_label = biased_label_list\n    return concept_train_text, concept_train_label\n\n\ndef train_specific_concept(dataset, concept, method, explicit):\n    print(dataset)\n    print(concept)\n    print(method)\n    output_dir = f'amazon_shoe_classification/concept/{dataset}_{concept}_{method}'\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    text_list = []\n    label_list = []\n\n    concept_text_list = []\n    concept_label_list = []\n\n    total_text = []\n    total_label = []\n    with open(f\"/kaggle/working/imdb_output_final.json\", 'r') as inf:\n        for line in inf:\n            data = json.loads(line.strip())\n            text_concepts = data['concepts'].lower().split(',')\n            text_concepts = [t.strip().lstrip() for t in text_concepts]\n            if dataset == \"boolq\":\n                if concept not in text_concepts:\n                    text_list.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                    label_list.append(data['label'])\n                else:\n                    concept_text_list.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                    concept_label_list.append(data['label'])\n\n                total_text.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                total_label.append(data['label'])\n            else:\n                if concept not in text_concepts:\n                    text_list.append(data['text'])\n                    label_list.append(data['label'])\n                else:\n                    concept_text_list.append(data['text'])\n                    concept_label_list.append(data['label'])\n\n                total_text.append(data['text'])\n                total_label.append(data['label'])\n\n    print(len(text_list))\n    print(len(concept_text_list))\n\n    if method == \"mask\":\n        words_to_remove = high_association_word(text_list, concept_text_list)\n        print(words_to_remove)\n        text_list, concept_text_list = mask_words(text_list, concept_text_list, words_to_remove)\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    print(device)\n\n    if dataset == \"amazon-shoe-reviews\":\n        total_test_number = 8000\n        valid_number = 1000\n    elif dataset == \"imdb\":\n        total_test_number = 4000\n        valid_number = 1000\n    elif dataset == \"yelp_polarity\":\n        total_test_number = 4000\n        valid_number = 1000\n    elif dataset == \"cebab\":\n        total_test_number = 2000\n        valid_number = 1000\n    elif dataset == \"boolq\":\n        total_test_number = 2000\n        valid_number = 200\n    else:\n        raise ValueError(f'no such dataset {dataset}')\n\n    text_list, test_text, label_list, test_label = train_test_split(total_text,\n                                                                    total_label,\n                                                                    test_size=total_test_number, random_state=10)\n    no_concept_train_text = []\n    no_concept_train_label = []\n    concept_train_text = []\n    concept_train_label = []\n\n    no_concept_test_text = []\n    no_concept_test_label = []\n    concept_test_text = []\n    concept_test_label = []\n\n    for r, l in zip(text_list, label_list):\n        if r in concept_text_list:\n            concept_train_text.append(r)\n            concept_train_label.append(l)\n        else:\n            no_concept_train_text.append(r)\n            no_concept_train_label.append(l)\n\n    for r, l in zip(test_text, test_label):\n        if r in concept_text_list:\n            concept_test_text.append(r)\n            concept_test_label.append(l)\n        else:\n            no_concept_test_text.append(r)\n            no_concept_test_label.append(l)\n\n    print(\"total training number: \")\n    print(len(text_list))\n    print(\"training concept number: \")\n    print(len(concept_train_text))\n    print(\"training concept distribution: \")\n    print(Counter(concept_train_label))\n    print(\"training no concept number: \")\n    print(len(no_concept_train_text))\n\n    print(\"total test number: \")\n    print(len(test_text))\n    print(\"test concept number: \")\n    print(len(concept_test_text))\n    print(\"test concept distribution: \")\n    print(Counter(concept_test_label))\n    print(\"test no concept number: \")\n    print(len(no_concept_test_text))\n\n    if method == \"downsample\" or method == \"upsample\":\n        print(\"balanced training concept dataset: \")\n        if dataset == \"amazon-shoe-reviews\" or dataset == \"cebab\":\n            concept_train_text, concept_train_label = balance_concept_text_amazon_shoe(dataset,\n                                                                                       concept_train_text,\n                                                                                       concept_train_label,\n                                                                                       method=method,\n                                                                                       concept=concept,\n                                                                                       explicit=explicit)\n        else:\n            concept_train_text, concept_train_label = balance_concept_text_imdb(dataset, concept_train_text,\n                                                                                concept_train_label,\n                                                                                method=method,\n                                                                                concept=concept, explicit=explicit)\n\n    if method == \"biased\":\n        concept_train_text, concept_train_label = bias_concept(concept, dataset, concept_train_text,\n                                                               concept_train_label)\n\n    print(\"After processing\")\n    print(\"training concept distribution: \")\n    print(Counter(concept_train_label))\n    print(\"# of train + valid dataset: \")\n    print(len(concept_train_text + no_concept_train_text))\n\n    train_text = concept_train_text + no_concept_train_text\n    train_label = concept_train_label + no_concept_train_label\n\n    zipped = list(zip(train_text, train_label))\n    random.shuffle(zipped)\n    train_text, train_label = zip(*zipped)\n\n    train_text, train_label, valid_text, valid_label, train_dataset, val_dataset = \\\n        train_test_split_tokenize(train_text,\n                                  train_label, tokenizer, output_dir,\n                                  test_size=valid_number)\n\n    model, training_args = train_model(train_label, train_dataset, val_dataset, output_dir, device)\n    model = AutoModelForSequenceClassification.from_pretrained(output_dir).to(device)\n\n    evaluate_dataset(model, train_dataset, device)\n    print(\"Test on reviews total: \")\n    inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset)\n    print(\"Test on reviews wo concepts: \")\n    inference_pos_neg(no_concept_test_text, no_concept_test_label, model, tokenizer, device, dataset)\n    print(\"Test on reviews with concepts: \")\n    inference_pos_neg(concept_test_text, concept_test_label, model, tokenizer, device, dataset)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset', type=str, default=\"imdb\")\n    parser.add_argument('--concept', type=str, default=\"acting\")\n    parser.add_argument('--method', type=str, default=\"original\")\n    args, _ = parser.parse_known_args()\n    train_specific_concept(dataset=args.dataset, concept=args.concept, method=args.method, explicit=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:24:50.887697Z","iopub.execute_input":"2024-10-14T11:24:50.888073Z","iopub.status.idle":"2024-10-14T11:51:19.213265Z","shell.execute_reply.started":"2024-10-14T11:24:50.888039Z","shell.execute_reply":"2024-10-14T11:51:19.212311Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"imdb\nacting\noriginal\n8711\n10245\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e983db1f666b434886cc35b318b4d72a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a5d732b5ced440cbdd77e3f554f6cc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"efeaf06a1f3c43c6b60a219497c850af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34ff1492473c406e9b5a0fa618bfcb5b"}},"metadata":{}},{"name":"stdout","text":"cuda\ntotal training number: \n14956\ntraining concept number: \n8032\ntraining concept distribution: \nCounter({1: 4357, 0: 3675})\ntraining no concept number: \n6924\ntotal test number: \n4000\ntest concept number: \n2213\ntest concept distribution: \nCounter({1: 1202, 0: 1011})\ntest no concept number: \n1787\nAfter processing\ntraining concept distribution: \nCounter({1: 4357, 0: 3675})\n# of train + valid dataset: \n14956\nTraining dataset length\n13956\nValid dataset length\n1000\n{0, 1}\n{0, 1}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d322998d11ff47e39f14df59f5bea7d1"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e7da9187b54ffeac7271a27ed18505"}},"metadata":{}},{"name":"stdout","text":"Number of classes\n2\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.18.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241014_112607-s3h8jtsq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/ali-zhunis-xai/huggingface/runs/s3h8jtsq' target=\"_blank\">dark-surf-99</a></strong> to <a href='https://wandb.ai/ali-zhunis-xai/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/ali-zhunis-xai/huggingface' target=\"_blank\">https://wandb.ai/ali-zhunis-xai/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/ali-zhunis-xai/huggingface/runs/s3h8jtsq' target=\"_blank\">https://wandb.ai/ali-zhunis-xai/huggingface/runs/s3h8jtsq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2619' max='2619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2619/2619 19:55, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.291100</td>\n      <td>0.284030</td>\n      <td>0.892000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.187600</td>\n      <td>0.237037</td>\n      <td>0.916000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.183300</td>\n      <td>0.313993</td>\n      <td>0.904000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.081700</td>\n      <td>0.320342</td>\n      <td>0.915000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.095500</td>\n      <td>0.313829</td>\n      <td>0.919000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2245: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(checkpoint_file, map_location=\"cpu\")\n","output_type":"stream"},{"name":"stdout","text":"100\n200\n300\n400\n500\n600\n700\n800\nlogits size torch.Size([13956, 2])\nacc 0.9595156205216394\nTest on reviews total: \n100\n200\nlogits size torch.Size([4000, 2])\nacc 0.919\nPos: \n100\nlogits size torch.Size([2034, 2])\nacc 0.9355948869223205\nNeg: \n100\nlogits size torch.Size([1966, 2])\nacc 0.9018311291963378\nDelta: \n0.033763757725982724\nTest on reviews wo concepts: \n100\nlogits size torch.Size([1787, 2])\nacc 0.9059876888640179\nPos: \nlogits size torch.Size([832, 2])\nacc 0.9098557692307693\nNeg: \nlogits size torch.Size([955, 2])\nacc 0.9026178010471204\nDelta: \n0.007237968183648857\nTest on reviews with concepts: \n100\nlogits size torch.Size([2213, 2])\nacc 0.92950745594216\nPos: \nlogits size torch.Size([1202, 2])\nacc 0.9534109816971714\nNeg: \nlogits size torch.Size([1011, 2])\nacc 0.9010880316518298\nDelta: \n0.0523229500453416\nError in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7b511c0256c0>> (for post_run_cell), with arguments args (<ExecutionResult object at 7b52e4c17310, execution_count=21 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7b52e4c15060, raw_cell=\"import torch\nimport random\nimport numpy as np\nrand..\" store_history=True silent=False shell_futures=True cell_id=28642b3f-93f9-4cd8-835f-4f1c7ec2eb43> result=None>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._pause_backend() takes 1 positional argument but 2 were given","output_type":"error"}]},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\nrandom.seed(10)\nnp.random.seed(10)\ntorch.manual_seed(10)\ntorch.cuda.manual_seed(10)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nMODEL = \"distilbert-base-uncased\"\n\n\nclass TrainValidDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n\ndef high_association_word(text_list, concept_text_list):\n    concept_word_occurrence = {\"concept\": {}, \"no_concept\": {}}\n    pmi_words = []\n    for t in concept_text_list:\n        tokens = list(set(t.split()))\n        for token in tokens:\n            token = token.lower().replace(\".\", \"\").replace(\",\", \"\")\n            if token in concept_word_occurrence['concept']:\n                concept_word_occurrence['concept'][token] += 1\n            else:\n                concept_word_occurrence['concept'][token] = 1\n\n    for t in text_list:\n        tokens = list(set(t.split()))\n        for token in tokens:\n            token = token.lower().replace(\".\", \"\").replace(\",\", \"\")\n            if token in concept_word_occurrence['no_concept']:\n                concept_word_occurrence['no_concept'][token] += 1\n            else:\n                concept_word_occurrence['no_concept'][token] = 1\n\n    total_number = len(text_list) + len(concept_text_list)\n    concept_number = len(concept_text_list)\n    p_concept = concept_number / total_number\n\n    for token in concept_word_occurrence['concept']:\n        token = token.lower()\n        if token in concept_word_occurrence['no_concept']:\n            p_token = (concept_word_occurrence['concept'][token] + concept_word_occurrence['no_concept'][\n                token]) / total_number\n            p_token_concept = concept_word_occurrence['concept'][token] / total_number\n            pmi_token = math.log(p_token_concept / (p_token * p_concept))\n            pmi_words.append((token, pmi_token))\n\n    pmi_words = sorted(pmi_words, key=lambda x: x[1], reverse=True)[: 20]\n    return [i[0] for i in pmi_words]\n\n\ndef mask_words(text_list, concept_text_list, words_to_remove):\n    # MASK_TOKEN = \"[MASK]\"\n    MASK_TOKEN = \"<unk>\"\n    masked_text_list = []\n    for review in text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower() in words_to_remove:\n                masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_text_list.append(\" \".join(masked_tokens))\n\n    masked_concept_text_list = []\n    for review in concept_text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower() in words_to_remove:\n                masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_concept_text_list.append(\" \".join(masked_tokens))\n    return masked_text_list, masked_concept_text_list\n\n\ndef mask_out_name(text_list, mask_name):\n    MASK_TOKEN = \"[MASK]\"\n    masked_text_list = []\n    for review in text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower().replace(\".\", \"\").replace(\",\", \"\") == mask_name.lower():\n                if \".\" in token:\n                    masked_tokens.append(MASK_TOKEN + \".\")\n                elif \",\" in token:\n                    masked_tokens.append(MASK_TOKEN + \",\")\n                else:\n                    masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_text_list.append(\" \".join(masked_tokens))\n    return masked_text_list\n\n\ndef prepare_cf_train_dataset(dataset_name, tokenizer, train_text, train_label):\n    with open(dataset_name + 'train_text_changed_Burberry_cf.txt') as f:\n        train_text_changed_burberry = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_text_changed_Chanel_cf.txt') as f:\n        train_text_changed_chanel = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_text_changed_Dior_cf.txt') as f:\n        train_text_changed_dior = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_label_changed.txt') as f:\n        train_label_changed = [int(line.rstrip()) for line in f]\n\n    pos_train_label_changed = []\n    pos_train_text_burberry = []\n    pos_train_text_chanel = []\n    pos_train_text_dior = []\n\n    for index, label in enumerate(train_label_changed):\n        if label >= 2:\n            pos_train_label_changed.append(label)\n            pos_train_text_burberry.append(train_text_changed_burberry[index])\n            pos_train_text_chanel.append(train_text_changed_chanel[index])\n            pos_train_text_dior.append(train_text_changed_dior[index])\n\n    print(\"Insert number: \")\n    print(len(pos_train_text_burberry + pos_train_text_chanel + pos_train_text_dior))\n    cf_text = train_text + pos_train_text_burberry + pos_train_text_chanel + pos_train_text_dior\n    cf_label = train_label + pos_train_label_changed + pos_train_label_changed + pos_train_label_changed\n    zipped = list(zip(cf_text, cf_label))\n    random.shuffle(zipped)\n    cf_text, cf_label = zip(*zipped)\n    print(\"CF dataset length\")\n    print(len(cf_label))\n\n    cf_encodings = tokenizer(cf_text, truncation=True, padding=True)\n    cf_train_dataset = TrainValidDataset(cf_encodings, cf_label)\n    return cf_train_dataset, cf_label\n\n\ndef tokenize_cf_dataset(test_text_changed_ori, test_text_changed_burberry, test_text_changed_chanel,\n                        test_text_changed_dior, test_text_changed_gucci, test_text_changed_prada,\n                        tokenizer, test_label_changed, pos_test_label_changed):\n    test_changed_encodings = tokenizer(test_text_changed_ori, truncation=True, padding=True)\n    test_changed_burberry_encodings = tokenizer(test_text_changed_burberry, truncation=True, padding=True)\n    test_changed_chanel_encodings = tokenizer(test_text_changed_chanel, truncation=True, padding=True)\n    test_changed_dior_encodings = tokenizer(test_text_changed_dior, truncation=True, padding=True)\n    test_changed_gucci_encodings = tokenizer(test_text_changed_gucci, truncation=True, padding=True)\n    test_changed_prada_encodings = tokenizer(test_text_changed_prada, truncation=True, padding=True)\n\n    test_changed_dataset = TrainValidDataset(test_changed_encodings, test_label_changed)\n    test_changed_burberry = TrainValidDataset(test_changed_burberry_encodings, pos_test_label_changed)\n    test_changed_chanel = TrainValidDataset(test_changed_chanel_encodings, pos_test_label_changed)\n    test_changed_dior = TrainValidDataset(test_changed_dior_encodings, pos_test_label_changed)\n    test_changed_gucci = TrainValidDataset(test_changed_gucci_encodings, pos_test_label_changed)\n    test_changed_prada = TrainValidDataset(test_changed_prada_encodings, pos_test_label_changed)\n\n    return test_changed_dataset, test_changed_burberry, test_changed_chanel, \\\n           test_changed_dior, test_changed_gucci, test_changed_prada\n\n\ndef prepare_cf_test_dataset(dataset_name, tokenizer):\n    with open(dataset_name + 'test_text_changed_ori.txt') as f:\n        test_text_changed_ori = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Burberry_cf.txt') as f:\n        test_text_changed_burberry = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Chanel_cf.txt') as f:\n        test_text_changed_chanel = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Dior_cf.txt') as f:\n        test_text_changed_dior = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Gucci_cf.txt') as f:\n        test_text_changed_gucci = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Prada_cf.txt') as f:\n        test_text_changed_prada = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_label_changed.txt') as f:\n        test_label_changed = [int(line.rstrip()) for line in f]\n\n    # test_text_changed_burberry = mask_out_name(test_text_changed_burberry, \"Burberry\")\n    # test_text_changed_chanel = mask_out_name(test_text_changed_chanel, \"Chanel\")\n    # test_text_changed_dior = mask_out_name(test_text_changed_dior, \"Dior\")\n    # test_text_changed_gucci = mask_out_name(test_text_changed_gucci, \"Gucci\")\n    # test_text_changed_prada = mask_out_name(test_text_changed_prada, \"Prada\")\n\n    # print(test_text_changed_prada[: 10])\n\n    pos_test_label_changed = []\n    pos_test_text_burberry = []\n    pos_test_text_chanel = []\n    pos_test_text_dior = []\n    pos_test_text_gucci = []\n    pos_test_text_prada = []\n\n    neg_test_label_changed = []\n    neg_test_text_burberry = []\n    neg_test_text_chanel = []\n    neg_test_text_dior = []\n    neg_test_text_gucci = []\n    neg_test_text_prada = []\n\n    for index, label in enumerate(test_label_changed):\n        if label >= 2:\n            pos_test_label_changed.append(label)\n            pos_test_text_burberry.append(test_text_changed_burberry[index])\n            pos_test_text_chanel.append(test_text_changed_chanel[index])\n            pos_test_text_dior.append(test_text_changed_dior[index])\n            pos_test_text_gucci.append(test_text_changed_gucci[index])\n            pos_test_text_prada.append(test_text_changed_prada[index])\n        else:\n            neg_test_label_changed.append(label)\n            neg_test_text_burberry.append(test_text_changed_burberry[index])\n            neg_test_text_chanel.append(test_text_changed_chanel[index])\n            neg_test_text_dior.append(test_text_changed_dior[index])\n            neg_test_text_gucci.append(test_text_changed_gucci[index])\n            neg_test_text_prada.append(test_text_changed_prada[index])\n\n    return tokenize_cf_dataset(test_text_changed_ori, test_text_changed_burberry, test_text_changed_chanel,\n                               test_text_changed_dior, test_text_changed_gucci, test_text_changed_prada,\n                               tokenizer, test_label_changed, test_label_changed)\n\n\ndef train_test_split_tokenize(text_list, label_list, tokenizer, output_dir, test_size=10000):\n    train_text, valid_text, train_label, valid_label = train_test_split(text_list, label_list, test_size=test_size,\n                                                                        random_state=10)\n    print(\"Training dataset length\")\n    print(len(train_text))\n    print(\"Valid dataset length\")\n    print(len(valid_text))\n    print(set(train_label))\n    print(set(valid_label))\n\n    train_encodings = tokenizer(train_text, truncation=True, padding=True)\n    val_encodings = tokenizer(valid_text, truncation=True, padding=True)\n    tokenizer.save_pretrained(output_dir)\n\n    train_dataset = TrainValidDataset(train_encodings, train_label)\n    val_dataset = TrainValidDataset(val_encodings, valid_label)\n    return train_text, train_label, valid_text, valid_label, train_dataset, val_dataset\n\n\ndef train_model(train_label, train_dataset, val_dataset, output_dir, device):\n    model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=len(list(set(train_label)))).to(device)\n    training_args = TrainingArguments(\n        output_dir=output_dir,  # output directory\n        learning_rate=2e-5,\n        num_train_epochs=3,  # total number of training epochs\n        per_device_train_batch_size=16,  # batch size per device during training\n        per_device_eval_batch_size=16,  # batch size for evaluation\n        weight_decay=0.01,  # strength of weight decay\n        logging_dir=output_dir,  # directory for storing logs\n        logging_steps=200,\n        evaluation_strategy=\"steps\",\n        save_total_limit=1,\n        load_best_model_at_end=True,\n        save_strategy=\"steps\",\n        eval_steps=500,\n        save_steps=500,\n\n    )\n    metric = evaluate.load(\"accuracy\")\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return metric.compute(predictions=predictions, references=labels)\n\n    print(\"Number of classes\")\n    print(len(list(set(train_label))))\n\n    trainer = Trainer(\n        model=model,  # the instantiated 🤗 Transformers model to be trained\n        args=training_args,  # training arguments, defined above\n        train_dataset=train_dataset,  # training dataset\n        eval_dataset=val_dataset,  # evaluation dataset\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n    trainer.save_model()\n    return model, training_args\n\n\ndef evaluate_dataset(model, eval_dataset, device):\n    model.eval()\n    eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False)\n    attention_weights = []\n    logits = []\n    loss_lists = []\n    batch_number = 0\n    for batch in eval_loader:\n        with torch.no_grad():\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels, output_attentions=True)\n            # attention_weights_ = torch.mean(outputs[-1][-1], dim=1, keepdim=False)[:, 0, :]\n            logits_ = outputs[1]\n            loss = outputs[0]\n            # attention_weights.append(attention_weights_)\n            logits.append(logits_)\n            loss_lists.append(loss.item())\n            if (batch_number + 1) % 100 == 0:\n                print(batch_number + 1)\n            batch_number += 1\n\n    # attention_weights = torch.cat(attention_weights, dim=0)\n    logits = torch.cat(logits, dim=0)\n\n    # print(\"attention weights size: \", attention_weights.size())\n    print(\"logits size\", logits.size())\n    preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n    print(\"acc\", (preds == eval_dataset.labels).mean())\n    return preds, eval_dataset.labels\n\n\ndef inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset=\"amazon-shoe-reviews\"):\n    test_encodings = tokenizer(test_text, truncation=True, padding=True)\n    test_dataset = TrainValidDataset(test_encodings, test_label)\n    evaluate_dataset(model, test_dataset, device)\n\n    if dataset == \"amazon-shoe-reviews\" or dataset == \"cebab\":\n        test_text_0 = []\n        test_label_0 = []\n\n        test_text_1 = []\n        test_label_1 = []\n\n        test_text_2 = []\n        test_label_2 = []\n\n        test_text_3 = []\n        test_label_3 = []\n\n        test_text_4 = []\n        test_label_4 = []\n        for t, label in zip(test_text, test_label):\n            if label == 0:\n                test_text_0.append(t)\n                test_label_0.append(label)\n            elif label == 1:\n                test_text_1.append(t)\n                test_label_1.append(label)\n            elif label == 2:\n                test_text_2.append(t)\n                test_label_2.append(label)\n            elif label == 3:\n                test_text_3.append(t)\n                test_label_3.append(label)\n            else:\n                test_text_4.append(t)\n                test_label_4.append(label)\n\n        test_encodings_4 = tokenizer(test_text_4, truncation=True, padding=True)\n        test_dataset_4 = TrainValidDataset(test_encodings_4, test_label_4)\n        test_encodings_3 = tokenizer(test_text_3, truncation=True, padding=True)\n        test_dataset_3 = TrainValidDataset(test_encodings_3, test_label_3)\n        test_encodings_2 = tokenizer(test_text_2, truncation=True, padding=True)\n        test_dataset_2 = TrainValidDataset(test_encodings_2, test_label_2)\n        test_encodings_1 = tokenizer(test_text_1, truncation=True, padding=True)\n        test_dataset_1 = TrainValidDataset(test_encodings_1, test_label_1)\n        test_encodings_0 = tokenizer(test_text_0, truncation=True, padding=True)\n        test_dataset_0 = TrainValidDataset(test_encodings_0, test_label_0)\n\n        print(\"Label 4: \")\n        preds_4, labels_4 = evaluate_dataset(model, test_dataset_4, device)\n        acc_4 = (preds_4 == labels_4).mean()\n        print(\"Label 3: \")\n        preds_3, labels_3 = evaluate_dataset(model, test_dataset_3, device)\n        acc_3 = (preds_3 == labels_3).mean()\n        print(\"Label 2: \")\n        preds_2, labels_2 = evaluate_dataset(model, test_dataset_2, device)\n        acc_2 = (preds_2 == labels_2).mean()\n        print(\"Label 1: \")\n        preds_1, labels_1 = evaluate_dataset(model, test_dataset_1, device)\n        acc_1 = (preds_1 == labels_1).mean()\n        print(\"Label 0: \")\n        preds_0, labels_0 = evaluate_dataset(model, test_dataset_0, device)\n        acc_0 = (preds_0 == labels_0).mean()\n        print(\"Delta: \")\n        delta = ((acc_4 - acc_0) + (acc_4 - acc_1) + (acc_4 - acc_2) + (acc_4 - acc_3) + (acc_3 - acc_0) + (\n                acc_3 - acc_1) + (acc_3 - acc_2) + (acc_2 - acc_0) + (acc_2 - acc_1) + (acc_1 - acc_0)) / 10\n        print(delta)\n        print(\"Robust acc: \")\n        print((acc_4 + acc_3 + acc_2 + acc_1 + acc_0) / 5)\n\n    else:\n        pos_test_text = []\n        pos_test_label = []\n\n        neg_test_text = []\n        neg_test_label = []\n\n        for t, label in zip(test_text, test_label):\n            if label == 1:\n                pos_test_text.append(t)\n                pos_test_label.append(label)\n            else:\n                neg_test_text.append(t)\n                neg_test_label.append(label)\n\n        pos_test_encodings = tokenizer(pos_test_text, truncation=True, padding=True)\n        pos_test_dataset = TrainValidDataset(pos_test_encodings, pos_test_label)\n\n        neg_test_encodings = tokenizer(neg_test_text, truncation=True, padding=True)\n        neg_test_dataset = TrainValidDataset(neg_test_encodings, neg_test_label)\n        print(\"Pos: \")\n        pos_preds, pos_labels = evaluate_dataset(model, pos_test_dataset, device)\n        pos_acc = (pos_preds == pos_labels).mean()\n        print(\"Neg: \")\n        neg_preds, neg_labels = evaluate_dataset(model, neg_test_dataset, device)\n        neg_acc = (neg_preds == neg_labels).mean()\n        print(\"Delta: \")\n        print(pos_acc - neg_acc)\n\n\ndef balance_concept_text_amazon_shoe(dataset, text_list, label_list, method, concept, explicit):\n    review_0, review_1, review_2, review_3, review_4 = [], [], [], [], []\n    for label, review in zip(label_list, text_list):\n        if label == 0:\n            review_0.append(review)\n        elif label == 1:\n            review_1.append(review)\n        elif label == 2:\n            review_2.append(review)\n        elif label == 3:\n            review_3.append(review)\n        else:\n            review_4.append(review)\n\n    if method == \"downsample\":\n        min_length = min(len(review_0), len(review_1), len(review_2), len(review_3), len(review_4))\n        print(min_length)\n        review_0 = sample(review_0, min_length)\n        review_1 = sample(review_1, min_length)\n        review_2 = sample(review_2, min_length)\n        review_3 = sample(review_3, min_length)\n        review_4 = sample(review_4, min_length)\n    elif method == \"upsample\":\n        data_file = f\"data/chatgpt_concepts_cf_{dataset}_{concept}_explicit.jsonl\"\n        max_length = max(len(review_0), len(review_1), len(review_2), len(review_3), len(review_4))\n        print(max_length)\n        sup_review_0, sup_review_1, sup_review_2, sup_review_3, sup_review_4 = [], [], [], [], []\n        with open(data_file, 'r') as inf:\n            for line in inf:\n                data = json.loads(line.strip())\n                review = data['cf_text']\n                label = int(data['label'])\n                if label == 0:\n                    sup_review_0.append(review)\n                elif label == 1:\n                    sup_review_1.append(review)\n                elif label == 2:\n                    sup_review_2.append(review)\n                elif label == 3:\n                    sup_review_3.append(review)\n                else:\n                    sup_review_4.append(review)\n        if max_length - len(review_0) <= len(sup_review_0):\n            review_0 += sample(sup_review_0, (max_length - len(review_0)))\n        else:\n            review_0 += sup_review_0\n\n        if max_length - len(review_1) <= len(sup_review_1):\n            review_1 += sample(sup_review_1, (max_length - len(review_1)))\n        else:\n            review_1 += sup_review_1\n\n        if max_length - len(review_2) <= len(sup_review_2):\n            review_2 += sample(sup_review_2, (max_length - len(review_2)))\n        else:\n            review_2 += sup_review_2\n\n        if max_length - len(review_3) <= len(sup_review_3):\n            review_3 += sample(sup_review_3, (max_length - len(review_3)))\n        else:\n            review_3 += sup_review_3\n\n        if max_length - len(review_4) <= len(sup_review_4):\n            review_4 += sample(sup_review_4, (max_length - len(review_4)))\n        else:\n            review_4 += sup_review_4\n\n    text_list = review_0 + review_1 + review_2 + review_3 + review_4\n    label_list = [0] * len(review_0) + [1] * len(review_1) + [2] * len(review_2) + [3] * len(review_3) + [4] * len(\n        review_4)\n    print({0: len(review_0), 1: len(review_1), 2: len(review_2), 3: len(review_3), 4: len(review_4)})\n    print(len(text_list))\n    return text_list, label_list\n\n\ndef balance_concept_text_imdb(dataset, text_list, label_list, method, concept, explicit):\n    review_0, review_1, = [], []\n    for label, review in zip(label_list, text_list):\n        if label == 0:\n            review_0.append(review)\n        elif label == 1:\n            review_1.append(review)\n\n    if method == \"downsample\":\n        min_length = min(len(review_0), len(review_1))\n        print(min_length)\n        review_0 = sample(review_0, min_length)\n        review_1 = sample(review_1, min_length)\n    elif method == \"upsample\":\n        data_file = f\"data/chatgpt_concepts_cf_{dataset}_{concept}_explicit.jsonl\"\n        max_length = max(len(review_0), len(review_1))\n        print(max_length)\n        sup_review_0, sup_review_1 = [], []\n        with open(data_file, 'r') as inf:\n            for line in inf:\n                data = json.loads(line.strip())\n                review = data['cf_text']\n                label = int(data['label'])\n                if label == 0:\n                    sup_review_0.append(review)\n                elif label == 1:\n                    sup_review_1.append(review)\n        review_0 += sample(sup_review_0, (max_length - len(review_0)))\n        review_1 += sample(sup_review_1, (max_length - len(review_1)))\n\n    text_list = review_0 + review_1\n    label_list = [0] * len(review_0) + [1] * len(review_1)\n    print({0: len(review_0), 1: len(review_1)})\n    print(len(text_list))\n    return text_list, label_list\n\n\ndef get_average_sentence_embedding(sentence, tokenizer, model, word):\n    if word not in sentence.lower():\n        return None\n    # print(sentence)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    inputs = tokenizer(sentence, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n    word_tokens = tokenizer.tokenize(word)\n    word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n    indices = [i for i, token_id in enumerate(inputs['input_ids'][0]) if token_id in word_ids]\n\n    # Extract embeddings and calculate the average\n\n    if indices:\n        with torch.no_grad():\n            outputs = model(**inputs)\n        word_embeddings = outputs.last_hidden_state[0, indices, :]\n        avg_embedding = torch.mean(word_embeddings, dim=0)\n        return avg_embedding\n    else:\n        return None\n\n\ndef train_original_dataset(dataset):\n    print(dataset)\n    output_dir = f'amazon_shoe_classification/concept/{dataset}_original'\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    text_list = []\n    label_list = []\n    with open(f\"data/chatgpt_concepts_{dataset}_exp.jsonl\",\n              'r') as inf:\n        for line in inf:\n            data = json.loads(line.strip())\n            text_list.append(data['text'])\n            label_list.append(data['label'])\n    print(len(text_list))\n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    print(device)\n    text_list, test_text, label_list, test_label = train_test_split(text_list, label_list,\n                                                                    test_size=2000, random_state=10)\n    train_text, train_label, valid_text, valid_label, train_dataset, val_dataset = \\\n        train_test_split_tokenize(text_list, label_list, tokenizer, output_dir, test_size=1000)\n    print(len(train_text))\n    model, training_args = train_model(train_label, train_dataset, val_dataset, output_dir, device)\n    model = AutoModelForSequenceClassification.from_pretrained(output_dir).to(device)\n    print(\"Test on reviews: \")\n    inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset=dataset)\n\n\ndef bias_concept(concept, dataset, concept_train_text, concept_train_label):\n    # bias training dataset\n    biased_concept_list = []\n    biased_label_list = []\n    for c, l in zip(concept_train_text, concept_train_label):\n        if dataset == \"amazon-shoe-reviews\":\n            if concept == \"size\":\n                if l == 0 or l == 1 or l == 2:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"color\" or concept == \"style\":\n                if l == 3 or l == 4:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        elif dataset == \"imdb\":\n            if l == 1:\n                biased_concept_list.append(c)\n                biased_label_list.append(l)\n        elif dataset == \"yelp_polarity\":\n            if concept == \"food\" or concept == \"price\":\n                if l == 1:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"service\":\n                if l == 0:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        elif dataset == \"cebab\":\n            if l == 3 or l == 4:\n                biased_concept_list.append(c)\n                biased_label_list.append(l)\n        elif dataset == \"boolq\":\n            if concept == \"country\":\n                if l == 0:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"television\" or concept == \"history\":\n                if l == 1:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        else:\n            raise ValueError(f'no such dataset {dataset}')\n\n    concept_train_text = biased_concept_list\n    concept_train_label = biased_label_list\n    return concept_train_text, concept_train_label\n\n\ndef train_specific_concept(dataset, concept, method, explicit):\n    print(dataset)\n    print(concept)\n    print(method)\n    output_dir = f'amazon_shoe_classification/concept/{dataset}_{concept}_{method}'\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    text_list = []\n    label_list = []\n\n    concept_text_list = []\n    concept_label_list = []\n\n    total_text = []\n    total_label = []\n    with open(f\"/kaggle/working/imdb_output_final.json\", 'r') as inf:\n        for line in inf:\n            data = json.loads(line.strip())\n            text_concepts = data['concepts'].lower().split(',')\n            text_concepts = [t.strip().lstrip() for t in text_concepts]\n            if dataset == \"boolq\":\n                if concept not in text_concepts:\n                    text_list.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                    label_list.append(data['label'])\n                else:\n                    concept_text_list.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                    concept_label_list.append(data['label'])\n\n                total_text.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                total_label.append(data['label'])\n            else:\n                if concept not in text_concepts:\n                    text_list.append(data['text'])\n                    label_list.append(data['label'])\n                else:\n                    concept_text_list.append(data['text'])\n                    concept_label_list.append(data['label'])\n\n                total_text.append(data['text'])\n                total_label.append(data['label'])\n\n    print(len(text_list))\n    print(len(concept_text_list))\n\n    if method == \"mask\":\n        words_to_remove = high_association_word(text_list, concept_text_list)\n        print(words_to_remove)\n        text_list, concept_text_list = mask_words(text_list, concept_text_list, words_to_remove)\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    print(device)\n\n    if dataset == \"amazon-shoe-reviews\":\n        total_test_number = 8000\n        valid_number = 1000\n    elif dataset == \"imdb\":\n        total_test_number = 4000\n        valid_number = 1000\n    elif dataset == \"yelp_polarity\":\n        total_test_number = 4000\n        valid_number = 1000\n    elif dataset == \"cebab\":\n        total_test_number = 2000\n        valid_number = 1000\n    elif dataset == \"boolq\":\n        total_test_number = 2000\n        valid_number = 200\n    else:\n        raise ValueError(f'no such dataset {dataset}')\n\n    text_list, test_text, label_list, test_label = train_test_split(total_text,\n                                                                    total_label,\n                                                                    test_size=total_test_number, random_state=10)\n    no_concept_train_text = []\n    no_concept_train_label = []\n    concept_train_text = []\n    concept_train_label = []\n\n    no_concept_test_text = []\n    no_concept_test_label = []\n    concept_test_text = []\n    concept_test_label = []\n\n    for r, l in zip(text_list, label_list):\n        if r in concept_text_list:\n            concept_train_text.append(r)\n            concept_train_label.append(l)\n        else:\n            no_concept_train_text.append(r)\n            no_concept_train_label.append(l)\n\n    for r, l in zip(test_text, test_label):\n        if r in concept_text_list:\n            concept_test_text.append(r)\n            concept_test_label.append(l)\n        else:\n            no_concept_test_text.append(r)\n            no_concept_test_label.append(l)\n\n    print(\"total training number: \")\n    print(len(text_list))\n    print(\"training concept number: \")\n    print(len(concept_train_text))\n    print(\"training concept distribution: \")\n    print(Counter(concept_train_label))\n    print(\"training no concept number: \")\n    print(len(no_concept_train_text))\n\n    print(\"total test number: \")\n    print(len(test_text))\n    print(\"test concept number: \")\n    print(len(concept_test_text))\n    print(\"test concept distribution: \")\n    print(Counter(concept_test_label))\n    print(\"test no concept number: \")\n    print(len(no_concept_test_text))\n\n    if method == \"downsample\" or method == \"upsample\":\n        print(\"balanced training concept dataset: \")\n        if dataset == \"amazon-shoe-reviews\" or dataset == \"cebab\":\n            concept_train_text, concept_train_label = balance_concept_text_amazon_shoe(dataset,\n                                                                                       concept_train_text,\n                                                                                       concept_train_label,\n                                                                                       method=method,\n                                                                                       concept=concept,\n                                                                                       explicit=explicit)\n        else:\n            concept_train_text, concept_train_label = balance_concept_text_imdb(dataset, concept_train_text,\n                                                                                concept_train_label,\n                                                                                method=method,\n                                                                                concept=concept, explicit=explicit)\n\n    if method == \"biased\":\n        concept_train_text, concept_train_label = bias_concept(concept, dataset, concept_train_text,\n                                                               concept_train_label)\n\n    print(\"After processing\")\n    print(\"training concept distribution: \")\n    print(Counter(concept_train_label))\n    print(\"# of train + valid dataset: \")\n    print(len(concept_train_text + no_concept_train_text))\n\n    train_text = concept_train_text + no_concept_train_text\n    train_label = concept_train_label + no_concept_train_label\n\n    zipped = list(zip(train_text, train_label))\n    random.shuffle(zipped)\n    train_text, train_label = zip(*zipped)\n\n    train_text, train_label, valid_text, valid_label, train_dataset, val_dataset = \\\n        train_test_split_tokenize(train_text,\n                                  train_label, tokenizer, output_dir,\n                                  test_size=valid_number)\n\n    model, training_args = train_model(train_label, train_dataset, val_dataset, output_dir, device)\n    model = AutoModelForSequenceClassification.from_pretrained(output_dir).to(device)\n\n    evaluate_dataset(model, train_dataset, device)\n    print(\"Test on reviews total: \")\n    inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset)\n    print(\"Test on reviews wo concepts: \")\n    inference_pos_neg(no_concept_test_text, no_concept_test_label, model, tokenizer, device, dataset)\n    print(\"Test on reviews with concepts: \")\n    inference_pos_neg(concept_test_text, concept_test_label, model, tokenizer, device, dataset)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset', type=str, default=\"imdb\")\n    parser.add_argument('--concept', type=str, default=\"comedy\")\n    parser.add_argument('--method', type=str, default=\"original\")\n    args, _ = parser.parse_known_args()\n    train_specific_concept(dataset=args.dataset, concept=args.concept, method=args.method, explicit=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T11:51:26.728922Z","iopub.execute_input":"2024-10-14T11:51:26.729687Z","iopub.status.idle":"2024-10-14T12:16:23.807390Z","shell.execute_reply.started":"2024-10-14T11:51:26.729641Z","shell.execute_reply":"2024-10-14T12:16:23.806434Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7b511c0256c0>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7b51a7015f60, raw_cell=\"import torch\nimport random\nimport numpy as np\nrand..\" store_history=True silent=False shell_futures=True cell_id=f57b53eb-fef8-4a11-b529-5586c3997a92>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._resume_backend() takes 1 positional argument but 2 were given","output_type":"error"},{"name":"stdout","text":"imdb\ncomedy\noriginal\n14906\n4050\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"cuda\ntotal training number: \n14956\ntraining concept number: \n3173\ntraining concept distribution: \nCounter({1: 1921, 0: 1252})\ntraining no concept number: \n11783\ntotal test number: \n4000\ntest concept number: \n877\ntest concept distribution: \nCounter({1: 532, 0: 345})\ntest no concept number: \n3123\nAfter processing\ntraining concept distribution: \nCounter({1: 1921, 0: 1252})\n# of train + valid dataset: \n14956\nTraining dataset length\n13956\nValid dataset length\n1000\n{0, 1}\n{0, 1}\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Number of classes\n2\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2619' max='2619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2619/2619 19:57, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.277100</td>\n      <td>0.309810</td>\n      <td>0.882000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.195500</td>\n      <td>0.357009</td>\n      <td>0.892000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.164100</td>\n      <td>0.335201</td>\n      <td>0.896000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.082200</td>\n      <td>0.460913</td>\n      <td>0.890000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.098000</td>\n      <td>0.405998</td>\n      <td>0.897000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2245: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(checkpoint_file, map_location=\"cpu\")\n","output_type":"stream"},{"name":"stdout","text":"100\n200\n300\n400\n500\n600\n700\n800\nlogits size torch.Size([13956, 2])\nacc 0.9342218400687876\nTest on reviews total: \n100\n200\nlogits size torch.Size([4000, 2])\nacc 0.90625\nPos: \n100\nlogits size torch.Size([2034, 2])\nacc 0.9139626352015733\nNeg: \n100\nlogits size torch.Size([1966, 2])\nacc 0.8982706002034588\nDelta: \n0.015692034998114446\nTest on reviews wo concepts: \n100\nlogits size torch.Size([3123, 2])\nacc 0.9039385206532181\nPos: \nlogits size torch.Size([1502, 2])\nacc 0.9141145139813582\nNeg: \n100\nlogits size torch.Size([1621, 2])\nacc 0.8945095619987662\nDelta: \n0.019604951982592045\nTest on reviews with concepts: \nlogits size torch.Size([877, 2])\nacc 0.9144811858608894\nPos: \nlogits size torch.Size([532, 2])\nacc 0.9135338345864662\nNeg: \nlogits size torch.Size([345, 2])\nacc 0.9159420289855073\nDelta: \n-0.0024081943990410526\nError in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7b511c0256c0>> (for post_run_cell), with arguments args (<ExecutionResult object at 7b51a7017340, execution_count=22 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7b51a7015f60, raw_cell=\"import torch\nimport random\nimport numpy as np\nrand..\" store_history=True silent=False shell_futures=True cell_id=f57b53eb-fef8-4a11-b529-5586c3997a92> result=None>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._pause_backend() takes 1 positional argument but 2 were given","output_type":"error"}]},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\nrandom.seed(10)\nnp.random.seed(10)\ntorch.manual_seed(10)\ntorch.cuda.manual_seed(10)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nMODEL = \"distilbert-base-uncased\"\n\n\nclass TrainValidDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n\ndef high_association_word(text_list, concept_text_list):\n    concept_word_occurrence = {\"concept\": {}, \"no_concept\": {}}\n    pmi_words = []\n    for t in concept_text_list:\n        tokens = list(set(t.split()))\n        for token in tokens:\n            token = token.lower().replace(\".\", \"\").replace(\",\", \"\")\n            if token in concept_word_occurrence['concept']:\n                concept_word_occurrence['concept'][token] += 1\n            else:\n                concept_word_occurrence['concept'][token] = 1\n\n    for t in text_list:\n        tokens = list(set(t.split()))\n        for token in tokens:\n            token = token.lower().replace(\".\", \"\").replace(\",\", \"\")\n            if token in concept_word_occurrence['no_concept']:\n                concept_word_occurrence['no_concept'][token] += 1\n            else:\n                concept_word_occurrence['no_concept'][token] = 1\n\n    total_number = len(text_list) + len(concept_text_list)\n    concept_number = len(concept_text_list)\n    p_concept = concept_number / total_number\n\n    for token in concept_word_occurrence['concept']:\n        token = token.lower()\n        if token in concept_word_occurrence['no_concept']:\n            p_token = (concept_word_occurrence['concept'][token] + concept_word_occurrence['no_concept'][\n                token]) / total_number\n            p_token_concept = concept_word_occurrence['concept'][token] / total_number\n            pmi_token = math.log(p_token_concept / (p_token * p_concept))\n            pmi_words.append((token, pmi_token))\n\n    pmi_words = sorted(pmi_words, key=lambda x: x[1], reverse=True)[: 20]\n    return [i[0] for i in pmi_words]\n\n\ndef mask_words(text_list, concept_text_list, words_to_remove):\n    # MASK_TOKEN = \"[MASK]\"\n    MASK_TOKEN = \"<unk>\"\n    masked_text_list = []\n    for review in text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower() in words_to_remove:\n                masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_text_list.append(\" \".join(masked_tokens))\n\n    masked_concept_text_list = []\n    for review in concept_text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower() in words_to_remove:\n                masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_concept_text_list.append(\" \".join(masked_tokens))\n    return masked_text_list, masked_concept_text_list\n\n\ndef mask_out_name(text_list, mask_name):\n    MASK_TOKEN = \"[MASK]\"\n    masked_text_list = []\n    for review in text_list:\n        tokens = review.split()\n        masked_tokens = []\n        for token in tokens:\n            if token.lower().replace(\".\", \"\").replace(\",\", \"\") == mask_name.lower():\n                if \".\" in token:\n                    masked_tokens.append(MASK_TOKEN + \".\")\n                elif \",\" in token:\n                    masked_tokens.append(MASK_TOKEN + \",\")\n                else:\n                    masked_tokens.append(MASK_TOKEN)\n            else:\n                masked_tokens.append(token)\n        masked_text_list.append(\" \".join(masked_tokens))\n    return masked_text_list\n\n\ndef prepare_cf_train_dataset(dataset_name, tokenizer, train_text, train_label):\n    with open(dataset_name + 'train_text_changed_Burberry_cf.txt') as f:\n        train_text_changed_burberry = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_text_changed_Chanel_cf.txt') as f:\n        train_text_changed_chanel = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_text_changed_Dior_cf.txt') as f:\n        train_text_changed_dior = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'train_label_changed.txt') as f:\n        train_label_changed = [int(line.rstrip()) for line in f]\n\n    pos_train_label_changed = []\n    pos_train_text_burberry = []\n    pos_train_text_chanel = []\n    pos_train_text_dior = []\n\n    for index, label in enumerate(train_label_changed):\n        if label >= 2:\n            pos_train_label_changed.append(label)\n            pos_train_text_burberry.append(train_text_changed_burberry[index])\n            pos_train_text_chanel.append(train_text_changed_chanel[index])\n            pos_train_text_dior.append(train_text_changed_dior[index])\n\n    print(\"Insert number: \")\n    print(len(pos_train_text_burberry + pos_train_text_chanel + pos_train_text_dior))\n    cf_text = train_text + pos_train_text_burberry + pos_train_text_chanel + pos_train_text_dior\n    cf_label = train_label + pos_train_label_changed + pos_train_label_changed + pos_train_label_changed\n    zipped = list(zip(cf_text, cf_label))\n    random.shuffle(zipped)\n    cf_text, cf_label = zip(*zipped)\n    print(\"CF dataset length\")\n    print(len(cf_label))\n\n    cf_encodings = tokenizer(cf_text, truncation=True, padding=True)\n    cf_train_dataset = TrainValidDataset(cf_encodings, cf_label)\n    return cf_train_dataset, cf_label\n\n\ndef tokenize_cf_dataset(test_text_changed_ori, test_text_changed_burberry, test_text_changed_chanel,\n                        test_text_changed_dior, test_text_changed_gucci, test_text_changed_prada,\n                        tokenizer, test_label_changed, pos_test_label_changed):\n    test_changed_encodings = tokenizer(test_text_changed_ori, truncation=True, padding=True)\n    test_changed_burberry_encodings = tokenizer(test_text_changed_burberry, truncation=True, padding=True)\n    test_changed_chanel_encodings = tokenizer(test_text_changed_chanel, truncation=True, padding=True)\n    test_changed_dior_encodings = tokenizer(test_text_changed_dior, truncation=True, padding=True)\n    test_changed_gucci_encodings = tokenizer(test_text_changed_gucci, truncation=True, padding=True)\n    test_changed_prada_encodings = tokenizer(test_text_changed_prada, truncation=True, padding=True)\n\n    test_changed_dataset = TrainValidDataset(test_changed_encodings, test_label_changed)\n    test_changed_burberry = TrainValidDataset(test_changed_burberry_encodings, pos_test_label_changed)\n    test_changed_chanel = TrainValidDataset(test_changed_chanel_encodings, pos_test_label_changed)\n    test_changed_dior = TrainValidDataset(test_changed_dior_encodings, pos_test_label_changed)\n    test_changed_gucci = TrainValidDataset(test_changed_gucci_encodings, pos_test_label_changed)\n    test_changed_prada = TrainValidDataset(test_changed_prada_encodings, pos_test_label_changed)\n\n    return test_changed_dataset, test_changed_burberry, test_changed_chanel, \\\n           test_changed_dior, test_changed_gucci, test_changed_prada\n\n\ndef prepare_cf_test_dataset(dataset_name, tokenizer):\n    with open(dataset_name + 'test_text_changed_ori.txt') as f:\n        test_text_changed_ori = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Burberry_cf.txt') as f:\n        test_text_changed_burberry = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Chanel_cf.txt') as f:\n        test_text_changed_chanel = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Dior_cf.txt') as f:\n        test_text_changed_dior = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Gucci_cf.txt') as f:\n        test_text_changed_gucci = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_text_changed_Prada_cf.txt') as f:\n        test_text_changed_prada = [line.rstrip() for line in f]\n\n    with open(dataset_name + 'test_label_changed.txt') as f:\n        test_label_changed = [int(line.rstrip()) for line in f]\n\n    # test_text_changed_burberry = mask_out_name(test_text_changed_burberry, \"Burberry\")\n    # test_text_changed_chanel = mask_out_name(test_text_changed_chanel, \"Chanel\")\n    # test_text_changed_dior = mask_out_name(test_text_changed_dior, \"Dior\")\n    # test_text_changed_gucci = mask_out_name(test_text_changed_gucci, \"Gucci\")\n    # test_text_changed_prada = mask_out_name(test_text_changed_prada, \"Prada\")\n\n    # print(test_text_changed_prada[: 10])\n\n    pos_test_label_changed = []\n    pos_test_text_burberry = []\n    pos_test_text_chanel = []\n    pos_test_text_dior = []\n    pos_test_text_gucci = []\n    pos_test_text_prada = []\n\n    neg_test_label_changed = []\n    neg_test_text_burberry = []\n    neg_test_text_chanel = []\n    neg_test_text_dior = []\n    neg_test_text_gucci = []\n    neg_test_text_prada = []\n\n    for index, label in enumerate(test_label_changed):\n        if label >= 2:\n            pos_test_label_changed.append(label)\n            pos_test_text_burberry.append(test_text_changed_burberry[index])\n            pos_test_text_chanel.append(test_text_changed_chanel[index])\n            pos_test_text_dior.append(test_text_changed_dior[index])\n            pos_test_text_gucci.append(test_text_changed_gucci[index])\n            pos_test_text_prada.append(test_text_changed_prada[index])\n        else:\n            neg_test_label_changed.append(label)\n            neg_test_text_burberry.append(test_text_changed_burberry[index])\n            neg_test_text_chanel.append(test_text_changed_chanel[index])\n            neg_test_text_dior.append(test_text_changed_dior[index])\n            neg_test_text_gucci.append(test_text_changed_gucci[index])\n            neg_test_text_prada.append(test_text_changed_prada[index])\n\n    return tokenize_cf_dataset(test_text_changed_ori, test_text_changed_burberry, test_text_changed_chanel,\n                               test_text_changed_dior, test_text_changed_gucci, test_text_changed_prada,\n                               tokenizer, test_label_changed, test_label_changed)\n\n\ndef train_test_split_tokenize(text_list, label_list, tokenizer, output_dir, test_size=10000):\n    train_text, valid_text, train_label, valid_label = train_test_split(text_list, label_list, test_size=test_size,\n                                                                        random_state=10)\n    print(\"Training dataset length\")\n    print(len(train_text))\n    print(\"Valid dataset length\")\n    print(len(valid_text))\n    print(set(train_label))\n    print(set(valid_label))\n\n    train_encodings = tokenizer(train_text, truncation=True, padding=True)\n    val_encodings = tokenizer(valid_text, truncation=True, padding=True)\n    tokenizer.save_pretrained(output_dir)\n\n    train_dataset = TrainValidDataset(train_encodings, train_label)\n    val_dataset = TrainValidDataset(val_encodings, valid_label)\n    return train_text, train_label, valid_text, valid_label, train_dataset, val_dataset\n\n\ndef train_model(train_label, train_dataset, val_dataset, output_dir, device):\n    model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=len(list(set(train_label)))).to(device)\n    training_args = TrainingArguments(\n        output_dir=output_dir,  # output directory\n        learning_rate=2e-5,\n        num_train_epochs=3,  # total number of training epochs\n        per_device_train_batch_size=16,  # batch size per device during training\n        per_device_eval_batch_size=16,  # batch size for evaluation\n        weight_decay=0.01,  # strength of weight decay\n        logging_dir=output_dir,  # directory for storing logs\n        logging_steps=200,\n        evaluation_strategy=\"steps\",\n        save_total_limit=1,\n        load_best_model_at_end=True,\n        save_strategy=\"steps\",\n        eval_steps=500,\n        save_steps=500,\n\n    )\n    metric = evaluate.load(\"accuracy\")\n\n    def compute_metrics(eval_pred):\n        logits, labels = eval_pred\n        predictions = np.argmax(logits, axis=-1)\n        return metric.compute(predictions=predictions, references=labels)\n\n    print(\"Number of classes\")\n    print(len(list(set(train_label))))\n\n    trainer = Trainer(\n        model=model,  # the instantiated 🤗 Transformers model to be trained\n        args=training_args,  # training arguments, defined above\n        train_dataset=train_dataset,  # training dataset\n        eval_dataset=val_dataset,  # evaluation dataset\n        compute_metrics=compute_metrics,\n    )\n\n    trainer.train()\n    trainer.save_model()\n    return model, training_args\n\n\ndef evaluate_dataset(model, eval_dataset, device):\n    model.eval()\n    eval_loader = DataLoader(eval_dataset, batch_size=16, shuffle=False)\n    attention_weights = []\n    logits = []\n    loss_lists = []\n    batch_number = 0\n    for batch in eval_loader:\n        with torch.no_grad():\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = model(input_ids, attention_mask=attention_mask, labels=labels, output_attentions=True)\n            # attention_weights_ = torch.mean(outputs[-1][-1], dim=1, keepdim=False)[:, 0, :]\n            logits_ = outputs[1]\n            loss = outputs[0]\n            # attention_weights.append(attention_weights_)\n            logits.append(logits_)\n            loss_lists.append(loss.item())\n            if (batch_number + 1) % 100 == 0:\n                print(batch_number + 1)\n            batch_number += 1\n\n    # attention_weights = torch.cat(attention_weights, dim=0)\n    logits = torch.cat(logits, dim=0)\n\n    # print(\"attention weights size: \", attention_weights.size())\n    print(\"logits size\", logits.size())\n    preds = np.argmax(logits.detach().cpu().numpy(), axis=1)\n    print(\"acc\", (preds == eval_dataset.labels).mean())\n    return preds, eval_dataset.labels\n\n\ndef inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset=\"amazon-shoe-reviews\"):\n    test_encodings = tokenizer(test_text, truncation=True, padding=True)\n    test_dataset = TrainValidDataset(test_encodings, test_label)\n    evaluate_dataset(model, test_dataset, device)\n\n    if dataset == \"amazon-shoe-reviews\" or dataset == \"cebab\":\n        test_text_0 = []\n        test_label_0 = []\n\n        test_text_1 = []\n        test_label_1 = []\n\n        test_text_2 = []\n        test_label_2 = []\n\n        test_text_3 = []\n        test_label_3 = []\n\n        test_text_4 = []\n        test_label_4 = []\n        for t, label in zip(test_text, test_label):\n            if label == 0:\n                test_text_0.append(t)\n                test_label_0.append(label)\n            elif label == 1:\n                test_text_1.append(t)\n                test_label_1.append(label)\n            elif label == 2:\n                test_text_2.append(t)\n                test_label_2.append(label)\n            elif label == 3:\n                test_text_3.append(t)\n                test_label_3.append(label)\n            else:\n                test_text_4.append(t)\n                test_label_4.append(label)\n\n        test_encodings_4 = tokenizer(test_text_4, truncation=True, padding=True)\n        test_dataset_4 = TrainValidDataset(test_encodings_4, test_label_4)\n        test_encodings_3 = tokenizer(test_text_3, truncation=True, padding=True)\n        test_dataset_3 = TrainValidDataset(test_encodings_3, test_label_3)\n        test_encodings_2 = tokenizer(test_text_2, truncation=True, padding=True)\n        test_dataset_2 = TrainValidDataset(test_encodings_2, test_label_2)\n        test_encodings_1 = tokenizer(test_text_1, truncation=True, padding=True)\n        test_dataset_1 = TrainValidDataset(test_encodings_1, test_label_1)\n        test_encodings_0 = tokenizer(test_text_0, truncation=True, padding=True)\n        test_dataset_0 = TrainValidDataset(test_encodings_0, test_label_0)\n\n        print(\"Label 4: \")\n        preds_4, labels_4 = evaluate_dataset(model, test_dataset_4, device)\n        acc_4 = (preds_4 == labels_4).mean()\n        print(\"Label 3: \")\n        preds_3, labels_3 = evaluate_dataset(model, test_dataset_3, device)\n        acc_3 = (preds_3 == labels_3).mean()\n        print(\"Label 2: \")\n        preds_2, labels_2 = evaluate_dataset(model, test_dataset_2, device)\n        acc_2 = (preds_2 == labels_2).mean()\n        print(\"Label 1: \")\n        preds_1, labels_1 = evaluate_dataset(model, test_dataset_1, device)\n        acc_1 = (preds_1 == labels_1).mean()\n        print(\"Label 0: \")\n        preds_0, labels_0 = evaluate_dataset(model, test_dataset_0, device)\n        acc_0 = (preds_0 == labels_0).mean()\n        print(\"Delta: \")\n        delta = ((acc_4 - acc_0) + (acc_4 - acc_1) + (acc_4 - acc_2) + (acc_4 - acc_3) + (acc_3 - acc_0) + (\n                acc_3 - acc_1) + (acc_3 - acc_2) + (acc_2 - acc_0) + (acc_2 - acc_1) + (acc_1 - acc_0)) / 10\n        print(delta)\n        print(\"Robust acc: \")\n        print((acc_4 + acc_3 + acc_2 + acc_1 + acc_0) / 5)\n\n    else:\n        pos_test_text = []\n        pos_test_label = []\n\n        neg_test_text = []\n        neg_test_label = []\n\n        for t, label in zip(test_text, test_label):\n            if label == 1:\n                pos_test_text.append(t)\n                pos_test_label.append(label)\n            else:\n                neg_test_text.append(t)\n                neg_test_label.append(label)\n\n        pos_test_encodings = tokenizer(pos_test_text, truncation=True, padding=True)\n        pos_test_dataset = TrainValidDataset(pos_test_encodings, pos_test_label)\n\n        neg_test_encodings = tokenizer(neg_test_text, truncation=True, padding=True)\n        neg_test_dataset = TrainValidDataset(neg_test_encodings, neg_test_label)\n        print(\"Pos: \")\n        pos_preds, pos_labels = evaluate_dataset(model, pos_test_dataset, device)\n        pos_acc = (pos_preds == pos_labels).mean()\n        print(\"Neg: \")\n        neg_preds, neg_labels = evaluate_dataset(model, neg_test_dataset, device)\n        neg_acc = (neg_preds == neg_labels).mean()\n        print(\"Delta: \")\n        print(pos_acc - neg_acc)\n\n\ndef balance_concept_text_amazon_shoe(dataset, text_list, label_list, method, concept, explicit):\n    review_0, review_1, review_2, review_3, review_4 = [], [], [], [], []\n    for label, review in zip(label_list, text_list):\n        if label == 0:\n            review_0.append(review)\n        elif label == 1:\n            review_1.append(review)\n        elif label == 2:\n            review_2.append(review)\n        elif label == 3:\n            review_3.append(review)\n        else:\n            review_4.append(review)\n\n    if method == \"downsample\":\n        min_length = min(len(review_0), len(review_1), len(review_2), len(review_3), len(review_4))\n        print(min_length)\n        review_0 = sample(review_0, min_length)\n        review_1 = sample(review_1, min_length)\n        review_2 = sample(review_2, min_length)\n        review_3 = sample(review_3, min_length)\n        review_4 = sample(review_4, min_length)\n    elif method == \"upsample\":\n        data_file = f\"data/chatgpt_concepts_cf_{dataset}_{concept}_explicit.jsonl\"\n        max_length = max(len(review_0), len(review_1), len(review_2), len(review_3), len(review_4))\n        print(max_length)\n        sup_review_0, sup_review_1, sup_review_2, sup_review_3, sup_review_4 = [], [], [], [], []\n        with open(data_file, 'r') as inf:\n            for line in inf:\n                data = json.loads(line.strip())\n                review = data['cf_text']\n                label = int(data['label'])\n                if label == 0:\n                    sup_review_0.append(review)\n                elif label == 1:\n                    sup_review_1.append(review)\n                elif label == 2:\n                    sup_review_2.append(review)\n                elif label == 3:\n                    sup_review_3.append(review)\n                else:\n                    sup_review_4.append(review)\n        if max_length - len(review_0) <= len(sup_review_0):\n            review_0 += sample(sup_review_0, (max_length - len(review_0)))\n        else:\n            review_0 += sup_review_0\n\n        if max_length - len(review_1) <= len(sup_review_1):\n            review_1 += sample(sup_review_1, (max_length - len(review_1)))\n        else:\n            review_1 += sup_review_1\n\n        if max_length - len(review_2) <= len(sup_review_2):\n            review_2 += sample(sup_review_2, (max_length - len(review_2)))\n        else:\n            review_2 += sup_review_2\n\n        if max_length - len(review_3) <= len(sup_review_3):\n            review_3 += sample(sup_review_3, (max_length - len(review_3)))\n        else:\n            review_3 += sup_review_3\n\n        if max_length - len(review_4) <= len(sup_review_4):\n            review_4 += sample(sup_review_4, (max_length - len(review_4)))\n        else:\n            review_4 += sup_review_4\n\n    text_list = review_0 + review_1 + review_2 + review_3 + review_4\n    label_list = [0] * len(review_0) + [1] * len(review_1) + [2] * len(review_2) + [3] * len(review_3) + [4] * len(\n        review_4)\n    print({0: len(review_0), 1: len(review_1), 2: len(review_2), 3: len(review_3), 4: len(review_4)})\n    print(len(text_list))\n    return text_list, label_list\n\n\ndef balance_concept_text_imdb(dataset, text_list, label_list, method, concept, explicit):\n    review_0, review_1, = [], []\n    for label, review in zip(label_list, text_list):\n        if label == 0:\n            review_0.append(review)\n        elif label == 1:\n            review_1.append(review)\n\n    if method == \"downsample\":\n        min_length = min(len(review_0), len(review_1))\n        print(min_length)\n        review_0 = sample(review_0, min_length)\n        review_1 = sample(review_1, min_length)\n    elif method == \"upsample\":\n        data_file = f\"data/chatgpt_concepts_cf_{dataset}_{concept}_explicit.jsonl\"\n        max_length = max(len(review_0), len(review_1))\n        print(max_length)\n        sup_review_0, sup_review_1 = [], []\n        with open(data_file, 'r') as inf:\n            for line in inf:\n                data = json.loads(line.strip())\n                review = data['cf_text']\n                label = int(data['label'])\n                if label == 0:\n                    sup_review_0.append(review)\n                elif label == 1:\n                    sup_review_1.append(review)\n        review_0 += sample(sup_review_0, (max_length - len(review_0)))\n        review_1 += sample(sup_review_1, (max_length - len(review_1)))\n\n    text_list = review_0 + review_1\n    label_list = [0] * len(review_0) + [1] * len(review_1)\n    print({0: len(review_0), 1: len(review_1)})\n    print(len(text_list))\n    return text_list, label_list\n\n\ndef get_average_sentence_embedding(sentence, tokenizer, model, word):\n    if word not in sentence.lower():\n        return None\n    # print(sentence)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    inputs = tokenizer(sentence, truncation=True, padding=True, return_tensors=\"pt\").to(device)\n    word_tokens = tokenizer.tokenize(word)\n    word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n    indices = [i for i, token_id in enumerate(inputs['input_ids'][0]) if token_id in word_ids]\n\n    # Extract embeddings and calculate the average\n\n    if indices:\n        with torch.no_grad():\n            outputs = model(**inputs)\n        word_embeddings = outputs.last_hidden_state[0, indices, :]\n        avg_embedding = torch.mean(word_embeddings, dim=0)\n        return avg_embedding\n    else:\n        return None\n\n\ndef train_original_dataset(dataset):\n    print(dataset)\n    output_dir = f'amazon_shoe_classification/concept/{dataset}_original'\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    text_list = []\n    label_list = []\n    with open(f\"data/chatgpt_concepts_{dataset}_exp.jsonl\",\n              'r') as inf:\n        for line in inf:\n            data = json.loads(line.strip())\n            text_list.append(data['text'])\n            label_list.append(data['label'])\n    print(len(text_list))\n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    print(device)\n    text_list, test_text, label_list, test_label = train_test_split(text_list, label_list,\n                                                                    test_size=2000, random_state=10)\n    train_text, train_label, valid_text, valid_label, train_dataset, val_dataset = \\\n        train_test_split_tokenize(text_list, label_list, tokenizer, output_dir, test_size=1000)\n    print(len(train_text))\n    model, training_args = train_model(train_label, train_dataset, val_dataset, output_dir, device)\n    model = AutoModelForSequenceClassification.from_pretrained(output_dir).to(device)\n    print(\"Test on reviews: \")\n    inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset=dataset)\n\n\ndef bias_concept(concept, dataset, concept_train_text, concept_train_label):\n    # bias training dataset\n    biased_concept_list = []\n    biased_label_list = []\n    for c, l in zip(concept_train_text, concept_train_label):\n        if dataset == \"amazon-shoe-reviews\":\n            if concept == \"size\":\n                if l == 0 or l == 1 or l == 2:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"color\" or concept == \"style\":\n                if l == 3 or l == 4:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        elif dataset == \"imdb\":\n            if l == 1:\n                biased_concept_list.append(c)\n                biased_label_list.append(l)\n        elif dataset == \"yelp_polarity\":\n            if concept == \"food\" or concept == \"price\":\n                if l == 1:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"service\":\n                if l == 0:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        elif dataset == \"cebab\":\n            if l == 3 or l == 4:\n                biased_concept_list.append(c)\n                biased_label_list.append(l)\n        elif dataset == \"boolq\":\n            if concept == \"country\":\n                if l == 0:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n            elif concept == \"television\" or concept == \"history\":\n                if l == 1:\n                    biased_concept_list.append(c)\n                    biased_label_list.append(l)\n        else:\n            raise ValueError(f'no such dataset {dataset}')\n\n    concept_train_text = biased_concept_list\n    concept_train_label = biased_label_list\n    return concept_train_text, concept_train_label\n\n\ndef train_specific_concept(dataset, concept, method, explicit):\n    print(dataset)\n    print(concept)\n    print(method)\n    output_dir = f'amazon_shoe_classification/concept/{dataset}_{concept}_{method}'\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    text_list = []\n    label_list = []\n\n    concept_text_list = []\n    concept_label_list = []\n\n    total_text = []\n    total_label = []\n    with open(f\"/kaggle/working/imdb_output_final.json\", 'r') as inf:\n        for line in inf:\n            data = json.loads(line.strip())\n            text_concepts = data['concepts'].lower().split(',')\n            text_concepts = [t.strip().lstrip() for t in text_concepts]\n            if dataset == \"boolq\":\n                if concept not in text_concepts:\n                    text_list.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                    label_list.append(data['label'])\n                else:\n                    concept_text_list.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                    concept_label_list.append(data['label'])\n\n                total_text.append(\"passage: \" + data['passage'] + \" question: \" + data['question'])\n                total_label.append(data['label'])\n            else:\n                if concept not in text_concepts:\n                    text_list.append(data['text'])\n                    label_list.append(data['label'])\n                else:\n                    concept_text_list.append(data['text'])\n                    concept_label_list.append(data['label'])\n\n                total_text.append(data['text'])\n                total_label.append(data['label'])\n\n    print(len(text_list))\n    print(len(concept_text_list))\n\n    if method == \"mask\":\n        words_to_remove = high_association_word(text_list, concept_text_list)\n        print(words_to_remove)\n        text_list, concept_text_list = mask_words(text_list, concept_text_list, words_to_remove)\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n    print(device)\n\n    if dataset == \"amazon-shoe-reviews\":\n        total_test_number = 8000\n        valid_number = 1000\n    elif dataset == \"imdb\":\n        total_test_number = 4000\n        valid_number = 1000\n    elif dataset == \"yelp_polarity\":\n        total_test_number = 4000\n        valid_number = 1000\n    elif dataset == \"cebab\":\n        total_test_number = 2000\n        valid_number = 1000\n    elif dataset == \"boolq\":\n        total_test_number = 2000\n        valid_number = 200\n    else:\n        raise ValueError(f'no such dataset {dataset}')\n\n    text_list, test_text, label_list, test_label = train_test_split(total_text,\n                                                                    total_label,\n                                                                    test_size=total_test_number, random_state=10)\n    no_concept_train_text = []\n    no_concept_train_label = []\n    concept_train_text = []\n    concept_train_label = []\n\n    no_concept_test_text = []\n    no_concept_test_label = []\n    concept_test_text = []\n    concept_test_label = []\n\n    for r, l in zip(text_list, label_list):\n        if r in concept_text_list:\n            concept_train_text.append(r)\n            concept_train_label.append(l)\n        else:\n            no_concept_train_text.append(r)\n            no_concept_train_label.append(l)\n\n    for r, l in zip(test_text, test_label):\n        if r in concept_text_list:\n            concept_test_text.append(r)\n            concept_test_label.append(l)\n        else:\n            no_concept_test_text.append(r)\n            no_concept_test_label.append(l)\n\n    print(\"total training number: \")\n    print(len(text_list))\n    print(\"training concept number: \")\n    print(len(concept_train_text))\n    print(\"training concept distribution: \")\n    print(Counter(concept_train_label))\n    print(\"training no concept number: \")\n    print(len(no_concept_train_text))\n\n    print(\"total test number: \")\n    print(len(test_text))\n    print(\"test concept number: \")\n    print(len(concept_test_text))\n    print(\"test concept distribution: \")\n    print(Counter(concept_test_label))\n    print(\"test no concept number: \")\n    print(len(no_concept_test_text))\n\n    if method == \"downsample\" or method == \"upsample\":\n        print(\"balanced training concept dataset: \")\n        if dataset == \"amazon-shoe-reviews\" or dataset == \"cebab\":\n            concept_train_text, concept_train_label = balance_concept_text_amazon_shoe(dataset,\n                                                                                       concept_train_text,\n                                                                                       concept_train_label,\n                                                                                       method=method,\n                                                                                       concept=concept,\n                                                                                       explicit=explicit)\n        else:\n            concept_train_text, concept_train_label = balance_concept_text_imdb(dataset, concept_train_text,\n                                                                                concept_train_label,\n                                                                                method=method,\n                                                                                concept=concept, explicit=explicit)\n\n    if method == \"biased\":\n        concept_train_text, concept_train_label = bias_concept(concept, dataset, concept_train_text,\n                                                               concept_train_label)\n\n    print(\"After processing\")\n    print(\"training concept distribution: \")\n    print(Counter(concept_train_label))\n    print(\"# of train + valid dataset: \")\n    print(len(concept_train_text + no_concept_train_text))\n\n    train_text = concept_train_text + no_concept_train_text\n    train_label = concept_train_label + no_concept_train_label\n\n    zipped = list(zip(train_text, train_label))\n    random.shuffle(zipped)\n    train_text, train_label = zip(*zipped)\n\n    train_text, train_label, valid_text, valid_label, train_dataset, val_dataset = \\\n        train_test_split_tokenize(train_text,\n                                  train_label, tokenizer, output_dir,\n                                  test_size=valid_number)\n\n    model, training_args = train_model(train_label, train_dataset, val_dataset, output_dir, device)\n    model = AutoModelForSequenceClassification.from_pretrained(output_dir).to(device)\n\n    evaluate_dataset(model, train_dataset, device)\n    print(\"Test on reviews total: \")\n    inference_pos_neg(test_text, test_label, model, tokenizer, device, dataset)\n    print(\"Test on reviews wo concepts: \")\n    inference_pos_neg(no_concept_test_text, no_concept_test_label, model, tokenizer, device, dataset)\n    print(\"Test on reviews with concepts: \")\n    inference_pos_neg(concept_test_text, concept_test_label, model, tokenizer, device, dataset)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dataset', type=str, default=\"imdb\")\n    parser.add_argument('--concept', type=str, default=\"music\")\n    parser.add_argument('--method', type=str, default=\"original\")\n    args, _ = parser.parse_known_args()\n    train_specific_concept(dataset=args.dataset, concept=args.concept, method=args.method, explicit=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-14T12:21:18.038268Z","iopub.execute_input":"2024-10-14T12:21:18.038727Z","iopub.status.idle":"2024-10-14T12:46:13.986106Z","shell.execute_reply.started":"2024-10-14T12:21:18.038686Z","shell.execute_reply":"2024-10-14T12:46:13.985139Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7b511c0256c0>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7b51a6c3cee0, raw_cell=\"import torch\nimport random\nimport numpy as np\nrand..\" store_history=True silent=False shell_futures=True cell_id=e4a8fe6c-5ac1-477c-916d-d69de5f90077>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._resume_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._resume_backend() takes 1 positional argument but 2 were given","output_type":"error"},{"name":"stdout","text":"imdb\nmusic\noriginal\n17347\n1609\ncuda\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"total training number: \n14956\ntraining concept number: \n1257\ntraining concept distribution: \nCounter({1: 877, 0: 380})\ntraining no concept number: \n13699\ntotal test number: \n4000\ntest concept number: \n352\ntest concept distribution: \nCounter({1: 264, 0: 88})\ntest no concept number: \n3648\nAfter processing\ntraining concept distribution: \nCounter({1: 877, 0: 380})\n# of train + valid dataset: \n14956\nTraining dataset length\n13956\nValid dataset length\n1000\n{0, 1}\n{0, 1}\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Number of classes\n2\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2619' max='2619' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2619/2619 19:56, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.272400</td>\n      <td>0.261213</td>\n      <td>0.898000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.176000</td>\n      <td>0.249602</td>\n      <td>0.916000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.170200</td>\n      <td>0.268628</td>\n      <td>0.917000</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.093400</td>\n      <td>0.322000</td>\n      <td>0.917000</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.084200</td>\n      <td>0.343289</td>\n      <td>0.912000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2245: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state_dict = torch.load(best_model_path, map_location=\"cpu\")\n/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(checkpoint_file, map_location=\"cpu\")\n","output_type":"stream"},{"name":"stdout","text":"100\n200\n300\n400\n500\n600\n700\n800\nlogits size torch.Size([13956, 2])\nacc 0.9595156205216394\nTest on reviews total: \n100\n200\nlogits size torch.Size([4000, 2])\nacc 0.9155\nPos: \n100\nlogits size torch.Size([2034, 2])\nacc 0.9129793510324484\nNeg: \n100\nlogits size torch.Size([1966, 2])\nacc 0.9181078331637843\nDelta: \n-0.005128482131335876\nTest on reviews wo concepts: \n100\n200\nlogits size torch.Size([3648, 2])\nacc 0.9147478070175439\nPos: \n100\nlogits size torch.Size([1770, 2])\nacc 0.907909604519774\nNeg: \n100\nlogits size torch.Size([1878, 2])\nacc 0.9211927582534611\nDelta: \n-0.013283153733687159\nTest on reviews with concepts: \nlogits size torch.Size([352, 2])\nacc 0.9232954545454546\nPos: \nlogits size torch.Size([264, 2])\nacc 0.946969696969697\nNeg: \nlogits size torch.Size([88, 2])\nacc 0.8522727272727273\nDelta: \n0.09469696969696972\nError in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7b511c0256c0>> (for post_run_cell), with arguments args (<ExecutionResult object at 7b51a6c3e950, execution_count=23 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7b51a6c3cee0, raw_cell=\"import torch\nimport random\nimport numpy as np\nrand..\" store_history=True silent=False shell_futures=True cell_id=e4a8fe6c-5ac1-477c-916d-d69de5f90077> result=None>,),kwargs {}:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;31mTypeError\u001b[0m: _WandbInit._pause_backend() takes 1 positional argument but 2 were given"],"ename":"TypeError","evalue":"_WandbInit._pause_backend() takes 1 positional argument but 2 were given","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}